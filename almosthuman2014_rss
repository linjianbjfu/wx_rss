<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>almosthuman2014</id>
    <title>机器之心</title>
    <updated>2024-03-13T08:00:15.298Z</updated>
    <generator>awesome</generator>
    <author>
        <name>机器之心</name>
    </author>
    <subtitle>专业的人工智能媒体和产业服务平台</subtitle>
    <logo>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</logo>
    <icon>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</icon>
    <entry>
        <title type="html"><![CDATA[向数字世界AGI迈进！智能体已经从头开玩「荒野大镖客 2」了]]></title>
        <id>2650910796_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910796&amp;idx=1&amp;sn=405be9dd0b0d200e4fad13a91b2c2a98&amp;chksm=84e47432b393fd24e2a980e131d265b5e9575557335c95250d22ff77d581a7a59e226f91c1aa#rd"/>
        <updated>2024-03-13T04:25:08.000Z</updated>
        <summary type="html"><![CDATA[<p style="text-align: justify;line-height: 1.75em;"><br></p><div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 29.75px;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;"><span style="display: none;line-height: 0px;">‍</span>机器之心发布</span></p><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 29.75px;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><p style="text-align: justify;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3366566767409479682" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWic8lfCWeamg5rKlyJibKe87nD6hw0PJH9ejlGIXcAs0D6MYH4eicuiaaic9HRDPKaribe8mibI83vQ74qgg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1920" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3366566767409479682"></iframe></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 12px;color: rgb(136, 136, 136);">Video: Cradle从头开始完成主线任务<strong><br></strong></span></p><p style="text-align: center;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">通用计算机控制</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">信息革命产生了数字世界，数字世界为大模型的诞生提供了数据，也最容易实现通用人工智能（AGI）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin: 3pt 0pt;text-align: center;font-family: 等线;font-size: 12pt;line-height: 16px;"><span style="display: inline-block;overflow: hidden;transform: rotate(0deg);width: 300.111px;height: 242.774px;"><img class="rich_pages wxw-img" data-imgfileid="503427048" data-ratio="0.8092592592592592" data-type="png" data-w="1080" height="242.774" style="width: 300.111px;height: 242.774px;" width="300.111" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nplE4aOBzfQP1yIPibghyibUXQibicaOGZwpPhABHmNUQ21xLg8ibfmMPUjw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">向数字世界 AGI 迈进，北京智源人工智能研究院、新加坡南洋理工大学、北京大学携手提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制 General Computer Control (GCC)</strong></span>，即智能体需要像人一样<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>看屏幕，通过键盘、鼠标完成计算机上的所有任务。</strong></span>在过去很长一段时间里，人工智能研究以游戏为场景，而 GCC 将为通用人工智能研究提供场景，也将进一步促进大模型和 AI Agents 的落地与产业化。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为此，研究团队提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体框架 Cradle</strong></span>，使智能体不依赖任何内部 API 直接控制键盘、鼠标和任何软件交互，无论开源还是闭源，甚至能玩《荒野大镖客 2》这样的商业 3A 游戏大作！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427082" data-ratio="0.4546296296296296" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87niaDSpOeuxk906Zw1g5ymo3jCafo6bqBX4SF6Ghxl4cVibGlAmdnOh9VA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2403.03186</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://baai-agents.github.io/Cradle/</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">代码链接：https://github.com/BAAI-Agents/Cradle</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">随着大模型的发展，越来越多的智能体（AI Agents）研究关注计算机控制，包括浏览网页、操作智能手机、玩游戏等。然而，已有研究依赖内部 API 获取输入，并输出预先定义好的动作。要构建能完成计算机上一切任务的<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用智能体</strong></span>，必须使用最通用和最标准的输入输出与计算机进行交互。因此，通用计算机控制使用统一的输入和输出，从而让智能体的通用性变为可能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">但通用性带来了操作上的难度：（1）使用计算机屏幕作为输入对智能体的视频理解能力提出了更高的要求，例如由于没有内部 API，需要通过视觉信息判断动作是否执行成功；（2）使用键盘和鼠标操作作为输出使得智能体需要更高的时空操作精度，比如键盘按键和鼠标点击通常额外涉及时间维度。如何解决这些难题是构建<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体 (GCC Agents) </strong></span>的挑战！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">Cradle：操控一切软件</span></strong></p><p style="text-align: center;line-height: 1.75em;"><span style="font-family: 等线;font-size: 12pt;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427074" data-ratio="0.32599118942731276" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nKiaWLObjvy0phjEalIrEcDaHL0ibM4cPdNFyW63TibicYyox6zia9tt2nDA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「计算机指任何以用户为中心的计算设备，包括 PC、智能手机和平板电脑等。尽管 Cradle 着重于键盘和鼠标操作，但可以很容易扩展到控制手柄和触摸屏等」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通用计算机控制智能体框架 Cradle 主要由 6 个模块组成：信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块。Cradle 高度的通用性来源于其对和计算机交互过程中的原始输入输出的合理封装和抽象。以从屏幕中显示的视频作为输入，提取其中的文本和视觉信息进行决策，并且输出底层操作系统中控制键盘和鼠标的信号去和计算机交互，使得其可以不依赖于任何假设与所有软件进行交互。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427056" data-ratio="0.4889867841409692" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nIvWAwI0IFCvRZJCgqtJlpPLycCm0mPvOfdOHqmictf4r6f3BVtOK0fQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「Cradle 主要由信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块等 6 个模块组成，其强大的决策推理来自于 “反思过去，总结现在，规划未来”」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">同时 Cradle 强大的决策推理模块让其得以自发和软件进行交互并且完成任务，这个过程可以被简单地总结为：<span style="color: rgb(61, 170, 214);"><strong>反思过去，总结现在，规划未来</strong></span>。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">反思过去</span></strong></span><span style="font-size: 15px;">：使用执行过往动作过程的视频作为输入，分别提取出其中关键的文本和视觉信息，通过反思来判断上一步动作是否执行成功、任务是否完成以及如何改进。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">总结现在</span></strong></span><span style="font-size: 15px;">：反思完之后，总结当前情况，并且以此为依据来决定是否更换任务目标或是修改任务内容。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">规划未来</span></strong></span><span style="font-size: 15px;">：最后根据当前任务和现状生成或者更新技能，并且从已学会的技能中检索与当前任务相关的技能作为备选，然后从中选取合适的技能实例化为动作去执行。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在决策推理的同时，Cradle 会周期性地总结和维护储存在情境记忆中的历史信息以及储存在长期记忆中的技能。这一过程的大脑是多模态大模型，如 GPT-4V，但是 Cradle 为其添加了总结、反思以及记忆等功能，形成了完整的面向通用计算机控制的智能体框架，有效解决了通用性所带来的难题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>Cradle：带你从头开始探索《荒野大镖客 2》</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了证明框架的通用性和强大的决策能力，研究团队选择将 Cradle 部署到最为困难以及鲜有人探索的的商业 3A 游戏大作《荒野大镖客 2》。他们认为作为操作最为困难的软件，假如 Cradle 能够在 3A 游戏上自由探索甚至完成主线剧情，那么说明该框架有巨大潜力泛化到其他游戏和软件上。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427057" data-ratio="0.5418502202643172" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n7vyhvoWFaQFBpOjZBmhbJEZ8Z9f5PpQ48icfO0HUfRG1YQvZ7iakf4qA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「与 Minecraft 这样的开源游戏不同，大多数商业游戏特别是 3A 游戏并不提供内部 API 接口，使得类似 Voyager 这样的依赖内部 API 获取输入并输出预定义动作的框架无法迁移到其他游戏中」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以 GPT-4V 为基础，Cradle 能直接根据游戏内的提示和教程生成对应的可执行代码作为技能，一步步丰富自己的技能库， 并在之后的游戏中重复使用这些技能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427058" data-ratio="0.2753303964757709" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nNW7n247IjrvyEWsM1ZTxYRARsDoGDV1T99aF6GhYfRNDUmW17LuVmA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在执行了错误动作之后，Cradle 能够有效地通过反思来发现并且纠正错误。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427059" data-ratio="1.4933920704845816" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87njcTXfOkrjF8nyeibUMVlKjUnVDLicT1X2cWtoIHrLeD1Z01iaXCiblagNw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Cradle 不仅能从头开始跟随游戏指引生成相应技能，完成长达 40 分钟时的主线剧情，还能在开放世界自由探索，骑马，打猎，战斗，与 NPC 对话，使用道具，操作地图，甚至商店购物，均不在话下。这是首个能长时间游玩商业 3A 游戏的智能体。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427075" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nePKbttiaiaXwBfkQxeNMfE4UeZv0BBcSqiar1UiccAstTF8EdkLribib6nRA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427076" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nb8cJkMTbVXx2NGjPbQ3TGYj0OlOceHDnUOKVibm27foeO00HyPAtaHw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427077" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nicFnjjatKpYgC0A9riakWm7LYZyzyeuCpU0cCTLbH48GhAKmJiafOsUUQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427078" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n5K8uySbPlRXEfPdnKCuhynVAGSS4CaQg3lnShHQdvUzysua3bicNMQg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;">结束语</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">开源的 Cradle 代码可以很容易扩展到其他软件和游戏。研究团队表示，为了能够实现真正的通用计算机控制，后续 Cradle 还将移植到更多软件和游戏上，也鼓励相关研究团队 / 工业界开展进一步研究与探索。目标是让智能体可以与无论是开源还是闭源的所有软件进行交互并持续自我提升，实现通用性，最终成为通用人工智能诞生的摇篮。</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">"GCC is a cradle for AGI."&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;—The Cradle team</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>One more thing：Cradle 技术解读直播</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">3 月 14 日 14:30-15:30，论文一作新加坡南洋理工大学博士生谭伟豪进行线上解读报告。点击「阅读原文」报名或扫描下图二维码报名。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427062" data-ratio="1.777533039647577" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nLGo0G61d60wCybpNGx9OgCOnaiaf9BA6ib17HagP6iaUOHy0Y91IolzxQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: justify;margin-bottom: 0px;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p><a href="https://event.baai.ac.cn/activities/766">阅读原文</a>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[被误解的「中文版Sora」背后，字节跳动有哪些技术？]]></title>
        <id>2650910630_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910630&amp;idx=1&amp;sn=6ff041f26968b98f62f3eb4fe6b80be5&amp;chksm=84e46bd8b393e2ced5de69d15fc1761261ecd41dae13d930805d3a068ed76eaef09c39ce9bb0#rd"/>
        <updated>2024-03-12T04:10:57.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><strong>作者：蛋酱</strong></span></p></div></div></div></div></div><p style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><br style="outline: 0px;visibility: visible;"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">2024 开年，OpenAI 就在生成式 AI 领域扔下了重磅炸弹：Sora。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这几年，视频生成领域的技术迭代持续加速，很多科技公司也公布了相关技术进展和落地成果。在此之前，Pika、Runway 都曾推出过类似产品，但 Sora 放出的 Demo，显然以一己之力抬高了视频生成领域的标准。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今后的这场竞争中，哪家公司将率先打造出超越 Sora 的产品，仍是未知数。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">国内这边，目光聚集于一众科技大厂。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此前有消息称，字节跳动在 Sora 发布之前就研发出了一款名为 Boximator 的视频生成模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Boximator 提供了一种能够精确控制视频中物体的生成方法。用户无需编写复杂的文本提示，可以直接在参考图像中通过在物体周围画方框来选择目标，然后添加一些方框和线条来定义目标的结束位置或跨帧的整个运动路径，如下图所示：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426699" data-ratio="1" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5Wsqs8iax8wdJu52OpJUjqVwMbk0W1e4IfYa1dX3Mc0RlYU44d7Umt3g/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对此，字节跳动保持了低调的态度：相关人士回复媒体，Boximator 是视频生成领域控制对象运动的技术方法研究项目。目前还无法作为完善的产品落地，距离国外领先的视频生成模型在画面质量、保真率、视频时长等方面还有很大差距。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">在对应的技术论文介绍（<span style="color: rgb(123, 12, 0);">https://arxiv.org/abs/2402.01566</span>）中，我们也能看到，Boximator 是以插件的形式运行，可与现有的视频生成模型无缝集成，在保持视频质量的同时，增加运动控制功能。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">视频生成背后的技术涉及多个细分方向，与图像 / 视频理解、图像生成、超分辨率等技术都有关系。深挖之后，我们发现在众多分支领域，字节跳动已公开发表了一些研究成果。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇文章将介绍来自字节跳动智能创作团队的 9 项研究，涉及文生图、文生视频、图生视频、视频理解等多项最新成果。我们不妨从这些研究中，追踪探索视觉生成类模型的技术进展。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">关于视频生成，</span></strong><strong style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;"><span style="font-size: 16px;">字节有哪些成果？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今年 1 月上旬，字节跳动就发布过一个视频生成模型 MagicVideo-V2，一度引发社区热议。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426700" data-ratio="0.27685185185185185" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5jVLt4KVbYcn4yLMKMibbIG23vaK3PxnZic0iad0zUcw1a4sLEjlRn6OtQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2401.04468</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://magicvideov2.github.io/</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicVideo-V2 的创新在于将文本到图像模型、视频运动生成器、参考图像嵌入模块和帧插值模块集成到端到端视频生成 pipeline 中。得益于这一架构设计，MagicVideo-V2 在「审美」上能够保持着稳定的高水平表现，不仅生成美观的高分辨率视频，还兼具比较好的保真度和流畅度。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具体而言，研究者首先使用 T2I 模块创建一个 1024×1024 的图像，封装所描述的场景。随后，I2V 模块对该静态图像进行动画处理，生成 600×600×32 的帧序列，之前的潜在噪声确保了初始帧的连续性。V2V 模块将这些帧增强到 1048×1048 分辨率，同时完善视频内容。最后，插值模块将序列扩展到 94 个帧，得到 1048×1048 分辨率的视频，所生成视频具有较高的美学质量和时间平滑性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426701" data-ratio="0.6027777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5TKEabqn35fUibpP2NzdLBDeP56Rg3dwmHP4ovJAgM8nH5SmNeqcIdlA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者进行的大规模用户评估证明：MagicVideo-V2 比一些知名的 T2V 方法更受青睐（绿色、灰色和粉色条分别代表 MagicVideo-V2 被评为较好、相当或较差）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426705" data-ratio="0.46296296296296297" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG50MAyQGIKBjJOcj1PbV5wyr0ia0GNEkXfbeBFRnVRDeicl8BpaA1YblsQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426706" data-ratio="0.6712962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ozBavx1n1r9eds5h7V9qqe4MmPsqwaecuqGjUYlEgohcxwImGSQkjg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">高质量视频生成背后</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">统一视觉和语言学习的研究范式</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 MagicVideo-V2 的论文中，我们可以看出，视频生成技术的进展，离不开文生图、图生视频等 AIGC 技术的铺路。而生成高审美水准内容的基础在于理解，特别是模型对于视觉和语言两种模态学习、融合能力的进步。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">近年来，大语言模型的可扩展性和通用能力，催生出了统一视觉和语言学习的研究范式。为了跨越「视觉」和「语言」两种模态之间的天然鸿沟，研究者们将预训练好的大语言模型和视觉模型的表征连接起来，提取跨模态特性，完成如视觉问题解答、图像字幕、视觉知识推理和对话等任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这些方向上，字节跳动也有相关探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如，针对开放世界视觉任务中的多目标推理分割挑战，字节跳动联合北京交通大学、北京科技大学的研究者提出了高效像素级推理大模型 PixelLM，并将其开源。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426707" data-ratio="0.2657407407407407" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5zvbnueM6QY05fmzJUaN4EBg96RWHVfPxiapsfV0ymqDYe6MiaiaX2Qjcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：PixelLM:Pixel Reasoning with Large Multimodal Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.02228.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://pixellm.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 能够熟练地处理具有任意数量的开放集目标和不同推理复杂性的任务，下图展示了 PixelLM 在各种分割任务中生成高质量目标掩码的能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426708" data-ratio="0.5972222222222222" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5SVv18alibpVRaB37xtuU0d322YIWVurnm45rAEA46AQnNWgZ3eu8RQg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 的核心是一个新颖的像素解码器和一个分割 codebook：codebook 包含了可学习的 token，这些 token 编码了与不同视觉尺度目标参考相关的上下文和知识，像素解码器根据 codebook token 的隐藏嵌入和图像特征生成目标掩码。在保持 LMM 基本结构的同时，PixelLM 可以在没有额外的、昂贵的视觉分割模型的情况下生成高质量的掩码，从而提高了效率和向不同应用程序的可迁移性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426709" data-ratio="0.42962962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OTmDhz7tAQRlMfCriaTz7Y9w9aWsG2FVL6JURGSs23eicjRloibmncicmg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">值得关注的是，研究者构建了一个全面的多目标推理分割数据集 MUSE。他们从 LVIS 数据集中选取了共 910k 个高质量实例分割掩码以及基于图像内容的详细文本描述，利用这些构建了 246k 个问题 - 答案对。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">相比于图像，如果涉及视频内容，模型遭遇的挑战难度就又增加了不少。因为视频不仅包含丰富多变的视觉信息，还涉及时间序列的动态变化。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">现有的多模态大模型在处理视频内容时，通常将视频帧转化为一系列的视觉 token，并与语言 token 结合以生成文本。但随着生成文本长度的增加，视频内容的影响会逐渐减弱，导致生成的文本越来越多地偏离原视频内容，产生所谓的「幻觉」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">面对这一问题，字节跳动联合浙江大学提出了专门针对视频内容的复杂性设计的多模态大模型 Vista-LLaMA。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426710" data-ratio="0.1824074074074074" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5VtWmwwAIctKEUzZ8Fwx85m8HZtu9ic2jUyGE3NESujxhyOLiaq1bPVGA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Vista-LLaMA:Reliable Video Narrator via Equal Distance to Visual Tokens</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.08870.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://jinxxian.github.io/Vista-LLaMA/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Vista-LLaMA 采用了一种改良的注意力机制 —— 视觉等距离 token 注意力（EDVT），在处理视觉与文本 token 时去除了传统的相对位置编码，同时保留了文本与文本之间的相对位置编码。这种方法大幅提高了语言模型对视频内容的理解深度和准确性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">特别是，Vista-LLaMA 引入的序列化视觉投影器为视频中的时间序列分析问题提供了新的视角，它通过线性投影层编码视觉 token 的时间上下文，增强了模型对视频动态变化的理解能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426711" data-ratio="0.5481481481481482" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OaDTMNu8Hfvha9f7GTsQrFHNCSRIbtWaP7wBJuiaH0mdZVObf6KwYcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在最近被 ICLR 2024 接收的一项研究中，字节跳动的研究者还探讨了一种提升模型对视频内容学习能力的预训练方法。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于视频 - 文本训练语料的规模和质量有限，大多数视觉语言基础模型都采用图像 - 文本数据集进行预训练，并主要关注视觉语义表征建模，而忽略了时间语义表征和相关性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决这个问题，他们提出了 COSA，一种串联样本预训练视觉语言基础模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426712" data-ratio="0.43796296296296294" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ib5ahY8Tqe16OTfhSsTdJVic6k02NcrSvKGFWRwpPCdZOC0YcYaPguPA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：COSA: Concatenated Sample Pretrained Vision-Language Foundation Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2306.09085.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://github.com/TXH-mercury/COSA</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">COSA 仅使用图像 - 文本语料库对视觉内容和事件级时间线索进行联合建模。研究者将多个图像 - 文本对按顺序串联起来，作为预训练的输入。这种转换能有效地将现有的图像 - 文本语料库转换成伪长格式视频 - 段落语料库，从而实现更丰富的场景转换和明确的事件 - 描述对应关系。实验证明，COSA 能够持续提高各种下游任务的性能，包括长 / 短视频 - 文本任务和图像 - 文本任务（如检索、字幕和问题解答）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426713" data-ratio="0.6305555555555555" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG50G7Orb0e6gHgaqQmjx5CEDKcibb3F4P6R1T274DPNpuGh8NERlWJpHA/640?wx_fmt=png&amp;from=appmsg"></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426714" data-ratio="0.7157407407407408" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5pUc1p7JqV5peicYoxBLiaFahpDggPPIeDjVfPW85Kgw3DXkMobmjb5ibg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">从图像到视频</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">被重新认识的「扩散模型」</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在视觉 - 语言模型之外，扩散模型同样是大部分视频生成模型采用的技术。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">通过在大量图像 - 文本配对数据集上进行严格训练，扩散模型能够完全根据文本信息生成细节丰富的图像。除了图片生成，扩散模型还可用于音频生成、时间序列生成、3D 点云生成等等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如在一些短视频应用中，用户只需要提供一张图片，就能生成一段以假乱真的动作视频。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">数百年来保持神秘微笑的蒙娜丽莎，都能马上跑起来：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426715" data-ratio="0.340129749768304" data-s="300,640" data-type="gif" data-w="1079" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5YGBUu20483zxUbDuOVDVXefpHGFFdeHMuw00gWB9Nn57BKcJM03tGg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这项有趣应用背后的技术，是新加坡国立大学和字节跳动的研究者联合推出的「MagicAnimate」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 是一个基于扩散的人类图像动画框架，在根据特定的运动序列生成视频的任务中，能够很好地保证整个动画的时间一致性并提升动画保真度。而且，MagicAnimate 项目是开源的。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426716" data-ratio="0.30462962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57T0nyfyXZYeACstbcLqmWlsmOCX2kKK07h9ZDuoeeLjagEGvUogdEQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicAnimate:Temporally Consistent Human Image Animation using Diffusion Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2311.16498.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://showlab.github.io/magicanimate/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决生成动画普遍存在的「闪烁」问题，研究者通过将时间注意力（temporal attention）块合并到扩散主干网络中，来构建用于时间建模的视频扩散模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 将整个视频分解为重叠的片段，并简单地对重叠帧的预测进行平均。最后，研究者还引入图像 - 视频联合训练策略，以进一步增强参考图像保留能力和单帧保真度。虽然仅接受了真实人类数据的训练，MagicAnimate 却展现出了泛化到各种应用场景的能力，包括对未见过的领域数据进行动画处理、与文本 - 图像扩散模型的集成以及多人动画等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426717" data-ratio="0.4287037037037037" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5HEYz6EbqbpkEMIjm5BRhaciaRZcKqXRX1KoDFXp0N9GubpE2lS0l7NA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">另一项基于扩散模型思想的研究「DREAM-Talk」，则解决了从单张肖像图像生成会说话的情绪化人脸的任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426718" data-ratio="0.2361111111111111" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5EicnXYGAw14p2LJDc8WHs4kQiaVmGOGic6CCT7BYwlEWYKKRS6y0CJ65w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：DREAM-Talk:Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.13578.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://dreamtalkemo.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们知道，在这项任务中，很难同时实现富有表现力的情感对话和准确的唇语同步，通常为了保证唇语同步的准确性，表现力往往会大打折扣。&nbsp;</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">「DREAM-Talk」是一个基于扩散的音频驱动框架，分为两个阶段：首先，研究者提出了一个新颖的扩散模块 EmoDiff，可根据音频和参考情绪风格生成多种高度动态的情绪表情和头部姿势。鉴于唇部动作与音频之间的强相关性，研究者随后利用音频特征和情感风格对动态进行了改进，从而提高了唇部同步的准确性，此外还部署了一个视频到视频渲染模块，实现了将表情和唇部动作转移到任意肖像。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从效果上看，DREAM-Talk 在表现力、唇部同步准确性和感知质量方面的确不错：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426719" data-ratio="0.6333333333333333" data-s="300,640" data-type="png" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ia69w7QzS5gQQic1hgVIJicdzaF6GHqxC148zxibGmd4dHicoDbexcBSTQQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但不管是图像生成还是视频生成，当前基于扩散模型路线的研究都还有一些基础挑战需要解决。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如很多人关心生成内容的质量问题（对应 SAG、DREAM-Talk），这可能与扩散模型的生成过程中的一些步骤有关，比如引导采样。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">扩散模型中的引导采样大致可分为两类：需要训练的和无需训练的。免训练引导采样是利用现成的预训练网络（如美学评估模型）来引导生成过程，旨在以更少的步骤和更高的精度从预训练的模型中获取知识。当前的训练无指导采样算法基于对干净图像的一步估计来获得指导能量函数。然而，由于预训练网络是针对干净图像进行训练的，因此干净图像的一步估计过程可能不准确，尤其是在扩散模型的早期阶段，导致早期时间步骤的指导不准确。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">针对该问题，字节跳动和新加坡国立大学的研究者共同提出了 Symplectic Adjoint Guidance (SAG)。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426720" data-ratio="0.3592592592592593" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5yW6kEBibOQsCBJicryLHkM8ibBCqtXehF5vWibJWbImHehsvScFOYqYyuA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.12030.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">SAG 通过两个内阶段计算梯度引导：首先，SAG 通过 n 个函数调用估计干净图像，其中 n 作为一个灵活的参数，可以根据特定的图像质量要求进行调整。其次，SAG 使用对称偶方法精确高效地获得关于内存需求的梯度。这种方法可支持各种图像和视频生成任务，包括风格引导图像生成、美学改进和视频风格化，并有效提升了生成内容的质量。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最近入选 ICLR 2024 的一篇论文，则着重讨论了「扩散概率模型梯度反向传播的临界灵敏度方法」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426721" data-ratio="0.6240740740740741" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5QhkTtVM6kiaiazFO7FjBp9yJib36RQaQxIVuhunRNM1aElcoib6dnXIx6w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文标题：Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2307.10711.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于扩散概率模型的采样过程涉及对去噪 U-Net 的递归调用，因此 naïve 梯度反向传播需要存储所有迭代的中间状态，从而导致极高的内存消耗。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这篇论文中，研究者提出的 AdjointDPM 首先通过求解相应的概率流 ODE 从扩散模型中生成新样本。然后，通过求解另一个增强的 ODE，使用邻接灵敏度方法反向传播模型参数（包括调节信号、网络权重和初始噪声）损失的梯度。为了减少前向生成和梯度反向传播过程中的数值误差，研究者使用指数积分进一步将概率流 ODE 和增强型 ODE 重新参数化为简单的非刚性 ODE。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者指出，AdjointDPM 在三个任务中极具价值：将视觉效果转换为识别文本嵌入、针对特定类型的风格化对扩散概率模型进行微调，以及优化初始噪声以生成用于安全审计的对抗样本，以减少优化工作中的成本。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对于视觉类的感知任务，采用文本到图像的扩散模型作为特征提取器的方法也受到越来越多的关注。在这一方向上，字节跳动的研究者在论文中提出了一种简单而有效的方案。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426722" data-ratio="0.25277777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5gucl3XWVs3zIPcFf14Wkx9rgdtOJFcfNMwk0NyFyLCSAbnb5UfsUsw/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题；Harnessing Diffusion Models for Visual Perception with Meta Prompts</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.14733.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇论文的核心创新是在预训练的扩散模型中引入可学习的嵌入（元提示）以提取感知特征，不依赖额外的多模态模型来生成图像标题，也不使用数据集中的类别标签。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">元提示有两方面的作用：首先，作为 T2I 模型中文本嵌入的直接替代物，它可以在特征提取过程中激活与任务相关的特征；其次，它将用于重新排列提取的特征，以确保模型专注于与手头任务最相关的特征。此外，研究者还设计了一种循环细化训练策略，充分利用扩散模型的特性，从而获得更强的视觉特征。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">「中文版 Sora」诞生之前</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">还有多远的路要走？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这几篇新论文中，我们已经了解到字节跳动这样的国内科技公司，在视频生成技术上的一系列积极的探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但是与 Sora 相比，无论是字节跳动，还是 AI 视频生成领域的一众明星公司，都存在肉眼可见的差距。Sora 的优势建立在对 Scaling Law 的信仰和突破性的技术创新上：通过 patchs 统一视频数据，依托 Diffusion Transformer 等技术架构和 DALL・E 3 的语义理解能力，真正做到了「遥遥领先」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 2022 年文生图的大爆发，到 2024 年 Sora 的横空出世，人工智能领域的技术迭代速度，已经超过了大家的想象。2024 年，相信这一领域还会出现更多的「爆款」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">字节显然也在加紧投入技术研发。近期，谷歌 VideoPoet 项目负责人蒋路，开源多模态大模型 LLaVA 团队成员之一、前微软研究院首席研究员 Chunyuan Li 均被曝出已加入字节跳动智能创作团队。该团队还在大力招聘，官网上已放出多个大模型算法相关岗位。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不仅仅是字节，BAT 等老牌巨头也放出众多令人瞩目的视频生成研究成果，一众大模型创业公司更是极具冲劲。文生视频技术又将出现哪些新的突破？我们拭目以待。</span></p><p><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[前端不存在了？盲测64%的人更喜欢GPT-4V的设计，杨笛一等团队新作]]></title>
        <id>2650910318_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910318&amp;idx=1&amp;sn=79915af15817ce7b9c5ccbf45d6e662f&amp;chksm=84e46a10b393e3060f6ef83e9dab5fdcc9c1750115c4f79929af8a14275c35f43944bbdd7081#rd"/>
        <updated>2024-03-11T04:10:56.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：Panda</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="14" data-source-title=""><div class="js_blockquote_digest"><p>前端工程师是不是开始慌了？</p></div></blockquote><p><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">3 月 9 日央视的一档节目上，百度创始人、董事长兼 CEO 李彦宏指出，以后不会存在「程序员」这种职业了，因为只要会说话，人人都会具备程序员的能力。「未来的编程语言只会剩下两种，一种叫做英文，一种叫做中文。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426623" data-ratio="0.562962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5l0KeweYovnibpvA2k0NwknH9vQfLt9xhPmWChCicU6aFqiarjZQQdwqBA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">自大模型技术突破以来，越来越多的行业拥有了自动化的趋势，这其中进度最快的领域似乎是软件开发本身。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">根据你的自然语言指令，ChatGPT 这样的工具可以和你边聊边生成代码，结果逐渐靠谱且速度很快。在最近多模态技术进步以后，甚至截个图让 AI 自行领会意图也能生成你想要的设计：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426624" data-ratio="0.6972222222222222" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5fKM8WnLFbWAFBtzkvpBH0Rn4ptLl1R8xI7MD9UjibnhlZcSKlcIIKTA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这种方法是装装样子还是来真的？AI 距离「替代程序员」还有多远？有研究告诉我们：已经很可怕了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;"><span style="display: none;line-height: 0px;">‍</span>我们离自动化前端工程还有多远？</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">将视觉设计实现成执行功能的代码是一项颇具挑战性的任务，因为这需要理解视觉元素和它们的布局，然后将它们翻译成结构化的代码。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这个过程需要复杂的技能，也因此让很多普通人无法构建自己的网络应用，即便他们已经有了非常具体的构建或设计思路。不仅如此，由于这个过程需要不同领域的专业知识，因此往往需要具备不同技能的人互相合作，这就会让整个网页构建过程更加复杂，甚至可能导致目标设计与实际实现之间出现偏差。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">如果能基于视觉设计有效地自动生成功能性代码，那么势必有望实现前端网页应用开发的大众化，也就是让非专家人士也能轻松快捷地构建应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近些年，基于自然语言的代码生成领域发展迅速，但少有人研究基于用户界面（UI）设计来自动生成代码实现，原因包括用户界面存在多样化的视觉和文本信号、结果代码的搜索空间巨大等。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最近，多模态 LLM 进入了新的发展时代，大规模预训练模型可以针对多种基于视觉的任务通过处理视觉和文本输入来生成文本输出，其中代表性的模型包括 Flamingo、GPT-4V 和 Gemini。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这样的进展为上述任务带来了全新的解决方案范式：取一张用户网站设计的截图并将其提供给系统，就能得到完整的代码实现，然后这些代码又可以被渲染成用户想要的网页。整个过程是完全端到端式的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近日，斯坦福大学、佐治亚理工学院等机构的一个联合团队评估了当前的多模态模型在这一任务上的表现。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426625" data-ratio="0.44814814814814813" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5GCKeI78rQWZ4PrFxWfNMehP6aicTKEG8zy2q0lcPZKBHUPxQyP9L6ibw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Design2Code: How Far Are We From Automating Front-End Engineering?</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.03163.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://salt-nlp.github.io/Design2Code/</span><span style="font-size: 15px;"></span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们将这个任务称为 Design2Code。通过一系列的基准评测，我们可以从这些结果中了解自动化前端工程已经发展到哪一步了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了实现系统化和严格的基准评测，该团队为 Design2Code 任务构建了首个真实世界基准。表 1 给出了一些示例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426626" data-ratio="0.7787037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5FRUOWyrwI02A21t4o6QjKo8TSjklNnTNMtIEKMnWFSwO1I3w9MCSsg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了最好地反映真实用例，他们使用了真实世界的网页，而非用生成方法得到合成网页。他们收集了 C4 验证集中的网页，并对所有样本进行了仔细的人工调整，最终得到了 484 个高质量、高难度和多样化的网页。它们可代表不同复杂度的多种真实世界用例。他们执行了定性和定量分析，证明这个基准数据集覆盖了广泛的 HTML 标签用法、领域和复杂度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此外，为了促进高效的评估和模型开发，该团队还为这个任务开发了一些评估指标 —— 可自动比较生成网页的截图与给定的截图输入。这些新指标考虑的维度很全面，包括边界框匹配、文本内容、位置和所有已匹配视觉元素的颜色。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">然后，该团队调查了 GPT-4V 和 Gemini 等当前的多模态 LLM 在这一任务上的表现。为了让这些模型能展现出自己的最优能力，该团队使用了一些不同的 prompt 设计方案，包括文本增强式 prompt 设计和自我修正式 prompt 设计。其中文本增强式 prompt 设计是为视觉输入提供文本元素作为补充，从而可以降低光学字符识别（OCR）的任务负载；自我修正式 prompt 设计则是让模型比较之前的生成结果与输入的网页截图，让其自我改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者发现，在 GPT-4V 和 Gemini Pro 上，相比于使用直接 prompt 设计法，文本增强式 prompt 设计都能带来提升，但自我修正式方法只能为 GPT-4V 带来积极影响。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">尽管这些商用模型的表现是当前最佳的，但它们都是缺乏透明度的黑箱。因此，该团队还为这一任务贡献了一个开源的 18B 参数的已微调模型：Design2Code-18B。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">具体来说，该模型基于当前最佳的开源模型 CogAgent 构建，并使用合成的 Design2Code 数据进行了微调。令人惊讶的是，在新提出的基准上，尽管合成的训练数据与真实的测试数据之间存在差异，但这个「小型」开源模型的表现依然颇具竞争力 —— 足以媲美 Gemini Pro Vision。这说明专用型的「小型」开放模型是有发展潜力的，并且模型也可以从合成数据中学习获取技能。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">Design2Code</span></strong><strong><span style="font-size: 16px;"> 基准</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到基准数据，该团队首先收集了 C4 验证集中的所有网站链接。然后他们将所有 CSS 代码嵌入到了 HTML 文件中，从而让每个网页都只有一个代码实现文件。这样得到了共计 12.79 万个网页。然后他们又执行了进一步的过滤和处理，包括自动调整和人工调节。最终他们得到了包含 484 个测试样本的基准。下表 1 比较了新提出的 Design2Code 与 Huggingface 的 WebSight 数据集。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426627" data-ratio="0.37777777777777777" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5kBvb3KhwVFAxTS1OWk1qyQGNqtg9GBxpfHv9Yoe0lGicboTyU1S77kw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">图 2 总结了 Design2Code 的主要主题。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426628" data-ratio="1.0699815837937385" data-type="png" data-w="543" style="width: 308px;height: 330px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57Xdjz1S1pPSakictPMPBH2ohf9m6SaqXqSeY5bKSGqnrfOf6D18sMEA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">至于评估指标，该团队提出了一种高层级的视觉相似度指标，即比较参考网页和生成网页的相似度。另外他们还使用了一组低层级的元素匹配指标，包括块元素、位置、文本和颜色等的匹配程度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">结果自动评估和人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">表 2 和图 3 给出了自动评估的结果。请注意，这里的比较并不是公平的，因为不同模型有不同的模型大小和训练数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426633" data-ratio="0.5768518518518518" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5CW5Hbsj3w9gnL7GzM32pXxCWrFocYb3GMwkENctibxPGPm7Hk4tic9Pw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426634" data-ratio="0.9929577464788732" data-type="png" data-w="568" style="width: 278px;height: 276px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5N8x1qQ9HEdiaEylxrp8ialFw9WVK9KO7BDuOyUAYDMEUN59QS4IibLMJQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">可以观察到：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 在颜色之外的所有维度上都表现最好，而在颜色维度上领先的是 WebSight VLM-8B。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于 GPT-4V 和 Gemini Pro Vision，文本增强式 prompt 设计均可以成功提升块元素匹配分数和文本相似度分数，这说明提供提取出的文本元素是有用的。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对 GPT-4V 而言，自我修正式 prompt 设计可以为块元素匹配和位置相似度带来少量提升，但对 Gemini Pro Vision 来说却并无提升。可能的原因是：在没有外部反馈的前提下，LLM 执行内部自我校正的能力有限。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">通过比较 Design2Code-18B 和基础版本的 CogAgent-18B，可以看出微调能为所有维度带来显著提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">相比于 WebSight VLM-8B，该团队微调得到的 Design2Code-18B 在块元素匹配和文本相似度指标上表现更好，但在位置相似度和颜色相似度指标上表现更差。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队表示，前两个观察可以归因于更强更大的基础模型，而后两个则可归功于更大量的微调数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也进行了人类评估。下面是主要的评估协议和结果。每一个问题都由 5 位人类标注者给出评估意见，最终结果遵从多数意见。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">成对模型比较：也就是让标注者给一对生成的网页排名（一个来自基线方法，另一个来自受测方法），以决定哪一个与参考网页更相似。这里的基线是对 Gemini Pro Vision 采用直接 prompt 设计，收集的数据是其它七种方法与这种基线方法的胜 / 平 / 负的比例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426635" data-ratio="0.587037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG55FQge4TFcozW0iac3wbRvoMuG7DuLRwcNnSwFRtDR9lKdibC2ibXcUyOA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果见图 4，可以看出：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 显著优于其它基线，而且文本增强式 prompt 设计和自我修正式 prompt 设计能在直接 prompt 设计的基础上进一步提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文本增强式 prompt 设计可以少量提升 Gemini，但进一步增加自我修正方法却没有帮助。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WebSight VLM-8B 优于 Gemini 直接 prompt 设计方法（54% 的胜率和 35% 的败率），这说明在大量数据上进行微调可以在特定领域比肩商用模型。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">新模型 Design2Code-18B 的表现与 Gemini Pro Vision 直接 prompt 设计方法相当（38% 的胜率和 37% 的败率）。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">直接评估：尽管有这些比较，但读者可能还是会问：「我们离自动化前端工程还有多远？」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到一个更直观的答案，该团队进一步让人类标注者比较了参考网页与最佳的 AI 生成网页（使用了 GPT-4V 自我修正式 prompt 设计）。他们从两个方面进行了直接评估：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1.AI 生成的网页能否替代原始网页？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">人类标注者认为：AI 生成的网页中，49% 可与参考网页互换。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2. 参考网页和 AI 生成的网页哪个更好？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果有点出人意料：在 64% 的案例中，人类标注者更偏爱 GPT-4V 生成的网页，也就是说他们认为 AI 生成的网页比原始参考图像的设计更好！</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估 vs 人类评估</span></strong><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也研究了自动指标与人类配对偏好之间的相关性。结果发现，人类通常更关注高层级的视觉效果和布局，而不是细节内容，这说明人类的思考方式是自上而下的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，针对论文给出的结果，有人提出了不同意见，认为前端的工作流程远比表面看上去复杂，因此真正实现「自动化前端工程」还需要一段时间。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426636" data-ratio="0.6222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5IicWnRN4I4GopXnYKapwLPzKiaZpZpzZibLiaa67a7WXQ7Cf503MuaTJzg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426637" data-ratio="0.25462962962962965" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ibSYgvvIrlFsOUdBhwGIticKYia4DrUviaOQIbyIKQgQPG7gzxXvtzpH0w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于这个问题，你怎么看？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426638" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;color: rgba(0, 0, 0, 0.9);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于DiT，支持4K图像生成，华为诺亚0.6B文生图模型PixArt-Σ来了]]></title>
        <id>2650910168_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910168&amp;idx=1&amp;sn=28f0f0028fab54f86aaf9f0fcf897b4e&amp;chksm=84e469a6b393e0b0c2848c926abaf61a938e02275aff189145a170625ddb14580c0df12639ff#rd"/>
        <updated>2024-03-10T04:25:41.000Z</updated>
        <summary type="html"><![CDATA[<p style="text-align: justify;line-height: 1.75em;"><br></p><div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="20" data-source-title=""><div class="js_blockquote_digest"><div><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">这个模型和 Sora 一样采用了 DiT 框架。</span></p></div></div></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">众所周知，开发顶级的文生图（T2I）模型需要大量资源，因此资源有限的个人研究者基本都不可能承担得起，这也成为了 AIGC（人工智能内容生成）社区创新的一大阻碍。同时随着时间的推移，AIGC 社区又能获得持续更新的、更高质量的数据集和更先进的算法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">于是关键的问题来了：我们能以怎样的方式将这些新元素高效地整合进现有模型，依托有限的资源让模型变得更强大？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了探索这个问题，华为诺亚方舟实验室等研究机构的一个研究团队提出一种新的训练方法：由弱到强式训练（weak-to-strong training）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426500" data-ratio="0.2962962962962963" data-s="300,640" data-type="jpeg" data-w="972" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMBfyOSF62KuGyoS0peibtUgro2SrevibcUia01tgtxkvMZuYiahSIE04f5g/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.04692.pdf</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目页面：https://pixart-alpha.github.io/PixArt-sigma-project/</span><span style="font-size: 15px;color: rgb(123, 12, 0);"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">他们的研究基于他们去年十月提出的一种高效的文生图训练方法 PixArt-α，参阅机器之心报道《<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650893566&amp;idx=5&amp;sn=410500c06a19922d2e473e937826647e&amp;chksm=84e4a880b3932196af9191f8fcc9de7204213c8983ee90df627d3fe2fa19b978e6018cfd472f&amp;scene=21#wechat_redirect" textvalue="超低训练成本文生图模型 PixArt 来了，效果媲美 MJ，只需 SD 10% 训练时间" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">超低训练成本文生图模型 PixArt 来了，效果媲美 MJ，只需 SD 10% 训练时间</a>》。PixArt-α 是 DiT（扩散 Transformer）框架的一种早期尝试。而现在，随着 Sora 登上热搜以及 Stable Diffusion 层出不穷的应用，DiT 架构的有效性得到了研究社区越来越多工作的验证，例如 PixArt, Dit-3D, GenTron 等「1」。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队使用 PixArt-α 的预训练基础模型，通过整合高级元素以促进其持续提升，最终得到了一个更加强大的模型 PixArt-Σ。图 1 展示了一些生成结果示例。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426501" data-ratio="1.4834054834054835" data-s="300,640" data-type="jpeg" data-w="693" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMVbHsNQpEiaVJWUbmiaicvhZhgrUDiceGVjr11Pft15HNsC8ibYO8kVtWTBw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: center;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">PixArt-Σ 如何炼成？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">具体来说，为了实现由弱到强式训练，造出 PixArt-Σ，该团队采用了以下改进措施。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">更高质量的训练数据</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队收集了一个高质量数据集 Internal-Σ，其主要关注两个方面：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(1) 高质量图像：该数据集包含 3300 万张来自互联网的高分辨率图像，全都超过 1K 分辨率，包括 230 万张分辨率大约为 4K 的图像。这些图像的主要特点是美观度高并且涵盖广泛的艺术风格。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(2) 密集且准确的描述：为了给上述图像提供更精准和详细的描述，该团队将 PixArt-α 中使用的 LLaVA 替换成了一种更强大的图像描述器 Share-Captioner。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">不仅如此，为了提升模型对齐文本概念和视觉概念的能力，该团队将文本编码器（即 Flan-T5）的 token 长度扩展到了大约 300 词。他们观察到，这些改进可以有效消除模型产生幻觉的倾向，实现更高质量的文本 - 图像对齐。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">下表 1 展示了不同数据集的统计数据。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426502" data-ratio="0.36830601092896176" data-s="300,640" data-type="jpeg" data-w="915" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMiceqCreO9JMjaT91umTpzkfBakV7C8osicibgmYJeRgkL4ibb2u1H0X8Cg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">高效的 token 压缩</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了增强 PixArt-α，该团队将其生成分辨率从 1K 提升到了 4K。为了生成超高分辨率（如 2K/4K）的图像，token 数量会大幅增长，这就会导致计算需求大幅增长。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了解决这一难题，他们引入了一种专门针对 DiT 框架调整过的自注意力模块，其中使用了键和值 token 压缩。具体来说，他们使用了步长为 2 的分组卷积来执行键和值的局部聚合，如下图 7 所示。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426503" data-ratio="0.7302325581395349" data-s="300,640" data-type="jpeg" data-w="645" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMnibNWvAp0rcFK5ibdIRwe67PJd85rf6yPadQ2iag1R1iaib0wfbmUxmibmfA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，该团队还采用了一种专门设计的权重初始化方案，可在不使用 KV（键 - 值）压缩的前提下从预训练模型实现平滑适应。这一设计可有效将高分辨率图像生成的训练和推理时间降低大约 34%。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">由弱到强式训练策略</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队提出了多种微调技术，可快速高效地将弱模型调整为强模型。其中包括：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(1) 替换使用了一种更强大的变分自动编码器（VAE）：将 PixArt-α 的 VAE 替换成了 SDXL 的 VAE。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(2) 从低分辨率到高分辨率扩展，这个过程为了应对性能下降的问题，他们使用了位置嵌入（PE）插值方法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(3) 从不使用 KV 压缩的模型演进为使用 KV 压缩的模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">实验结果验证了由弱到强式训练方法的可行性和有效性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通过上述改进，PixArt-Σ 能以尽可能低的训练成本和尽可能少的模型参数生成高质量的 4K 分辨率图像。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">具体来说，通过从一个已经预训练的模型开始微调，该团队仅额外使用 PixArt-α 所需的 9% 的 GPU 时间，就得到了能生成 1K 高分辨率图像的模型。如此表现非常出色，因为其中还替换使用了新的训练数据和更强大的 VAE。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，PixArt-Σ 的参数量也只有 0.6B，相较之下，SDXL 和 SD Cascade 的参数量分别为 2.6B 和 5.1B。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">PixArt-Σ 生成的图像的美观程度足以比肩当前最顶级的文生图产品，比如 DALL・E 3 和 MJV6。此外，PixArt-Σ 还展现出了与文本 prompt 细粒度对齐的卓越能力。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br>图 2 展示了一张 PixArt-Σ 生成 4K 高分辨率图像的结果，可以看到生成结果很好地遵从了复杂且信息密集的文本指令。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426504" data-ratio="0.7122302158273381" data-s="300,640" data-type="jpeg" data-w="973" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMbjOEna6BQGKRLM0FPF8rXwOnia8xjBpaqwVLXR7rtmia5ziaAWOYaxiauQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">实验</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">实现细节</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">训练细节：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">对于执行条件特征提取的文本编码器，该团队按照 Imagen 和 PixArt-α 的做法使用了 T5 的编码器（即 Flan-T5-XXL）。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">基础扩散模型就是 PixArt-α。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">不同于大多数研究提取固定的 77 个文本 token 的做法，这里将文本 token 的长度从 PixArt-α 的 120 提升到了 300，因为 Internal-Σ 中整理的描述信息更加密集，可以提供高细粒度的细节。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">另外 VAE 使用了来自 SDXL 的已预训练的冻结版 VAE。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">其它实现细节与 PixArt-α 一样。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">模型是基于 PixArt-α 的 256px 预训练检查点开始微调的，并使用了位置嵌入插值技术。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">最终的模型（包括 1K 分辨率）是在 32 块 V100 GPU 上训练的。他们还额外使用了 16 块 A100 GPU 来训练 2K 和 4K 图像生成模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">评估指标：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">为了更好地展示美观度和语义能力，该团队收集了 3 万对高质量文本 - 图像，以对最强大的文生图模型进行基准评估。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">这里主要是通过人类和 AI 偏好来评估 PixArt-Σ，因为 FID 指标可能无法适当地反映生成质量。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">性能比较</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">图像质量评估：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">该团队定性地比较了 PixArt-Σ 与闭源文生图（T2I）产品和开源模型的生成质量。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">如图 3 所示，相比于开源模型 SDXL 和该团队之前的 PixArt-α，PixArt-Σ 生成的人像的真实感更高，并且也有更好的语义分析能力。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">与 SDXL 相比，PixArt-Σ 能更好地遵从用户指令。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426505" data-ratio="1.2919605077574048" data-s="300,640" data-type="jpeg" data-w="709" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHM18VG6IxibczibaeiaibnjJlY0MOL1JiaIYongZ4MicECYzCLoKMBOC2IDTIw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">PixArt-Σ 不仅优于开源模</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">型，而且与当前的闭源产品相比也颇具竞争力，如图 4 所示。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426507" data-ratio="1.1418539325842696" data-s="300,640" data-type="jpeg" data-w="712" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMjsHcBhiayT2QPcC6TO17mXGNRVk6zWyM9VrgzBFjuXLibZXY6P2tUXDA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">生成高分辨率图像：</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">新方法可以直接生成 4K 分辨率的图像，而无需任何后处理。</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">此外，PixArt-Σ 也能准确遵从用户提供的复杂和详细的长文本。</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">因此，用户无需费心去设计 prompt 也能得到让人满意的结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">人类 / AI（GPT-4V）偏好研究：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">该团队也研究了人类和 AI 对生成结果的偏好。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">他们收集了 6 个开源模型的生成结果，包括 PixArt-α、PixArt-Σ、SD1.5、Stable Turbo、Stable XL、Stable Cascade 和 Playground-V2.0。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">他们开发了一个网站，可通过展现 prompt 和对应的图像来收集人类偏好反馈。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">人类评估者可根据生成质量以及与 prompt 的匹配程度来给图像排名。结果见图 9 的蓝色条形图。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">可以看出人类评估者对 PixArt-Σ 的喜爱胜过其它 6 个生成器。相比于之前的文生图扩散模型，如 SDXL（2.6B 参数）和 SD Cascade（5.1B 参数），PixArt-Σ 能以少得多的参数（0.6B）生成质量更高且更符合用户 prompt 的图像。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426508" data-ratio="0.8250904704463209" data-s="300,640" data-type="jpeg" data-w="829" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHM2FpZeuIwXeSE5ZqVz670NoiaU4GY4pI2clqg5lgcS8eD6DlcWTcSTtA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，该团队还使用了先进的多模态模型 GPT-4 Vision 来执行 AI 偏好研究。他们的做法是给 GPT-4 Vision 提供两张图像，让它基于图像质量和图像 - 文本对齐程度进行投票。结果见图 9 中的橙色和绿色条形图，可以看到情况与人类评估基本一致。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队也进行了消融研究来验证各种改进措施的有效性。更多详情，请访问原论文。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 15px;">参考文章：1.<span style="color: rgb(136, 136, 136);font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">https://www.shoufachen.com/Awesome-Diffusion-Transformers/</span></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><br></span></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426509" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[奥特曼重返OpenAI董事会：看完3万份文件，调查组认定了]]></title>
        <id>2650910146_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910146&amp;idx=1&amp;sn=21ded1f8772cef5bb40780e9bdf2bfba&amp;chksm=84e469bcb393e0aa49d0160edff7024d3ef051db981656f441435614ac1c8d438ecb5e89d180#rd"/>
        <updated>2024-03-09T03:23:37.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：小舟、泽南</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="9" data-source-title=""><div class="js_blockquote_digest"><p>过山车一样的剧情。</p></div></blockquote><p><span style="font-size: 15px;letter-spacing: 0.034em;">特别独立调查委员会发现，在去年 OpenAI 管理层动荡时，首席执行官萨姆・奥特曼（Sam Altman）的行为「不构成强制解雇」，现在他重新加入董事会了。</span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">历时超过 110 天，OpenAI 的宫斗剧现在迎来了盖棺定论的时刻。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426472" data-ratio="0.9782383419689119" data-s="300,640" data-type="jpeg" data-w="965" style="width: 558px;height: 546px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a381Rb4rWia6o9equk3QnAPgaattdFFfnDeyMLtiauC5p1qaIHE8fmXtVg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">没有通用人工智能危机，也和神秘的技术突破 Q* 无关，国际律师事务所 WilmerHale 在大量调查之后认定，这次动荡的原因在于董事会成员之间关系破裂。现在，奥特曼和 Greg Brockman 是「OpenAI 的正确领导者」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426471" data-ratio="0.575" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3opPpTWnHhSfLxHcew5v1rNntqVyqo7U2ejKZABY8YvZkibicic00oUnIA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在发布消息与记者通话时，奥特曼坐在 Greg Brockman 旁边，显得很高兴。不过在采访中他也被问及 OpenAI 联合创始人兼首席科学家 Ilya Sutskever 现在的情况，后者被认为在失败的政变中发挥了关键作用 —— 先是支持开除奥特曼，但当大多数 OpenAI 员工威胁如果奥特曼不回来就辞职时，他改变了立场。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在去年的宫斗发生后，Sutskever 便不再频繁向外界发声，这引发了人们对他未来参与公司事务的质疑。奥特曼在电话会议上表示：「没什么可宣布的，但 Ilya 太棒了…… 我希望我们在余下的职业生涯中共同努力。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">奥特曼表示，最近那些「让我们互相对抗的困难」没有破坏团队，他「很高兴整件事都结束了」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">曾经三天三换 CEO，现在<span style="font-size: 15px;letter-spacing: 0.51px;text-wrap: wrap;">「走在正确道路上」</span>，看到 OpenAI 的管理回到正轨，人们纷纷表示欢迎：已经迫不及待想用上 GPT-5 了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426473" data-ratio="0.7948717948717948" data-s="300,640" data-type="jpeg" data-w="975" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3HVNgNzkI1aXoiaoRD6bH4rGy258YoSzyuZxJMqsNdRicmbjKII5MJkibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过在极客和创业者扎堆的 hackernews 上也有人表示，这虽说不是坏事，但也很难评，他们就是宫斗赢了：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426474" data-ratio="0.38055555555555554" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3Okz3glP08JpQGTgfdZJSCpZ6P69SbktOziceEUXCuM52RkKMB1Q27wA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">的确，在奥特曼回归后，之前罢免他的董事已被纷纷解雇。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 在今天早晨发表了一份官方声明，简单介绍了其邀请的独立调查机构得出的结论。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426475" data-ratio="0.26851851851851855" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3oZcA9mrtIXXqqYibetfIatc13ibMGlkYFWTkI6YgR59VtUkNoMfCq3RA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，除奥特曼之外，OpenAI 其他新一届董事会成员包括：Sue Desmond-Hellmann、Nicole Seligman 和 Fidji Simo。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们将与 Quora 首席执行官 Adam D'Angelo、美国前财政部长拉里・萨默斯（Larry Summers）以及 Salesforce 前联合首席执行官、董事长布雷特・泰勒（Bret Taylor）共同监督公司事务。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 董事会特别委员会今天宣布完成由美国著名律师事务所 WilmerHale 执行的审查，该审查对 OpenAI 前任董事会成员、OpenAI 高管、前任董事会顾问以及其他相关证人进行了数十次询问；审查了 30000 多份文件，并评估了各种企业行为。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">根据 WilmerHale 制定的记录并根据特别委员会的建议，董事会对山姆·奥特曼和 Greg Brockman 持续领导 OpenAI 表示充分信任。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 董事会主席 Bret Taylor 表示：「我们一致认为 Sam 和 Greg 是 OpenAI 的合适领导者。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">审查的结果意味着：奥特曼作为首席执行官将重新加入 OpenAI 董事会。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">同时，OpenAI 董事会宣布了以下几点新举措：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">采用一套新的公司治理准则；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">优化 OpenAI 的利益冲突政策；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">创建举报热线，作为所有 OpenAI 员工和承包商的匿名举报资源；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">创建额外的董事会委员会，包括专注于 OpenAI 核心使命的实现和推进的「使命与战略委员会」。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">扩大后的董事会将优先考虑优化治理程序的关键工作，以最好地实现 OpenAI 的使命。Bret Taylor 补充道：「我们认识到我们在管理变革技术以造福全球方面所发挥的重要作用。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会认可 WilmerHale 在进行此次广泛审查中所做的重要工作，并感谢 OpenAI 现任和前任董事会成员、顾问和员工的合作。OpenAI 董事会特别委员会发布了审查结果摘要。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>WilmerHale 的审查和调查结果摘要</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2023 年 12 月 8 日，特别委员会聘请 WilmerHale 对 2023 年 11 月 17 日山姆·奥特曼和 Greg Brockman 被解除 OpenAI 董事会职务以及奥特曼终止首席执行官职务的事件进行审查。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会向 WilmerHale 提供了进行全面审查所需的资源和权力。许多 OpenAI 员工以及现任和前任董事会成员都配合了审查过程。WilmerHale 多次向特别委员会通报审查进展和结论。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 评估了引起前一届董事会注意的管理和治理问题，以及 WilmerHale 在审查过程中发现的其他问题。WilmerHale 发现前任董事会与<span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">山姆·奥特曼</span>之间的信任破裂，导致了 11 月 17 日的事件。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 审查了上一届董事会于去年 11 月 17 日发布的公开内容，并得出结论认为，该声明准确地叙述了前任董事会的决定和理由。WilmerHale 发现，前任董事会当时认为其行动将缓解内部管理挑战，但没有预料到其行动会破坏公司的稳定。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 还发现，先前董事会的决定并非出于对产品安全或保障、开发进度、OpenAI 的财务状况，或其向投资者、客户或业务合作伙伴的声明的担忧。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">相反，这是前任董事会与奥特曼之间关系破裂和失去信任的结果。WilmerHale 发现，前任董事会在较短的时间内执行了其决定，没有提前通知主要利益相关者，也没有进行全面调查，也没有让奥特曼有机会解决前任董事会的担忧。WilmerHale 发现，前任董事会在其广泛的自由裁量权范围内终止了奥特曼的职务，但也发现他的行为「不构成强制解雇」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在审查 WilmerHale 的调查结果后，特别委员会向全体董事会建议批准 11 月 21 日重新聘用奥特曼和 Brockman 的决定。了解审查结果后，特别委员会对奥特曼和 Brockman 对 OpenAI 的持续领导表示充分信任。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会很高兴结束此次审查，并期待继续开展 OpenAI 的重要工作。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>新任命的董事</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 本次任命的另外三位新董事，或许向人们指明了这家 AI 领域最热门公司未来的走向。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426476" data-ratio="0.6666666666666666" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3397Whmibjibqjthp6ib8wFdSudH3TkfRoOVqVGKd4VEDc7PialahUYPBfw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">苏德斯・蒙德 - 赫尔曼博士（Sue Desmond-Hellmann）是比尔和梅琳达・盖茨基金会的前首席执行官，目前也在辉瑞董事会以及总统科学技术顾问委员会任职，她是美国肿瘤学和生物技术领域的著名学者。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426478" data-ratio="1" data-s="300,640" data-type="png" data-w="1080" style="width: 437px;height: 437px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a36Bl7iahIAkoMEqy9s4ukw52TcVx9rwLkOSM0hyXVNNkZNQhCZURibESw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">妮可・塞利格曼（Nicole Seligman 是一位律师，她因在伊朗反对派听证会上代表 Oliver North 中校，以及在弹劾案中代表比尔・克林顿总统而受到美国全国的关注。她是索尼前执行副总裁和全球总法律顾问、索尼娱乐总裁，她在派拉蒙全球、Meira GTx 和 Intuitive Machines, Inc. 董事会任职。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426479" data-ratio="0.6666666666666666" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3rhlAlia7P1VqVjuia34ITe8GA01fqH8tRmqBm08jCX5iabM6MbxYiaOvMA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">菲吉・西莫（Fidji Simo）的职业经历则一直在科技领域的范畴之内。她曾在 eBay 工作，担任过 Facebook App 副总裁兼负责人、Instacart 的首席执行官和主席，目前她也在 Shopify 董事会上。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">参考内容：</span><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://www.theverge.com/2024/3/8/24094885/openai-sam-altman-investigation-board-results</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://www.theinformation.com/articles/sam-altman-to-return-to-openai-board-of-directors</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://news.ycombinator.com/item?id=39647105</span></em></span><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426496" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span><span style="color: rgb(136, 136, 136);font-size: 12px;font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[清华姚班本科生连发两作，十年来最大改进：矩阵乘法接近理论最优]]></title>
        <id>2650910100_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910100&amp;idx=1&amp;sn=f6fde6912a71233d5608b1af31f01bfb&amp;chksm=84e469eab393e0fc0a1f68ab7263928b49e32254b65e7fcec9d139784a3792a22b1b203bb175#rd"/>
        <updated>2024-03-08T04:32:19.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">选自QuantaMagazine</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编译</strong></span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">作者：Steve Nadis</strong></span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：杜伟、大盘鸡</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="41" data-source-title="" style="letter-spacing: 0.578px;text-wrap: wrap;"><div class="js_blockquote_digest"><p>通过消除「隐藏的低效」问题，计算机科学家提出了一种比以往更快的大型矩阵相乘新方法。</p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">矩阵乘法作为众多 GPU 算子的基础操作，是高性能计算的重要问题之一，也是 AI 等应用的基石。它的算法机制本身相当简单，但为了达到更快的速度，人们多年来不懈努力，优化程度却一直有限。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今日，在《量子杂志》的一篇报道中，我们看到了推动矩阵乘法速度进一步提升的两篇论文，其中清华姚班一位大四本科生全程参与了两篇论文的撰写，为该领域的算法改进带来了全新的希望。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503426404" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8n9DkCzAP5E6kMK4OLlKSbejibZQ8KViay4iakjwYpWeibC5rOqmvQnW39w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">矩阵乘法改进出现新「奇点」</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">计算机科学家是一群要求很高的人。对于他们来说，仅仅获得问题的正确答案是不够的，往往还要尽可能高效地获得答案。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们以矩阵或数字数组相乘为例，1812 年，法国数学家 Jacques Philippe Marie Binet 提出了一套人们至今仍在教授学生的基本规则。这套规则运行得很好，但已经有数学家找到了简化和加速该过程的方法。&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426407" data-ratio="1.0685096153846154" data-s="300,640" data-type="png" data-w="832" style="width: 321px;height: 343px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8t7CqObCDVQ1IzLSTjuwmx1KbrhuysDhYZjOo547vNJRxhEZ6iblsIMQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="letter-spacing: 0.034em;color: rgb(136, 136, 136);">法国数学家 Jacques Philippe Marie Binet。</span></em></span><span style="font-size: 15px;letter-spacing: 0.034em;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，加速矩阵乘法过程的任务成为数学和计算机科学的交叉点。研究人员至今仍在继续改进该过程，尽管近几十年来进展相当有限。名古屋大学计算机科学家 François Le Gall 表示，自 1987 年以来，矩阵乘法的数值改进「一直很小，而且极其难以实现」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最近，来自清华大学的段然（Ran Duan）、周任飞（Renfei Zhou）和加州大学伯克利分校的 Hongxun Wu 在解决这个长期存在的问题上迈出了重要一步，撰写的论文足足有 87 页。对于三位研究者的成果， Le Gall 表示尽管改进本身相对较小，但是「从概念上讲比以往的改进都大。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该论文被计算机科学领域的顶会 FOCS 2023 接收。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426408" data-ratio="0.28888888888888886" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8XiaozGfYwZAhiaCEn6eMbXn4mn7qOEFH31y3Xcd7qzoR6aguiadn1IFkw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文 v1 发布在 2022 年 10 月，v5 在 2023 年 11 月。论文地址：https://arxiv.org/abs/2210.10173</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">其中，段然为清华大学交叉信息研究院副教授，主要研究方向为图论算法、数据结构、计算理论。Hongxun Wu 为加州大学伯克利分校二年级博士生，也是清华姚班出身。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">周任飞为清华姚班 2020 级的大四本科生，主修理论计算机科学（TCS）。他主要研究（简洁）数据结构和快速矩阵乘法，并对 TCS 的其他领域具有广泛兴趣，比如流算法、博弈论和在线算法等。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此前，周任飞曾在理论计算机科学顶级会议 FOCS/SODA 上发表多篇论文。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426409" data-ratio="0.412962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8IVLS5ptsuRF9tRe7q7Kx4oicgP9VHjeEXziax3PNAWEdEqgUOs059z6w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">三位研究者的论文揭示了以前未知且未开发的潜在改进来源，并且已经取得了成果。2024 年 1 月发表的第二篇论文（周任飞同样参与撰写）以此为基础，展示了如何进一步增强矩阵乘法。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426410" data-ratio="0.3101851851851852" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8VT6scRkZMpvjWic4dVwR4RD1ibSg79PsjCX2Br32BQlt5Iy4OY4Utj3A/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://epubs.siam.org/doi/10.1137/1.9781611977912.134</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">哈佛大学理论计算机科学家 William Kuszmaul 对此表示，这是一项重大的技术突破，是十多年来我们所看到的矩阵乘法的最大改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>矩阵乘法要改进什么问题</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">矩阵乘法可能看起来是一个晦涩的问题，但它是一种基本的计算操作。它被融入了人们每天使用的大部分算法中，用于各种任务，从显示更清晰的计算机图形到解决网络理论中的物流问题。就像在计算的其他领域一样，速度至关重要。即使是微小的改进最终也可能大大减少所需要的时间、计算能力和金钱。但目前，理论家主要感兴趣的是弄清这个过程到底能够有多快。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">传统的两个 n×n 矩阵相乘的方法 —— 即将第一个矩阵中每一行的数字与第二个矩阵中每一列的数字相乘 —— 需要进行 n³ 次独立的乘法操作。对于 2 乘 2 的矩阵而言，这意味着需要进行 2³，也就是 8 次乘法操作。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426411" data-ratio="1.9321428571428572" data-s="300,640" data-type="png" data-w="560" style="width: 377px;height: 728px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8OYBh3BvicwAF0oIDZ5j9VflLiciblDl0ktQKltiaekibiaZKxF55Abdf07pg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1969 年，数学家 Volker Strassen 发现了一种更精巧的方法，只需 7 个乘法步骤和 18 个加法步骤，就能完成 2×2 矩阵的乘法运算。两年后，计算机科学家 Shmuel Winograd 证明，对于 2×2 矩阵来说，7 步乘法确实是绝对最小值。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426412" data-ratio="2.032142857142857" data-s="300,640" data-type="png" data-w="560" style="width: 442px;height: 898px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8RqNqoVFKorIjH2iaVNSCS9ImFMRiaKnMa9c34mgMqZXVSLhM6plr74FA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Strassen 利用同样的想法证明，所有较大的 n×n 矩阵也可以用少于&nbsp; n3 步的方法进行乘法运算。这一策略中的一个关键因素涉及一个称为分解的程序：将一个大矩阵分解成一个个更小的子矩阵，这些子矩阵最终可能小到 2×2 甚至 1×1（只是单个数字）。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于将巨型数组分解成小块的理由相当简单，麻省理工学院的计算机科学家 Virginia Vassilevska Williams 说：「对于一个大矩阵（比如 100×100 的矩阵），人类很难想到最佳的算法。」即使是 3 乘 3 的矩阵也还没有完全解决。「然而，人们可以使用已经为小矩阵开发的快速算法来获得更大矩阵的快速算法。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究人员确定，速度的关键在于减少乘法步骤的数量，尽可能将指数从 n3（传统方法）降低。可能的最低值 n² 基本上就是写出答案所需的时间。计算机科学家把这个指数称为 Ω，即 ω。nω 是当 n 越来越大时，成功将两个 n×n 矩阵相乘所需的最少步骤。同为 2024 年 1 月论文合著者的周任飞说：「这项工作的重点，是看你能接近 2 多少，并且是否可以在理论上实现。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>激光法</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1986 年，Strassen 取得了另一项重大突破，他推出了矩阵乘法的激光法。Strassen 用它确定了 ω 的上限值为 2.48。虽然该方法只是大型矩阵乘法的一个步骤，但却是最重要的步骤之一，因为研究人员一直在不断改进它。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一年后，Winograd 和 Don Coppersmith 推出了一种新算法，对激光法进行了完美的补充。这套工具的组合在后来几乎所有加速矩阵乘法的研究中都得到了应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">下面是一个简化的方法，让我们来看看这些不同的元素是如何结合在一起的。让我们从两个大型矩阵 A 和 B 开始，将它们相乘。首先，你要把它们分解成许多较小的子矩阵，有时也叫块。接下来，你就可以使用 Coppersmith 和 Winograd 的算法，将其作为处理并最终组装这些块的指导手册。Vassilevska Williams 说：「它告诉我在乘积矩阵 C 中要乘什么、加什么，以及哪些元素在哪里。」「它只是一个从 A 和 B 建立 C 的『配方』」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">然而，这里有一个问题：有时你会得到具有共同元素的块。保留这些共同元素会相当于将这些元素计算两次，因此在某个时候，需要消除这些重叠部分。研究人员通过「消灭」它们所在的块来解决这个问题 —— 将它们的分量设置为零以将它们从计算中移除。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426413" data-ratio="1.1398148148148148" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8T86ozib0E5RGBo1VJV3Viaxq8gJI8ZfYptHZJF4bgCd7mZBwYzUTtO0w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">Virginia Vassilevska Williams 是改进矩阵乘法新方法的团队成员之一，她提出了目前最快的方法。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这就是 Strassen 的激光法最终发挥作用的地方。Le Gall 说，「激光法通常非常有效，并且通常能找到消除重叠的子块的好方法」。在激光消除了所有重叠之后，你就可以构建最终的乘积矩阵 C。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">将这些各种技术结合起来，就得到了一种用尽量少的乘法总数来乘两个矩阵的算法，至少在理论上是这样。激光法并不是为了实际应用；它只是一种思考矩阵相乘的理想方式。周任飞表示，「我们从未在计算机上运行这种方法，我们进行对它的分析。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">正是这种分析促成了 ω 十多年来的最大改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>被发现的「隐藏损失」</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在段然、周任飞和 Hongxun Wu 的第一篇论文《Faster Matrix Multiplication via Asymmetric Hashing》中，他们表明，施特拉森算法的进程可以大大加快。这一切要得益于他们称之为「隐藏损失」（hidden loss）的概念。周任飞表示，该概念深深地隐藏在以前的分析中，是无意中消除了太多块的结果。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">激光法的工作原理是将重叠的块标记为垃圾，并安排处理，而其他块被认为有价值并将被保存。不过，选择过程有些随机。事实上，被标记为垃圾的块可能最终还是有用的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这并不完全令人惊讶，但通过检查许多随机选择，段然团队确定激光法系统性地低估了块的价值，因此应该保存更多的块，减少扔掉的块。而且，正如通常的情况一样，更少的浪费可以转化为更高的效率。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于段然团队的做法，Le Gall 认为，「能够保留更多块而不重叠，这种做法实现了更快的矩阵乘法算法。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在证明了这种损失的存在后，段然团队修改了激光法标记块的方式，从而大大减少了浪费。他们将 ω 的新上限设定在了 2.371866 左右，这要比 Josh Alman 和 Vassilevska Williams 在 2020 年设定的上限 2.3728596 有所改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这看起来是一个不大的变化，<strong><span style="color: rgb(61, 170, 214);">将上限降低了大约 0.001，但这是自 2010 年以来科学家们看到的最大进步</span></strong>。相比之下，Vassilevska Williams 和 Alman 2020 年的结果只比之前的结果提高了 0.00001。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426414" data-ratio="0.26296296296296295" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8LszHqFERdHOZ29vo0C3fM5mEWa8BqSKc7yViafg3XGgibaMnVcLeLJVQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当然，对研究人员来说，最令人兴奋的不仅仅是新纪录本身，该记录并没有持续多久。事实上，这篇论文揭示了一种新的改进途径，而在此之前，这种途径完全没有被注意到。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Le Gall 称，近四十年来，每个人都依赖相同的激光法。随着段然等三位研究者的论文出现，我们可以做得更好。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">因此，周任飞参与撰写的 2024 年 1 月的论文改善了这种新方法，进一步减少了隐藏损失。他们又<strong><span style="color: rgb(61, 170, 214);">进一步提高了 ω 的上限，使它降低到了 2.371552</span></strong>。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426415" data-ratio="0.4462962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8HC3t3XBFAiar2U8PibuAY2KIC7E3SIoQc4byzIJyO5WiahAlTWvBqWOnQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者还使用同样的方法来改进矩形（n×m）矩阵的乘法过程，该乘法过程在图论、机器学习和其他领域均有广泛应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">沿着这些方向取得一些进一步的进展几乎是肯定的，但这是有限度的。2015 年，Le Gall 和两位合作者证明，目前的方法，也就是激光法，再加上 Coppersmith 和 Winograd 的方法，无法得到低于 2.3078 的 ω。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Le Gall 说：「要想进一步改进，就必须在 Coppersmith and Winograd 的原始方法基础上加以改进，而这种方法自 1987 年以来就没有真正改变过。」但到目前为止，还没有人提出更好的方法。也许根本就没有。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">周任飞说：「改进 ω 实际上是理解这个问题的一部分。如果我们能很好地理解这个问题，就能设计出更好的算法。不过，人们对这个古老问题的理解还处于非常初级的阶段。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">原文链接：</span><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">https://www.quantamagazine.org/new-breakthrough-brings-matrix-multiplication-closer-to-ideal-20240307/</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426416" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[「还是谷歌好」，离职创业一年，我才发现训练大模型有这么多坑]]></title>
        <id>2650909932_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909932&amp;idx=1&amp;sn=8b248b29157a4120671baa55b8a491e2&amp;chksm=84e46892b393e184560cc8c2cc6e28dcba0978db3b041aff00a495a0a98047572dbe85bca490#rd"/>
        <updated>2024-03-07T04:27:45.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：蛋酱、小舟</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="19" data-source-title="" style="outline: 0px;color: var(--weui-FG-1);letter-spacing: 0.544px;text-wrap: wrap;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">Karpathy：</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中肯的，一针见血的。</span></p></div></blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;">如何在不到一年的时间里创办一家公司、筹集资金、购买芯片，并搭建出追赶 Gemini pro/GPT 3.5 的 LLM？</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">很多人都对构建基础架构和训练大语言模型和多模态模型感到好奇，但真正走完「从零开始」这一流程的人很少。我们普遍认为，储备技术人才是前提，掌握核心算法是关键，但实际上，工程实践中冒出来的挑战，也实在令人头疼。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一年前，乘着大模型的热潮，Yi Tay 离开了工作 3 年多的谷歌，参与创办了一家名为 Reka 的公司并担任首席科学家，主攻大型语言模型。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在谷歌时，Yi Tay 参与过许多知名的大型语言模型和多模态模型工作，包括 PaLM、UL2、Flan-U-PaLM、LaMDA/Bard、ViT-22B、PaLI、MUM 等。即使经验如此深厚，他还是遇到了以往无法想象的困难。为了帮助更多创业者避雷，Yi Tay 在一篇博客中分享了自己踩过的那些「坑」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">「计算稀缺和不可靠的计算提供商使事情比预期困难得多，但我们凭借强大的技术实力渡过了难关。终于，我写了这篇博文，揭示了其中的一些挑战和经验教训。我希望这篇文章对很多人来说都是有趣或有教育意义的。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章发出后，得到了众多技术创业者的议论和转发。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426233" data-ratio="0.16666666666666666" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teK958G7b0HTMMnERjN4aCibN3EAX2dVPSeiaQptv7klfDaoqmglVf3g8MA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">连 Andrej Karpathy 也深有同感：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426234" data-ratio="0.21851851851851853" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teKhe3ibaaFajxpicic3iaiccw2j7CRPVL0bx2Bs1ENKQGX6OfZNFFfnibgj28w/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span></p><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="64" data-source-title=""><div class="js_blockquote_digest"><p>成熟的公司有专门的团队维护集群。随着规模的扩大，集群已经脱离了工程学的范畴，变得更加生物化，因此需要专门负责「硬件健康」的团队。</p></div></blockquote><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="189" data-source-title=""><div class="js_blockquote_digest"><p>「照看」训练运行是一项令人沮丧的训练大型模型日常生活体验。你需要仔细监控运行的生命体征：损失峰值、数值问题、吞吐量、梯度规范、策略熵等。每当运行性能下降或趋于平稳时（可能经常发生），你都要快速查找堆栈跟踪，看看发生了什么。你必须快速完成这项工作，否则可能会有 10000 个 GPU 闲置。通常，这是你从未见过的新的、奇特的、可怕的错误，所以你需要寻求帮助，看看是否有人能发现问题所在。</p></div></blockquote><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="198" data-source-title=""><div class="js_blockquote_digest"><div><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最严重的错误发生在凌晨 4 点。通常没人能看到，所以你只能禁止一些看起来有点可疑的节点，并尝试重新启动运行。有时，运行失败只是因为你当天没有得到神的眷顾，所以你在启动命令中加入了 while True: 循环。潜在的问题可能多种多样，从某些 GPU 发热过高、偶尔突然做错乘法运算到某些路由器宕机导致网络文件系统 I/O 减少，再到数据中心的某个人在未沟通的维护过程中物理断开电线连接。有的问题你甚至永远不会知道。</span></p></div></div></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span><span style="font-size: 15px;letter-spacing: 0.034em;">也有人发现了亮点：</span><span style="font-size: 15px;letter-spacing: 0.034em;">Yi Tay 所说的「荒野」（Wild）意思是「谷歌之外的公司」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;"><img class="rich_pages wxw-img" data-imgfileid="503426235" data-ratio="0.2222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teKIsyuYdo7icNcB6Qic2gxkOh7L7YYm7e7H2BPa3ic9ya2Ikia4icJNvLf6TA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">要是从基础设施和硬件的角度来说，能媲美谷歌的团队还真是不多。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，让我们一起看看博客内容：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>LLM 时代的硬件彩票</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">训练模型的首要条件是获得计算能力。这看似简单易行，然而，最大的惊喜却是<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>计算提供商的不稳定性，以及集群、加速器及其连接质量因来源不同而存在的巨大差异</strong></span>。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">人们总以为这只是一个加速器选择的问题 / 争论（TPU 与 GPU 等），所有 GPU 集群都是一样的。我们的体验是，这很快就被证明是错误的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们对不同的服务提供商进行了抽样调查，发现即使是相同的硬件，即 GPU（H100），硬件质量的差异也非常大。请注意，这里的硬件指的是集群的整体质量，而不一定是芯片或加速器本身。整体感觉就像购买彩票一样。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">更具体地说，我们从几家计算提供商那里租用了几个集群，每个集群都有数百到数千个芯片。我们所见过的集群有的还过得去（只存在一些小问题，但只需花几个小时的时间就能解决），有的则完全无法使用，每隔几个小时就会因各种原因出现故障。具体来说，有些集群的节点每隔 N 个小时就会出现故障，问题包括布线问题（N 小得不合理）、GPU 硬件错误等。更令人惊讶的是，同一家提供商的每个集群在鲁棒性方面也可能存在巨大差异。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">同时，即使其他一些集群的节点明显更稳定，它们也可能存在 I/O 和文件系统不佳的问题，甚至连保存检查点都可能导致超时，或耗费大量时间来降低集群利用率。其他一些计算资源甚至需要完全不同的软件层才能运行，而且对自带代码库的团队不友好 — 运行实验或大型工作需要额外的迁移成本。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">凡事都不会尽善尽美，但可以确定的是，提供商的服务质量是参差不齐的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最令人沮丧的是什么？几乎不可能真正提前知道，尤其是在万事俱备的情况下，会得到什么样的硬件，以及体验会有多么强大 / 容错性如何。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此外，如果供应商不能按时交货，将装备时间推迟几个月，导致用户在数周或数月内无法从其他来源采购，你更无从得知。有些供应商还会不小心删除你的检查点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我有没有说过，不同的集群会有不同的模型翻转利用率（MFU）？如果你不幸找到了一个节点布线不良或存在其他问题的提供商，那么浪费的计算量是不可忽视的。如果系统的文件系统非常不理想，那么当团队成员开始跨集群传输大量数据时，训练运行的 MFU 就会下降。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">每个服务提供商的售后水平也各不相同。从礼貌客气到不冷不热，从「对话式」的预制回复到将所有问题都归咎于用户，不一而足。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">总之，我们尝试过的每一个集群都有自己的风格、斗争和失败模式。而且，几乎每个集群都需要自己的热修复程序来解决一系列问题。尽管如此，我们还是认识到故障安全是非常重要的，为任何集群找到快速的热修复方案都是关键所在。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在过去的几个月里，我们构建了许多工具，以确保一切都可用，例如，围绕监控、高效检查点和其他各种优化的工具，甚至安装了我们的自定义文件系统，以实现可扩展的数据存储，而这只是实际需求的冰山一角。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这些工具的组合为 MFU 带来了非同小可的改进，同时也最大限度地减少了在硬件条件恶劣的情况下的停机时间。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>GPU vs TPU</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">就我自己的公司来说，大部分时间都在使用 GPU 训练模型。不过在加入 Reka 之前，我在谷歌的大型语言模型训练中一直使用 TPU。CUDA 和 nccl 对我来说是最陌生的东西 (我是从一位曾在 Nvidia 工作的同事那里才知道它的发音是 Nickel 的）。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">与我在谷歌使用 TPU 的经历相比，GPU 的故障率让我完全大吃一惊。事实上，我并不记得 TPU 发生过很多故障，即使是在大型运行中也是如此，不过我不确定，自己是否只是因为拥有出色的基础架构和专门的硬件团队才不知道这一点。事实上，谷歌的 UL2 20B 模型是通过意外运行一个月来训练的。如果是在 GPU 领域，它肯定会在最初几天内就失败。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">话虽如此，<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>我认为这可能更多与管理加速器的硬件团队的能力有关</strong></span>，而不是底层芯片。拥有良好的硬件支持（来自计算提供商）非常重要。而这在很大程度上取决于他们是否真的有能力，于是，又印证了「硬件彩票」的概念。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPU 领域给人的感觉很奇怪。与分布式训练在 TPU pods 上的一等公民地位相比，多节点训练更像是一种事后考虑。在 GPU 领域，感觉就像不同的提供商以不同的方式将它们连接起来，以实现多节点训练，这导致不同地方的做法差异很大。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">虽然我不是硬件专家，但这就是我的真实印象。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>多集群设置的痛苦</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我职业生涯的大部分时间都是在谷歌基础架构上度过的，这些基础架构主要运行在 Borg、Xmanager 和 Colossus 上。因此，必须在不同的集群中建立新环境的概念对我来说非常陌生。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在当今时代，拥有多个加速器池集群似乎是不可避免的，除非专门在一个地点建立大量的加速器池。更具体地说，GPU 的供应（或供应不足）自然而然地造成了这种集群采购模式，在这种模式下，事物的性质是支离破碎的。训练大型模型还需要大量的 TB 级数据，即使只是移动数据也会带来诸多不便。同时，复制数据通常也不是一件简单的事情，而且在超大规模的情况下，复制数据的成本也很高。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">显然，最理想的情况是建立某种编排层，专门将作业发送到不同的服务器。我相信，许多注重人工智能的大公司一般都有某种基础设施，以提高研究人员的生活质量。但是，对于一家初创公司来说，在开始阶段建立这种复杂而花哨的 ML 训练基础设施其实是不可能的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">目前，我们公司开发了许多内部工作流程来缓解这些问题，并继续朝着世界级实验基础设施的黄金标准迈进。(有人告诉我，对于非顶级 / 大型公司来说，这种简陋的设置或多或少是一种常态）。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>野生代码</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">众所周知，一直以来我最喜欢的代码库是 T5X 和 Mesh Tensorflow，但它们有一些缺点：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1）它们在 Google 之外没有得到那么多的支持；</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2）它们有点被弃用了；</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">3）它们对我们团队中的非 xoogler 不友好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们最终选择了一些普通的、看似稳定且更流行的东西，即 pytorch。pytorch 对团队中的大多数人（除了我）来说更容易使用。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在最初的几个月里，我对 pip、git、docker 和所有「野生(wild)」的东西感到困惑。话又说回来，我不能 100% 确定在外部使用 google 代码库有多稳定或多用户友好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">坦率地说，外部代码库的质量明显落后于我在谷歌习惯使用的代码库。主要是因为谷歌内部的代码库往往是由 ML 大牛自己编写的（例如 Noam Shazeer、Barret Zoph、Adam Roberts、Hyung Won Chung 等人），并且与我在外部尝试过的代码库相比感觉更好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">另外，我从来不知道更改模型并行性的能力不是自动（免费）的，直到某些代码库要求我编写一个转换器来更改模型的并行性。对我来说，这绝对是一个「WTF 时刻」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">令人惊讶的是，这些代码库对大规模编码器 - 解码器训练甚至 prefixLM 训练的支持非常少。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>少一点原则，多一点 Yolo</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">系统地扩展模型通常需要有原则地从小到大，即分多个阶段（1B→8B→64B→300B 等）进行实验，然后选出获胜者并不断扩展它们。在初创公司中，我们执行大规模扫描来检查超参数的计算量要少得多。最后，我们不得不多次运行 Yolo，幸运的是结果很好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最终，我们只需要极少数的较小规模和较短的烧蚀运行即可获得强大的 21B Reka Flash 和 7B 边缘模型，以及我们即将推出的最大核心模型。在运行次数非常有限的情况下找到可靠的方案具有挑战性，并且考虑到搜索空间极其巨大，需要立即更改许多变量。为了做到这一点，人们必须放弃大型科技公司的系统性，而在很大程度上依赖「Yolo」、直觉和本能。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">值得庆幸的是，我以及我们团队中的许多人在我们的 ML 职业生涯中已经积累了相当多的「直觉」，可以在很短的尝试时间内得到正确的结果。虽然我们在之前的工作中训练过非常好的模型，但训练基础设施、数据、新想法的融合和其他环境问题的差异仍然可能导致结果的巨大差异。也就是说，强大的先验有助于显著减少搜索空间，这可能就是我们能够通过如此少的试验、资源和实验训练出真正强大的模型的原因之一。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 0px;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">原文链接：https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426277" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[刚刚，OpenAI官方发文驳斥马斯克，自曝8年间邮件往来截图]]></title>
        <id>2650909778_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909778&amp;idx=1&amp;sn=ff24affa123b432ed266603c244a7a3b&amp;chksm=84e4682cb393e13a97da7431683dcf99962616cd52f354a9e02ac6e451555bca2fa2d1bb0031#rd"/>
        <updated>2024-03-06T03:54:18.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="22" data-source-title="" style="outline: 0px;color: var(--weui-FG-1);font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p>「不幸的是，人类的未来掌握在■■■的手上。」</p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最热科技公司 OpenAI 对全球首富马斯克，这场史诗大战进入了新的高度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">刚刚，OpenAI 用一篇长文《OpenAI and Elon Musk》，正式驳斥了马斯克的所有指控。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426087" data-ratio="0.6203703703703703" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FsuShXVOic78NABmr5qibna5YKAQQJTggwkezXsTAUJIEPdMOXkNHddibA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">标题简洁，但内容却相当吸引眼球。OpenAI 直接晒出了八年来各位创始团队成员与马斯克的往来邮件截图，并反复重申 OpenAI 对成立使命的不懈追求。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章开篇表示：「OpenAI 的使命是确保 AGI 惠及全人类，这意味着既要构建安全、有益的 AGI，又要帮助创造广泛的利益。我们正在分享我们在实现使命方面所学到的知识，以及有关我们与马斯克关系的一些事实。我们打算驳回马斯克的所有主张。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">与此同时，曝光邮件内容中的一些「神秘信息」，再次加重了人们的好奇：马斯克所提到的人类未来，到底取决于什么？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426088" data-ratio="0.5435185185185185" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FcSt80n5bJ79X3dSuFs2Vt7diaVy9315MFSJns8OFoJCRYA8BBolAEPg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章由 Greg Brockman、Ilya Sutskever、John Schulman、Sam Altman、Wojciech Zaremba 共同署名。当科技圈都在猜测 Ilya 是否已经因为与 Sam Altman 的矛盾离开 OpenAI 的时候，署名算是一种变相回应：至少此时，Ilya 仍作为核心成员全力抵御 OpenAI 正在面对的风暴。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426089" data-ratio="0.992" data-type="png" data-w="1000" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FrP2O6BHsqJNsTemUsiaIG4iaRUJo7IqIpK4lCjlALo4P83lr6l1UOJ5g/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">几天前，马斯克这位前 OpenAI 联合创始人在一份长达 46 页、总字数超过 1.4 万字的诉讼文件中，指控 OpenAI 不计后果地开发人类级别的人工智能，并将其移交给微软。马斯克的诉讼要求法院强制 OpenAI 回归开源，并阻止公司及其创始人以及微软等背后支持者从中获利。（参见：<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909259&amp;idx=1&amp;sn=bfda3ce888d99208aa7cc6aa33844889&amp;chksm=84e46e35b393e723e624968bfc2e1e728ce738c5429e0b24cd6aa580b9fc45d5ad0379763b3b&amp;scene=21#wechat_redirect" textvalue="《马斯克起诉 OpenAI：他们做出了 AGI 还授权给微软，这是对创始协议赤裸裸的背叛》" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">《马斯克起诉 OpenAI：他们做出了 AGI 还授权给微软，这是对创始协议赤裸裸的背叛》</a>）</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI CEO 对此的态度显得意味深长，让事情变得越发神秘：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426091" data-ratio="0.2101851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8Fv9rO03ViawJZVf09EbeuQPfkk6xAicletVBFoR6y2nOIicrq7JOmeB8Ng/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426092" data-ratio="0.22037037037037038" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FyzO56n83ZpCL3tOQZqTYZcIibMEt4TcgkaqrZWCDQGZSZ1kPbT4xhww/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克与 OpenAI 之间是理念不合，还是利益之争，局外人很难揣测。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一切没说破的话，可能都在这封信里。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>OpenAI 的反击</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 在最新发布的长信中写道：&nbsp; &nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们意识到构建 AGI 需要的资源比我们最初想象的要多得多</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">（在 2015 年的一封邮件中）马斯克称，我们应该对外宣布一份 10 亿美元的启动资金。但总的来说，作为非营利组织的 OpenAI 总共从马斯克那里筹集了不到 4500 万美元，从其他捐助者那里筹集了超过 9000 万美元。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2015 年底创办 OpenAI 时，Greg Brockman 和 Sam Altman 最初计划筹集 1 亿美元。马斯克在一封电子邮件中表示：「我们需要一个比 1 亿美元大得多的数字，以避免听起来目标无望…… 我认为我们应该从 10 亿美元的资金承诺开始…… 对于其他人未提供的部分，我将负责补足。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426093" data-ratio="0.9648148148148148" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FfK1hfzIA5JqrqN780GeKW84QN3ZUrj2LNtVQBML3yt2PhEldiavckCQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们花了很多时间试图设想一条可行的通用人工智能道路。2017 年年初，我们意识到构建 AGI 将需要大量算力，并且开始计算 AGI 可能需要多少计算量。我们都知道，我们需要更多的资金才能成功完成我们的使命 —— 每年大概数十亿美元，这远远超过我们任何人，尤其是马斯克认为我们作为非营利组织能够筹集到的资金。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们和马斯克认识到：需要一个营利性实体来获取这些资源</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当我们讨论以营利为目的的结构以进一步实现我们的使命时，马斯克希望我们与特斯拉合并，或者由他来完全控制 OpenAI。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">后来，马斯克离开了 OpenAI，表示需要有一个与谷歌 / DeepMind 竞争的对手，而他将自己做这件事。他说他会支持我们找到自己的道路。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">事情是这样的，2017 年年底，我们和马斯克决定下一步的任务是创建一个营利性实体。马斯克想要获得多数股权、初始董事会控制权并担任首席执行官，并且在讨论期间，他扣留了资金。Reid Hoffman 弥补了工资和运营方面的缺口。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们无法与马斯克就营利性条款达成一致，因为我们认为任何个人对 OpenAI 拥有绝对控制权都是违背使命的。然后马斯克建议将 OpenAI 并入特斯拉。2018 年 2 月上旬，马斯克向我们转发了一封电子邮件，建议 OpenAI 应该「将特斯拉作为其摇钱树」，并评论说：「这是完全正确的……特斯拉是唯一有希望与谷歌相媲美的选择。即便如此，制衡谷歌的可能性也很小。它只是不为零。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426094" data-ratio="0.8314814814814815" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8F8iabWV9gg8tNrPrMBmUFOksXMpad75vW2iaKl03oCEtlrsuwHwQvdFzg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426095" data-ratio="0.8851851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FcuBnCWM4WhZF3CYdF7kXdamd52msxibb5ibAgAqhV7D1ym4pmvtKsDOg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426098" data-ratio="0.9342592592592592" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8F08LrzvicDqRddxexAMKaicOVcdDY2ico4PsJwY0v02ZW8h6eNlK8zZrYQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">很快，马斯克就选择离开 OpenAI，他说我们成功的可能性为零，他计划在特斯拉内部建立一个 AGI 竞争对手。2018 年 2 月底离开时，他告诉我们团队，他支持我们自己寻找融资数十亿美元的道路。2018 年 12 月，马斯克给我们发邮件说：「即使筹集到几亿美元也不够。做这件事需要每年迅速筹集数十亿美元，否则就算了。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426099" data-ratio="1.1222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FoyIvf0R1Pib7UR4zdiawTgvuALH2SsaqvrrCwOFvDurAl31IFLsmHotw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们通过构建可广泛使用的有益工具来推进我们的使命</span></strong><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们通过开源贡献等方式，使我们的技术能够广泛使用，从而增强人们的能力，改善他们的日常生活。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们广泛提供当今最强大的 AI，包括每天有数亿人使用的免费版本。例如，阿尔巴尼亚正在使用 OpenAI 的工具，将其加入欧盟的时间加快了 5.5 年；Digital Green 正在帮助肯尼亚和印度提高农民收入，以 OpenAI 的技术为基础将农业推广服务的成本降低至原来的 1/100；罗得岛州最大的医疗保健提供商 Lifespan 使用 GPT-4 将其手术同意书从大学阅读水平简化为六年级阅读水平；冰岛正在使用 GPT-4 保护冰岛语。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克明白，我们的使命并不意味着开源 AGI。Ilya 曾告诉马斯克「随着我们越来越接近构建人工智能，开始降低开放程度是有意义的。openAI 中的『开放』是指人工智能建成后，每个人都应从人工智能的成果中获益，但不分享科学成果也完全没问题......」。当时，马斯克回答道：「是的（Yup）」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426100" data-ratio="1.4444444444444444" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FxYiaVvRrvTIYVI1SRKO3WhibVd9IBdhyezfxy4vIS8HtuNX0Wyib5qm6Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426101" data-ratio="1.1583333333333334" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FibkF2VFibd4E1do2iazhjFtIcuiaWM0WmY7aMQMPx6qygVXmEU2MNiaOz0Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">我们很伤心，因为我们深深敬佩的一个人 —— 他激励我们向更高的目标迈进 —— 他说我们会失败，还成立了一个竞争对手公司。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">当我们在没有他的情况下开始朝着 OpenAI 的使命取得有意义的进展时，他又起诉了我们。</span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们专注于推进我们的使命，任重而道远。随着我们不断改进我们的工具，我们很高兴能部署这些系统，让每个人都能使用它们。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>吃瓜的网友</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 的长信回应一时之间引爆了社交圈，吃瓜网友纷纷搬起了小板凳，围观这场「闹剧」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">活跃软件开发者、Deep trading 创始人 Yam Peleg 表示，「赶紧来吃新瓜啊」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426102" data-ratio="0.475" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FkV6Qrlz1ap9N7PJ8ibBFAnmSsmoWKlqQhVPyxsZqEUFSRfWK0Z0uic4Q/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">著名学者、纽约大学教授 Gary Marcus 对 OpenAI 的声明嗤之以鼻，直言 OpenAI 所说的「我们持之以恒追求自己的使命，比如不受产生利益回报的限制、不为私人利益掣肘、在适用的情况下寻求技术开源」是胡说八道。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，微软拥有 OpenAI 的独家所有权，49％的利润都归微软所有。OpenAI 不是在帮助全人类，而是从艺术家、作家那里窃取来增加自己的利润。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">更重要的是，OpenAI 几乎完全放弃了开源，并且成为最不开放的 AI 公司之一。无论与马斯克之间的恩怨如何，OpenAI 声称保持使命不变都是不诚实的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">总之，OpenAI违背了 2015 年成立时的宣言。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426103" data-ratio="1.6861111111111111" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FulcuaYI9ibD2Fhsv9g7HR4Vz8uZ6VbeiaEZPsQE0dKclvyPOdFhNQa6g/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">另外，大家比较好奇的一点是双方邮件中的一句话，「不幸的是，人类的未来掌握在■■■的手上。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426104" data-ratio="0.4166666666666667" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FibXibOmCZPyTVkjnu5YcBUfZ5XaAvzkibgiaf8pQy8gAvLic0nDr5KUm2kQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">众多网友纷纷猜测■■■中是谁？谷歌吗？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">还有人发现了以前从未听过的有趣观点，「OpenAI 的开放（open）只意味着每个人都可以从 AI 构建的成果中获益，而不向科学界分享是完全可以的。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426105" data-ratio="1.150925925925926" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FYxRRWvzOkeGqiaU6oib3DgDPic4UVmCG0icw6kbgg3CHjghGehjKx8Phyg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当然，更多人迫切地想要看到马斯克会如何回应。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426106" data-ratio="0.2212962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FpvQ8z9qibib35Z8Kib5PGB2lvH8aoIdRszrmyOlVLhp0COdrhbBAmMoIg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">参考链接：https://openai.com/blog/openai-elon-musk</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426110" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[生成式 AI 时代，手机正在进行一次全栈革新？]]></title>
        <id>2650909512_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909512&amp;idx=1&amp;sn=fdf171220387e754d383ae66706b2cd4&amp;chksm=84e46f36b393e620f46bc5f78e775216cf9fbf04092e3f1d075a7cbb2c5554eea7f56dcb6d02#rd"/>
        <updated>2024-03-05T02:48:45.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心发布</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：泽南</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="16" data-source-title=""><div class="js_blockquote_digest"><p>手机行业的第三次重大变革开始了。</p></div></blockquote><p><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最近一段时间，AI 与大模型技术突飞猛进。春节刚过，前沿方向上就迎来了新一轮突破。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425719" data-ratio="1.775" data-type="gif" data-w="320" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0NO6jIOSk3n18L4pWY0feJ3WXKvLcsF9riafbuQHsJJ8VxF1tibpsI5yQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">OpenAI 的 Sora 一下子把 AI 视频生成的进度条拉快了半年。</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在大模型的应用领域，技术落地应用的速度也在加快。目前各家大厂的新一代旗舰手机已经悉数登场，它们绝大多数都搭载了大模型，能实现很多前所未有的功能。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425718" data-ratio="0.5616" data-type="gif" data-w="625" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0CKibic16xgQH3icT8VIYWjOviaoq0WRMSUb7Lia5H9ObdHSKVo8vn69FkpA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图片来自高通骁龙 8Gen3 宣传片：https://www.youtube.com/watch?v=0CqtpjlL25w</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为什么大家都选择在 2024 年入局 AI ？</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">答案似乎很明确。随着大模型逐步成熟、芯片端侧算力的增强，手机厂商有了明确的判断：2024 年将是 AI 手机的元年。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不过面对一致的目标，各家打法不尽相同：很多手机开始引入云端大模型应用，也有一些实现了小尺寸模型的端侧跑通。在这其中，已经落地多个 AI 功能的 OPPO Find X7 系列正在获得越来越多的认可。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">它做到了很多个「第一」：Find X7 是全球首个端侧应用 70 亿参数大语言模型的手机。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">它还是第一个支持 AI 通话摘要的手机，让用户可以在通话结束后让 AI 一键生成摘要，并自动生成待办事项和提醒：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425720" data-ratio="0.4564814814814815" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0GtKagz4Zw5bC8wo7WClyl7iboCnJ0rPTXCLvUhibJEZpFLropUdicbY2g/640?wx_fmt=gif&amp;from=appmsg"><span style="font-size: 15px;letter-spacing: 0.034em;text-align: justify;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">有了手机端 AI 大模型的加持，智能助手也不再是个「摆设」，OPPO 的小布助手实现了跨越式的体验提升。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">现在的小布能够更好地理解自然语言，还可以回答各种刁钻的问题。它拥有超过 100 种能力，包括文字生成图片、图片解释、AI 文章摘要等，能在办公效率、生活服务、学习教育等不同维度为用户提供帮助。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如你可以问小布为什么饼干上有很多小孔，并要求它以「四岁孩子能理解的方式」回答：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425711" data-ratio="0.4675925925925926" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0wcl4TCdlnLofOIL5X6ecyialGU6xUibRdxZiaJpiaWXVicZiauib3k1M2ic5DA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO 还带来了全新的 AIGC 消除功能。以往需要电脑专业软件复杂操作的改图任务，现在在手机上只需要简单一圈 AI 就能帮你完成主体消除、实景重绘。在这个过程中，大模型还能进行一定程度的「创作」，脑补出背景，得到一张没有人潮的风景照：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425715" data-ratio="0.4564814814814815" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0CnprNnkmTLxL6QzmNOLVHkzLw98JWicr9s3xYEicQKd7cbtd4oBz3U1Q/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Find X7 的大模型能力不仅支持超过 120 类主体的识别与分割，还可以实现发丝级的分割、多达 6 个的多主体分离。这就是手机端生成式 AI 时代的拍照新体验。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">手机作为最常见的消费电子产品，一直是各种 AI 技术优先落地的方向，AI 美颜、AI 助手等功能早已是智能手机的标配。而随着大模型的兴起，手机作为人类「外延器官」了解用户的优势，再加上生成式 AI 前所未有的突破，又带来了更智能、个性化的体验和更多样的玩法。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">AI 手机或许将是继功能机、智能机之后，手机行业的第三个重大变革阶段。而在这场变革中，OPPO 提前为我们展示了大模型技术突破后，手机的全新形态。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">端侧 70 亿参数大模型</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">为什么 OPPO 做到了？</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO Find X7 能够实现的很多新能力，得益于端侧运行的 70 亿参数大模型。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这代旗舰机上，首次搭载了OPPO 自主训练的 AndesGPT 70 亿参数大模型。通过端云协同的部署，它实现了领先不止一代的 AI 体验。相比 10 亿参数模型，该模型能展现出更高「智商」的理解能力，可以更准确地理解对话内容并生成重点明确、细节丰富的摘要内容。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">相比于同平台的其他模型，AndesGPT 70 亿参数版可以在 2000 字首字生成时带来 20 倍的更快响应，最高对 1.4 万字进行内容摘要，是其他模型的 3.5 倍，对话体验也更接近人类对话的速度与信息量。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425722" data-ratio="0.3333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0rfp79hicq1QMmjPzPW6FbQTWicQZYO7vfm3A0yac7Jib2RGq463I85SoA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们知道，当前的大模型军备竞赛中，各家科技公司都在抢购 GPU，毕竟跑大模型很耗费算力。要在算力与内存有限的手机上运行大模型并不是件简单的事，OPPO 是如何做到的？</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这是因为在新一代手机上，OPPO 实现了面向大模型，从软件、硬件到云平台的全面优化。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">首先，OPPO 与平台厂商进行紧密协作，基于对芯片的理解，和一直以来对于用户需求的洞察和理解，根据大模型和算法深度定制了 SoC 芯片，提升了高负载条件下芯片的运行调度，进而优化了大模型的运行效率。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425723" data-ratio="0.3333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0pKCDwPWtpeU2FdKy34GiafP090cdVyV67kd3IwEtJDGbtBAIPlsXdzQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对于用户需求的洞察和理解是 OPPO 的核心竞争力，通话智能摘要就是个好例子：通话录音是业内早已出现的功能，但将传统的通话录音与端侧大模型结合，就带来了颠覆传统应用的全新 AI 体验。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">大模型部署在端侧，除了需要计算资源的极致优化，另一个瓶颈在于存储。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在正常情况下，70 亿参数的大模型需要占用 28GB 内存。为了真正实现端侧部署，OPPO 用 INT4 量化的方式对模型进行了大幅度压缩，让原本占用 28GB 内存的模型现在只需要 3.9GB，既降低了资源需求，也几乎不影响 AI 模型的输出效果。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">「大模型在端侧性能消耗比较大，要通过并行计算的算子优化、对内存管理的优化等来降低损耗和系统资源占用。续航方面要根据用机情况来看，我们端侧大模型的功耗控制在用户可以接受的范围内」OPPO AI 中心产品总监张峻表示。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO 还是第一家真正意义上把端侧 70 亿参数大模型同时部署在高通和 MTK 两个不同平台的手机厂商，对用户「一视同仁」的同时，也验证了自身对 AI 优化部署的能力。目前，端侧视觉模型的手机端部署也被列在了他们的日程表上。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对于生成式 AI 能力来说，有时端侧模型无法处理的复杂任务，需要把数据传到云端，利用服务器端 AI 加速器的力量；而很多包含个人信息和偏好的信息，需要在手机端侧预先处理，以保证隐私。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">大模型的端云协同，是目前行业的共识。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这一方面，OPPO 在发布 AndesGPT 大模型时，提出了通过三级大模型部署策略实现的端云联合部署，满足了多场景高效适配。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425724" data-ratio="0.37222222222222223" data-type="png" data-w="1080" style="font-size: 15px;letter-spacing: 0.034em;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0cG48jFzQ2ibibt2ZqIKT6YVB1GzZ3MXibYCQTXOYxJKWIPM4uwic6epOQg/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在端云协同的架构下，OPPO AI 手机的算力供给不再局限于本地，同时用户信息也能保证不被泄露 —— 用户数据仅在端侧计算，云端更强大的计算能力则面向复杂任务处理，既提高了大模型计算时的整体性能和效率，也保证了安全。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">AI 技术能力之外，OPPO 还有一个大战略。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>加码 AI 战略</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>人工智能投入无上限</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">其实，OPPO 已在 AI 领域深耕多年：早在2020年，OPPO就已经开始探索大语言模型的训练、应用与落地，首个自研大模型 OBERT一度跃居中文大规模知识图谱问答KgCLUE排行榜的首位；2023 年，OPPO 自主训练的安第斯大模型（AndesGPT）在 Super CLUE 知识与百科能力排行榜上仅次于 GPT 4，领先于所有竞品。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此外，Find X7 上在端侧应用的 70 亿大模型，AI 算力跑分也在安卓榜单上名列第一。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO 创始人兼 CEO 陈明永判断，2024 年将是 AI 手机元年，五年内 AI 对手机行业的影响将不可忽视。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">2 月 20 日，OPPO 召开 AI 战略发布会，分享了他们在 AI 战略上详细的规划。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503425712" data-ratio="0.53" data-type="gif" data-w="600" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID06EllvuNMWAlFZSykkTicib8FISHOdicKcQTfTFGhAj2OfNqreIfPArklg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO 对真正的 AI 手机下了定义，认为其需要具备以下四大特征：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">能高效利用计算资源，满足生成式 AI 的计算需要；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">能敏锐感知真实世界，了解用户与环境的复杂信息；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">有强大的自我学习能力；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具备更充沛的创作能力，为用户提供持续的灵感与知识支持。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了适应这些能力需求，手机行业需要进行全面的技术革新与生态重构：在硬件架构上，高效能的 AI 算力底座、模型库的管理优化以及智慧仿生感知能力将成为 AI 手机的新的标准。AI 手机的 OS 系统通过内嵌智能体，将能高效地处理复杂任务，并可以主动创作。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">另一方面，未来的 AI 手机将支持更敏锐、更准确的自然语义理解，拥有更强大的自学习能力，可提供更符合直觉的多模态交互。由此看来，传统的应用生态将会在 AI 手机时代转向智能体生态，各类服务应用都会与 AI 能力无缝连接，实现真正的智能化服务。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p><img class="rich_pages wxw-img" data-imgfileid="503425725" data-ratio="0.3333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0F9iaFNjpMB6f42Ge5IQJSicPhdwoj3ovppicvIl29E0EibIiac9B84KGnlg/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;letter-spacing: 0.034em;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为此，OPPO 已经做足了准备。在云端算力上，OPPO 拥有能够支持千亿级 AI 模型训练的 OPPO AI 滨海湾数据中心，支持两毫秒的骨干网络链接超低时延，以及 100% 的纯绿色能源。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">未来，OPPO 将在算力上持续投入，部署 AndesGPT Titan、Turbo、Tiny 三个级别的模型以对应不同应用场景。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425726" data-ratio="0.3333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0foS7SPdtxzrxbTBW5c5ibdLSiaIhndN6H0ZncM3wt8dNz44RibdJtKTPQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在 Agent 能力上，OPPO 正式发布了 1+N 智能体生态战略。其中的「1」代表 OPPO AI 超级智能体，它基于知识图谱、文档数据以及搜索引擎，能精准理解用户意图，给出准确结果，充分调用其他多种工具；「N」则代表基于 OPPO AI Pro 智能体开发平台所赋能的全新智能体生态。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">相比大模型智能助手，智能体是更加快捷和主动的 AI 助手，可以根据你的设定，以最有效的方式完成各种目标，满足情感陪伴、求知探索、娱乐闲聊等多样的场景需求。同时，构建智能体的方法非常简便，无需编程代码基础，人们只需和大模型进行自然语言对话，提供必要的说明和知识即可。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">智能体也可以与大模型以外的其他服务相连接，访问更多信息和手机功能，以通用化的能力满足用户的各类需求。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">更重要的是，OPPO 的 1+N 也意味着联合更多合作伙伴与开发者。随着智能体布局的展开，人们就能共同打造出面向 AI 手机生态的服务体系。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">就像智能手机上的 APP 应用市场。不过这次，由 AI 连接的服务能力更强，与你的连接更紧密。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">OPPO 还在继续加码 AI。战略发布会上，OPPO 正式宣布成立 AI 中心，旨在整合研发资源，针对 AI 进行能力建设与研发。刘作虎表示，AI 中心的成立将汇聚整个公司的力量，已把 AI 作为手机下一个时代最重要的战略，对于投入不设上限。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">大模型加持的手机</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">还会如何进化？</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这还只是个开始。毕竟大模型是一个「改变世界」的技术，所有领域的应用都要用 AI 重做一遍。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最近，在人工智能上，很多人都有大动作：苹果停止了自己持续多年的造车项目，并将探索重点也转向生成式 AI；谷歌在发布原生多模态大模型 Gemini 时宣布，未来大模型会整合至安卓系统中；而高通在 MWC 大会上推出的新一代 AI Hub，已支持超过 75 种主流 AI 模型在端侧的加速。现在，从手机厂商到科技公司，再到芯片公司，英雄所见略同。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们可以预见，随着技术的进步与行业生态的构建，未来我们还会看到更加智能化的拍照、更快捷的人机交互、更加个性化的内容生成和更高效的任务处理。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">使用生成式 AI，过去复杂的工作将会变得更简单。人们可以无需打开专业软件，仅发出口头指令就能让 AI 自动完成复杂的工作，大幅提升工作效率。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">部署在端侧的生成式 AI，也可以让智能手机更加了解用户的习惯和所处位置。利用情境信息，数字助手将会更加个性化，带来更令人满意的答案，提供更主动的服务。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">更进一步，随着 AI 生成能力逐步进入多模态领域，下一代 AI 渲染工具将能利用文本、语音、图像或视频等各种类型的提示生成 3D 物体和场景，最终创造出全新的沉浸式内容体验。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">一句话，AI 手机将会为我们带来一场革命。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503425727" data-ratio="0.5775" data-s="300,640" data-type="gif" data-w="400" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0IYzzGMGzeEWLIXrbUcEPBC4x56zn0u1NM7Lz8uzvMbYNALQ0JvPfhQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">随着 OPPO 等手机厂商对 AI 技术的不断推动，我们与想象之间的距离已经近了。</span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[全面超越GPT-4，Claude 3终于来了，有大学生智商，支持百万token]]></title>
        <id>2650909461_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909461&amp;idx=1&amp;sn=b68ab649d8c55f94c5a9d4fd7ea9b010&amp;chksm=84e46f6bb393e67d59c7a527d9fa90ace7e5c4b0216208fed218c05e9ab8d73a747087dd6eee#rd"/>
        <updated>2024-03-04T16:54:40.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="12" data-source-title=""><div class="js_blockquote_digest"><p><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">性能比 GPT-4 强很多。</span></p></div></blockquote><p><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">大模型的纯文本方向，已经卷到头了？</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">刚刚，OpenAI 最大的竞争对手 Anthropic 发布了新一代 AI 大模型系列 ——Claude 3。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">该系列包含三个模型，按能力由弱到强排列分别是 Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus。其中，能力最强的 Opus 在多项基准测试中得分都超过了 GPT-4 和 Gemini 1.0 Ultra，在数学、编程、多语言理解、视觉等多个维度树立了新的行业基准。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Anthropic 表示，Claude 3 Opus 拥有人类本科生水平的知识。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425784" data-ratio="1.4709110867178925" data-type="png" data-w="911" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID09J8Jd4UZyNaictjy28po8AlqmQ7sjmcTHQ0I9koIPtwEXojJXYF6Fqw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">在新模型发布后，Claude 首次带来了对多模态能力的支持（Opus 版本的 MMMU 得分为 59.4%，超过 GPT-4V，与 Gemini &nbsp;1.0 Ultra 持平）。用户现在可以上传照片、图表、文档和其他类型的非结构化数据，让 AI 进行分析和解答。</span></p><p style="line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3355015436458770433" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicJl5G6IHORtWvwpmicdIID0NvibRLkwlhTATy9GOTYlTVYEkRt3Ef8aiaMPj38nyaEXlBq7R6qpaRJg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3355015436458770433"></iframe></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">此外，这三个模型也延续了 Claude 系列模型的传统强项 —— 长上下文窗口。其初始阶段支持 200K token 上下文窗口，不过，Anthropic 表示，三者都支持 100 万 token 的上下文输入（向特定客户开放），这大约是英文版《白鲸》或《哈利・波特与死亡圣器》的长度。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">不过，在定价上，能力最强的 Claude 3 也比 GPT-4 Turbo 要贵得多：GPT-4 Turbo 每百万 token 输入 / 输出收费为 10/30 美元 ；而 Claude 3 Opus 为 15/75 美元。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425785" data-ratio="0.38055555555555554" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0Y4YVicklmqyR3ddNKnozD631QBtIBvG3asACAK4h6WUwhxXbWaBnuicw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Opus 和 Sonnet 现可在 claude.ai 和 Claude API 中使用，Haiku 也将于不久后推出。亚马逊也第一时间宣布新模型登陆了 Amazon Bedrock。以下是 Anthropic 发布的官方 demo：</span></p><p style="line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3355023918750744581" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicJl5G6IHORtWvwpmicdIID0iaKkgb1Liawfto9OOWUcE4qJm8agVhNZSl2kiafellF1bLoOgqmvtfCwg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3355023918750744581"></iframe></p><p style="line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3355025180179283970" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicJl5G6IHORtWvwpmicdIID085ytEkZo72LIHuVKtOGTAN15lbe7SQlS52NSx3sdMIcx7hA2hYMUJQ%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3355025180179283970"></iframe></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">在 Anthropic 官宣之后，不少得到试用机会的研究者也晒出了自己的体验。有人说，Claude 3 Sonnet 解出了一道此前只有 GPT-4 才能解开的谜题。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425786" data-ratio="1.1333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0mJlUibs13m24xXpVwia7HG5X87XNicVpYDekViag3myxfkvHZjiaCCB7mMw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">不过，也有人表示，在实际体验方面，Claude 3 并没有彻底击败 GPT-4。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425787" data-ratio="0.5690515806988353" data-type="png" data-w="601" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0B6gITWnlEwQXoWXaLDVgk0M9TgMS1YnukUFtF56MubY6poiaMduWcAw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">Claude 3 系列模型</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 系列模型的三个版本分别是 Claude 3 Opus、Claude 3 Sonnet 和 Claude 3 Haiku。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425788" data-ratio="0.5743380855397149" data-type="png" data-w="982" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0WyIbJPLOXMlG8GibVSyYWlcBUxRP7ic1ic6vicpKMiazk5KbdibNjN6fDhKw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">其中 Claude 3 Opus 是智能程度最高的模型，支持 200k tokens 上下文窗口，在高度复杂的任务上实现了当前 SOTA 的性能。该模型能够以绝佳的流畅度和人类水平的理解能力来处理开放式 prompt 和未见过的场景。Claude 3 Opus 向我们展示了生成式 AI 可能达到的极限。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425789" data-ratio="0.881619937694704" data-type="png" data-w="963" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0JEGzKkY1iafHWQLcQQc8XD9cRPz8epbzo6iabzUvWsNWXoUEUmlRA5iaQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 Sonnet 在智能程度与运行速度之间实现了理想的平衡，尤其是对于企业工作负载而言。与同类模型相比，它以更低的成本提供了强大的性能，并专为大规模 AI 部署中的高耐用性而设计。Claude 3 Sonnet 支持的上下文窗口为 200k tokens。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425790" data-ratio="0.7477110885045778" data-type="png" data-w="983" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0uT8af1q1AJaxzUSIKrybkhn8atU1lEx7e5ibzXU8S8icJgS2IRVF5gRQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 Haiku 是速度最快、最紧凑的模型，具有近乎实时的响应能力。有趣的是，它支持的上下文窗口同样是 200k。该模型能够以无与伦比的速度回答简单的查询和请求，用户通过它可以构建模仿人类交互的无缝 AI 体验。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425791" data-ratio="0.7903391572456321" data-type="png" data-w="973" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0pib5icZaVVdO3499HLXO9iaPpF2pibgIXqpE2k5NVNHFnwbUa0yj7jaRdA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 0.034em;">接下来我们详看一下 Claude 3 系列模型的特性和性能表现。</span><br></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">全面超越 GPT-4，实现智能水平新 SOTA</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">作为 Claude 3 系列中智能水平最高的模型，Opus 在 AI 系统的大多数评估基准上都优于竞品，包括本科水平专家知识（MMLU）、研究生水平专家推理（GPQA） 、基础数学（GSM8K）等基准。并且，Opus 在复杂任务上表现出接近人类水平的理解力和流畅度，引领通用智能的前沿。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">此外，包括 Opus 在内，所有 Claude 3 系列模型都在分析和预测、细致内容创建、代码生成以及西班牙语、日语和法语等非英语语言对话方面实现了能力增强。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">下图为 Claude 3 模型与竞品模型在多个性能基准上的比较，可以看到，最强的 Opus 全面优于 OpenAI 的 GPT-4。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425792" data-ratio="0.887962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0ML0NAia3skn6yQXeeHJ3Mg0hRhticLSfsnATvNJIMEgzdDImhadSc2bA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">近乎实时响应</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 模型可以支持实时客户聊天、自动补充和数据提取等响应必须立即且实时的任务。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Haiku 是智能类别市场上速度最快且最具成本效益的型号。它可以在不到三秒的时间内读完一篇包含密集图表和图形信息的 arXiv 平台论文（约 10k tokens）。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">对于绝大多数工作，Sonnet 的速度比 Claude 2 和 Claude 2.1 快 2 倍，且智能水平更高。它擅长执行需要快速响应的任务，例如知识检索或销售自动化。Opus 的速度与 Claude 2 和 2.1 相似，但智能水平更高。</span></p><p style="line-height: 1.75em;text-align: left;"><br></p><p style="line-height: 1.75em;"><strong><span style="font-size: 15px;">强大的视觉能力</span></strong><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 具有与其他头部模型相当的复杂视觉功能。它们可以处理各种视觉格式数据，包括照片、图表、图形和技术图表。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Anthropic 表示，它们的一些客户 50% 以上的知识库以各种数据格式进行编程，例如 PDF、流程图或演示幻灯片。因此，新模型强大的视觉能力非常有帮助。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425793" data-ratio="0.4361111111111111" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0yE27gX2eEDwcsMwu2djfwD1G8QcPJBYQWXeD3EYqMwohWxV2iatf96Q/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">更少拒绝回复</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">以前的 Claude 模型经常做出不必要的拒绝，这表明模型缺乏语境理解。Anthropic 在这一领域取得了有意义的进展：与前几代模型相比，即使用户 prompt 接近系统底线，Opus、Sonnet 和 Haiku 拒绝回答的可能性明显降低。如下所示，Claude 3 模型对请求表现出更细致的理解，能够识别真正的有害 prompt，并且拒绝回答无害 prompt 的频率要少得多。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425794" data-ratio="0.41944444444444445" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0duAQxVcrSIwtibEcuK59DfAKn2F15TdpMhJiciapab8vY5GBaebNz4dRQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">准确率提高</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">为了评估模型的准确率，Anthropic 使用了大量复杂的、事实性问题来解决当前模型中的已知弱点。Anthropic 将答案分为正确答案、错误答案（或幻觉）和不确定性回答，也就是模型不知道答案，而不是提供不正确的信息。与 Claude 2.1 相比，Opus 在这些具有挑战性的开放式问题上的准确性（或正确答案）提高了一倍，同时也减少了错误回答。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">除了产生更值得信赖的回复之外，Anthropic 还将在 Claude 3 模型中启用引用，以便模型可以指向参考材料中的精确句子来证实回答。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425795" data-ratio="0.4074074074074074" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0bKbUnEiaL56NNUofzpWziaFVO9szLfziak5VCqibdmOqwRRruL51SjZlNg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">长上下文和近乎完美的召回能力</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 系列型号在发布时最初将提供 200K 上下文窗口。然而，官方表示所有三种模型都能够接收超过 100 万 token 的输入，此能力会被提供给需要增强处理能力的特定用户。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">为了有效地处理长上下文提示，模型需要强大的召回能力。Needle In A Haystack（NIAH）评估衡量模型可以从大量数据中准确回忆信息的能力。Anthropic 通过在每个提示中使用 30 个随机 Needle/question 对在不同的众包文档库上进行测试，增强了该基准的稳健性。Claude 3 Opus 不仅实现了近乎完美的召回率，超过 99% 的准确率。而且在某些情况下，它甚至识别出了评估本身的局限性，意识到「针」句子似乎是人为插入到原始文本中的。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425796" data-ratio="0.49444444444444446" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0JfibyicY67FnAM1eTtkQVpWbkIXRxMcFiazGMZn36k8hzxKEvqicqwJqqA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><strong style="text-align: left;font-size: var(--articleFontsize);letter-spacing: 0.034em;"><span style="font-size: 15px;">安全易用</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Anthropic 表示，其已建立专门团队来跟踪和减少安全风险。该公司也在开发 Constitutional AI 等方法来提高模型的安全性和透明度，并减轻新模式可能引发的隐私问题。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">虽然与之前的模型相比，Claude 3 模型系列在生物知识、网络相关知识和自主性的关键指标方面取得了进步，但根据研究，新模型处于 AI 安全级别 2（ASL-2）以内。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">在使用体验上，Claude 3 比以往模型更加擅长遵循复杂的多步骤指令，更加可以遵守品牌和响应准则，从而可以更好地开发可信赖的应用。此外，Anthropic 表示 Claude 3 模型现在更擅长以 JSON 等格式生成流行的结构化输出，从而可以更轻松地指导 Claude 进行自然语言分类和情感分析等用例。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">技术报告里写了什么</span></strong></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">目前，Anthropic 已经放出了 42 页的技术报告《The Claude 3 Model Family: Opus, Sonnet, Haiku》。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425797" data-ratio="0.7267441860465116" data-type="png" data-w="860" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0Aic30peeBMpJffFvGn1bDFa7iaVXfDUBLiaoJ1dTictGdWalOyZ3tcyzUA/640?wx_fmt=png&amp;from=appmsg"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">报告地址：https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">我们看到了 Claude 3 系列模型的训练数据、评估标准以及更详细的实验结果。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">在训练数据方面，Claude 3 系列模型接受了截至 2023 年 8 月互联网公开可用的专用混合数据的训练，以及来自第三方的非公开数据、数据标签服务商和付费承包商提供的数据、Claude 内部的数据。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 系列模型在以下多个指标上接受了广泛的评估，包括：</span></p><p style="line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;"><span style="font-size: 15px;">推理能力</span></p></li><li><p style="line-height: 1.75em;"><span style="font-size: 15px;">多语言能力</span></p></li><li><p style="line-height: 1.75em;"><span style="font-size: 15px;">长上下文</span></p></li><li><p style="line-height: 1.75em;"><span style="font-size: 15px;">可靠性 / 事实性</span></p></li><li><p style="line-height: 1.75em;"><span style="font-size: 15px;">多模态能力</span></p></li></ul><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">首先是推理、编程和问答任务上的评估结果，Claude 3 系列模型在一系列推理、阅读理解、数学、科学和编程的行业标准基准上与竞品模型展开了比较，结果显示不仅超越了自家以往模型，还在大多数情况下实现了新 SOTA。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425798" data-ratio="1.1398148148148148" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0CHsAXftRhEnw0uy1Z7uYLFOt8bmIwvriccL82SZvZjaicNFq4ibuF2YZg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Anthropic 在法学院入学考试 (LSAT) 、多州律师考试 (MBE)、美国数学竞赛 2023 年数学竞赛和研究生入学考试 (GRE) 普通考试中评估了 Claude 3 系列模型，具体结果如下表 2 所示。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503425799" data-ratio="0.5351851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0cmJPoNHLy7T1wRzsmZJunEfibHWpiagyeYGz8liaM4smldbRDGVVXRYNg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 系列模型具备多模态（图像和视频帧输入）能力，并且在解决超越简单文本理解的复杂多模态推理挑战方面取得了重大进展。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">一个典型的例子是 Claude 3 模型在 AI2D 科学图表基准上的表现，这是一种视觉问答评估，涉及图表解析并以多项选择格式回答相应的问题。&nbsp;</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">Claude 3 Sonnet 在 0-shot 设置中达到了 SOTA 水平 —— 89.2%，其次是 Claude 3 Opus（88.3%）和 Claude 3 Haiku（80.6%），具体结果如下表 3 所示。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425800" data-ratio="0.6592592592592592" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0toAgtO6UNh02J9Bh9TafDyMMUgnTPYRSEXKAd6ecTSHjGRZteOOr4A/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">针对这份技术报告，爱丁堡大学博士生符尧在第一时间给出了自己的分析。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">首先，在他看来，被评估的几个模型在 MMLU / GSM8K / HumanEval 等几项指标上基本没有区分度，真正需要关心的是为什么最好的模型在 GSM8K 上依然有 5% 的错误。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425801" data-ratio="0.8194444444444444" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0UcrGJvaB4kXIQObaMuhuH3vgPSosZ8o9zaLlNcT44xdUMtBQ7GbOlw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">他认为，真正能够把模型区分开的是 MATH 和 GPQA，这些超级棘手的问题是 AI 模型下一步应该瞄准的目标。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425802" data-ratio="0.8074074074074075" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0P9AUpj1k9GtArXylhOE8MhIQ2uq2nps44taHbcKzn2qmh3wTicC0pUw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">与 Claude 之前的模型相比，改进比较大的领域是金融和医学。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425803" data-ratio="0.9203703703703704" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0o2pWib9ibxbTI0Hqich9ibYsItQd9sNptVU7V04VK3AicXFh3ER6hZfTEpw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">视觉方面，Claude 3 表现出的视觉 OCR 能力让人看到了它在数据收集方面的巨大潜力。</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425804" data-ratio="1.0833333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0zVUiaTtalvZtdgbQVVsncA1ichSDB8KaKpy0OnLnrLlNJhiagm1NFwL1w/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">此外，他还发现了其他一些趋势：</span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425805" data-ratio="0.8546296296296296" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0xYaSuoU56HibTqqRoEIeeeDIfQ8yyfjXMa8wPrbh0RsDj1yLKtwcTUQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503425806" data-ratio="0.7824074074074074" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicJl5G6IHORtWvwpmicdIID0aHtCtbvpdlj4MoI6frBPMK1ncibeW9uV99H7bJzNgicFVBRc5DzKibldA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;"><span style="font-size: 15px;">从目前的评测基准和体验看来，Claude 3 在智能水平、多模态能力和速度上都取得了长足的进步。随着新系列模型的进一步优化和应用，我们或许将看到更加多元化的大模型生态。</span></p><p style="line-height: 1.75em;"><br></p><p style="line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">博客地址：https://www.anthropic.com/news/claude-3-family</span></p><p style="line-height: 1.75em;text-align: left;"><em><span style="color: rgb(123, 12, 0);font-size: 12px;"><br></span></em></p><p style="line-height: 1.75em;text-align: left;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">参考内容：https://www.cnbc.com/2024/03/04/google-backed-anthropic-debuts-claude-3-its-most-powerful-chatbot-yet.html</span></em></span></p><p style="line-height: 1.75em;text-align: left;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">https://www.aboutamazon.com/news/aws/amazon-bedrock-anthropic-ai-claude-3</span></em></span><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p style="line-height: 1.75em;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503425807" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
</feed>