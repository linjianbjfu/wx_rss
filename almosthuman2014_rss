<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>almosthuman2014</id>
    <title>机器之心</title>
    <updated>2024-03-18T03:00:20.272Z</updated>
    <generator>awesome</generator>
    <author>
        <name>机器之心</name>
    </author>
    <subtitle>专业的人工智能媒体和产业服务平台</subtitle>
    <logo>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</logo>
    <icon>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</icon>
    <entry>
        <title type="html"><![CDATA[没等来OpenAI，等来了Open-Sora全面开源]]></title>
        <id>2650911307_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650911307&amp;idx=1&amp;sn=a7f96f7af740a62e862db62230c05fc6&amp;chksm=84e47635b393ff23a9046643078eae23a3170a4c8d34c6248ba2df795f2e10388527c0086a47#rd"/>
        <updated>2024-03-17T23:57:28.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心发布</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">机器之心编辑部</strong></span></p></div></div></div></div></div><p style="outline: 0px;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);visibility: visible;letter-spacing: 0.578px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不久前 OpenAI Sora 以其惊人的视频生成效果迅速走红，在一众文生视频模型中突出重围，成为全球瞩目的焦点。继 2 周前推出成本直降 46% 的 Sora 训练推理复现流程后，Colossal-AI 团队<strong><span style="color: rgb(61, 170, 214);">全面开源全球首个类 Sora 架构视频生成模型 </span></strong>「Open-Sora 1.0」，涵盖了<strong><span style="color: rgb(61, 170, 214);">整个训练流程</span></strong>，包括<strong><span style="color: rgb(61, 170, 214);">数据处理、所有训练细节和模型权重，</span></strong>携手全球 AI 热爱者共同推进视频创作的新纪元。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">先睹为快，我们先看一段<strong><span style="color: rgb(61, 170, 214);">由 Colossal-AI 团队发布的「Open-Sora 1.0」模型生成的都市繁华掠影视频。</span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427605" data-ratio="1" data-s="300,640" data-type="gif" data-w="512" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQXWU4ROibPnibDCSJlMeB2x4ich0EcW5l1munYFsSJh2Q798IvEkT3EqOA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">Open-Sora 1.0 生成的都市繁华掠影</span></em></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这仅仅是 Sora 复现技术冰山的一角，关于以上文生视频的<strong><span style="color: rgb(61, 170, 214);">模型架构、训练好的模型权重、复现的所有训练细节、数据预处理过程、demo 展示和</span></strong><strong><span style="color: rgb(61, 170, 214);">详细的上手教程，</span></strong>Colossal-AI 团队已经全面免费开源在 GitHub，同时笔者第一时间联系了该团队，了解到他们将不断更新 Open-Sora 的相关解决方案和最新动态，感兴趣的朋友可以持续关注 Open-Sora 的开源社区。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">Open-Sora 开源地址：https://github.com/hpcaitech/Open-Sora</span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>全面解读 Sora 复现方案</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">接下来，我们将深入解读 Sora 复现方案的多个关键维度，包括模型架构设计、训练复现方案、数据预处理、模型生成效果展示以及高效训练优化策略。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">模型架构设计</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">模型采用了目前火热的 Diffusion Transformer (DiT) [1] 架构。作者团队以同样使用 DiT 架构的高质量开源文生图模型 PixArt-α [2] 为基座，在此基础上引入时间注意力层，将其扩展到了视频数据上。具体来说，整个架构包括一个预训练好的 VAE，一个文本编码器，和一个利用空间 - 时间注意力机制的 STDiT (Spatial Temporal Diffusion Transformer) 模型。其中，STDiT 每层的结构如下图所示。它采用串行的方式在二维的空间注意力模块上叠加一维的时间注意力模块，用于建模时序关系。在时间注意力模块之后，交叉注意力模块用于对齐文本的语意。与全注意力机制相比，这样的结构大大降低了训练和推理开销。与同样使用空间 - 时间注意力机制的 Latte [3] 模型相比，STDiT 可以更好的利用已经预训练好的图像 DiT 的权重，从而在视频数据上继续训练。</span></p><p style="text-align: center;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427621" data-ratio="1.6451612903225807" data-s="300,640" data-type="jpeg" data-w="372" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQ3XdjwvdLNs8osv8O5k50g4AiclvqLtY9WrAC8zTqd67kER6wQISh0Ag/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">STDiT 结构示意图</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">整个模型的训练和推理流程如下。据了解，在训练阶段首先采用预训练好的 Variational Autoencoder (VAE) 的编码器将视频数据进行压缩，然后在压缩之后的潜在空间中与文本嵌入 (text embedding) 一起训练 STDiT 扩散模型。在推理阶段，从 VAE 的潜在空间中随机采样出一个高斯噪声，与提示词嵌入 (prompt embedding) 一起输入到 STDiT 中，得到去噪之后的特征，最后输入到 VAE 的解码器，解码得到视频。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427607" data-ratio="0.19420289855072465" data-s="300,640" data-type="png" data-w="690" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQLsNDotzA3ialOqCMZertLAoxfJgaerLcoKQDXj6X26uO5Te7npBicEdA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">模型的训练流程</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">训练复现方案</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们向该团队了解到，Open-Sora 的复现方案参考了 Stable Video Diffusion (SVD)[3] 工作，共包括三个阶段，分别是：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">1) 大规模图像预训练；</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">2) 大规模视频预训练；</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">3) 高质量视频数据微调。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">每个阶段都会基于前一个阶段的权重继续训练。相比于从零开始单阶段训练，多阶段训练通过逐步扩展数据，更高效地达成高质量视频生成的目标。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427620" data-ratio="0.25" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQWVFZw9eCRnfjxoMo6QtO91pOqVLXHiaehzBXatV6ib2rDs1y3PuRoHAQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">训练方案三阶段</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);">第一阶段：大规模图像预训练</span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);"><br></span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">第一阶段通过大规模图像预训练，借助成熟的文生图模型，有效降低视频预训练成本。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">作者团队向我们透露，通过互联网上丰富的大规模图像数据和先进的文生图技术，我们可以训练一个高质量的文生图模型，该模型将作为下一阶段视频预训练的初始化权重。同时，由于目前没有高质量的时空 VAE，他们采用了 Stable Diffusion [5] 模型预训练好的图像 VAE。该策略不仅保障了初始模型的优越性能，还显著降低了视频预训练的整体成本。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);">第二阶段：大规模视频预训练</span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);"><br></span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">第二阶段执行大规模视频预训练，增加模型泛化能力，有效掌握视频的时间序列关联。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们了解到，这个阶段需要使用大量视频数据训练，保证视频题材的多样性，从而增加模型的泛化能力。第二阶段的模型在第一阶段文生图模型的基础上加入了时序注意力模块，用于学习视频中的时序关系。其余模块与第一阶段保持一致，并加载第一阶段权重作为初始化，同时初始化时序注意力模块输出为零，以达到更高效更快速的收敛。Colossal-AI 团队使用了 PixArt-alpha [2] 的开源权重作为第二阶段 STDiT 模型的初始化，以及采用了 T5 [6] 模型作为文本编码器。同时他们采用了 256x256 的小分辨率进行预训练，进一步增加了收敛速度，降低训练成本。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);">第三阶段：高质量视频数据微调</span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><strong><span style="color: rgb(61, 170, 214);"><br></span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">第三阶段对高质量视频数据进行微调，显著提升视频生成的质量。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">作者团队提及第三阶段用到的视频数据规模比第二阶段要少一个量级，但是视频的时长、分辨率和质量都更高。通过这种方式进行微调，他们实现了视频生成从短到长、从低分辨率到高分辨率、从低保真度到高保真度的高效扩展。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">作者团队表示，在 Open-Sora 的复现流程中，他们使用了 64 块 H800 进行训练。第二阶段的训练量一共是 2808 GPU hours，约合 7000 美元，第三阶段的训练量是 1920 GPU hours，大约 4500 美元。经过初步估算，整个训练方案成功把 Open-Sora 复现流程控制在了 1 万美元左右。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">数据预处理</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了进一步降低 Sora 复现的门槛和复杂度，Colossal-AI 团队在代码仓库中还提供了便捷的视频数据预处理脚本，让大家可以轻松启动 Sora 复现预训练，包括公开视频数据集下载，长视频根据镜头连续性分割为短视频片段，使用开源大语言模型 LLaVA [7] 生成精细的提示词。作者团队提到他们提供的批量视频标题生成代码可以用两卡 3 秒标注一个视频，并且质量接近于 GPT-4V。最终得到的视频 / 文本对可直接用于训练。借助他们在 GitHub 上提供的开源代码，我们可以轻松地在自己的数据集上快速生成训练所需的视频 / 文本对，显著降低了启动 Sora 复现项目的技术门槛和前期准备。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427619" data-ratio="0.5625579240037072" data-s="300,640" data-type="gif" data-w="1079" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQyHFff01X9JJdWvickJhc0tOf9bQxsCmuGQx5ugIhv3E3WPFrUC1hEibA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">基于数据预处理脚本自动生成的视频 / 文本对</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>模型生成效果展示</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">下面我们来看一下 Open-Sora 实际视频生成效果。比如让 Open-Sora 生成一段在悬崖海岸边，海水拍打着岩石的航拍画面。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427610" data-ratio="1" data-s="300,640" data-type="gif" data-w="512" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQXPrRJv5LAjGrKtaPVeXgZIUeRz2gYrpoe5PDuHYl2ISs5mT5KprMWQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">再让 Open-Sora 去捕捉山川瀑布从悬崖上澎湃而下，最终汇入湖泊的宏伟鸟瞰画面。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427611" data-ratio="1" data-s="300,640" data-type="gif" data-w="512" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQzWicKnyicZ5OEGEKBxfT7nGp44WsnECoFNWDicuPXFISyu2SCCsqW6Lvw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">除了上天还能入海，简单输入 prompt，让 Open-Sora 生成了一段水中世界的镜头，镜头中一只海龟在珊瑚礁间悠然游弋。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427612" data-ratio="1" data-s="300,640" data-type="gif" data-w="512" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQCMZl4keT7vPXMaGDYPtjRRgZBqHeo7nLXvTMm5icKAq5PmXvoEf1zfg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Open-Sora 还能通过延时摄影的手法，向我们展现了繁星闪烁的银河。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427613" data-ratio="1" data-s="300,640" data-type="gif" data-w="512" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQibAAakBnicfGibeFGX0HN787ia6YOEN53OVlykOlprY8jq3Xbwv8ZLGxsA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">如果你还有更多视频生成的有趣想法，可以访问 Open-Sora 开源社区获取模型权重进行免费的体验。<span style="color: rgb(123, 12, 0);">链接：https://github.com/hpcaitech/Open-Sora</span></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">值得注意的是，作者团队在 Github 上提到目前版本仅使用了 400K 的训练数据，模型的生成质量和遵循文本的能力都有待提升。例如在上面的乌龟视频中，生成的乌龟多了一只脚。Open-Sora 1.0 也并不擅长生成人像和复杂画面。作者团队在 Github 上列举了一系列待做规划，旨在不断解决现有缺陷，提升生成质量。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>高效训练加持</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">除了大幅降低 Sora 复现的技术门槛，提升视频生成在时长、分辨率、内容等多个维度的质量，作者团队还提供了 Colossal-AI 加速系统进行 Sora 复现的高效训练加持。通过算子优化和混合并行等高效训练策略，在处理 64 帧、512x512 分辨率视频的训练中，实现了 <strong><span style="color: rgb(61, 170, 214);">1.55 倍的加速效果</span></strong>。同时，得益于 Colossal-AI 的异构内存管理系统，在单台服务器上（8*H800）可以<strong><span style="color: rgb(61, 170, 214);">无阻碍地进行 1 分钟的 1080p 高清视频训练任务。</span></strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427614" data-ratio="0.5581835383159887" data-s="300,640" data-type="png" data-w="1057" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8wKeeF646S3EDxJiaTATzogVz7mRaJDO2Fs0TiaVPgwKBIBicUdYSSxISDgKB42gkjuJlYUbGpyXQVQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此外，在作者团队的报告中，我们也发现 STDiT 模型架构在训练时也展现出卓越的高效性。和采用全注意力机制的 DiT 相比，随着帧数的增加，<strong><span style="color: rgb(61, 170, 214);">STDiT 实现了高达 5 倍的加速效果</span></strong>，这在处理长视频序列等现实任务中尤为关键。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427615" data-ratio="0.5631528964862298" data-s="300,640" data-type="png" data-w="1053" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8wKeeF646S3EDxJiaTATzoggREebG7AftIgc5juLOfKUE5yT8iaO8xexpTFX5mb3ibBblpxXQCLibDrg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 16px;"><strong>一览 Open-Sora 模型视频生成效果</strong></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最后，让我们一睹Open-Sora模型在视频生成上的精彩表现。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3374279123732250625" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicoXico1KbWYKMBrZwic3a4yQ7l6Xbq7X6Wv4rk2N8LpGibcXUzK2tSDfRD2DhlrANloUiaPMHZg3ic8pg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1920" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3374279123732250625"></iframe></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">欢迎持续关注 Open-Sora 开源项目：https://github.com/hpcaitech/Open-Sora</span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">作者团队表示，他们将会继续维护和优化 Open-Sora 项目，预计将使用更多的视频训练数据，以生成更高质量、更长时长的视频内容，并支持多分辨率特性，切实推进 AI 技术在电影、游戏、广告等领域的落地。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">参考链接：<br></span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[1] https://arxiv.org/abs/2212.09748 Scalable Diffusion Models with Transformers</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[2] https://arxiv.org/abs/2310.00426 PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[3] https://arxiv.org/abs/2311.15127 Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[4] https://arxiv.org/abs/2401.03048 Latte: Latent Diffusion Transformer for Video Generation</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[5] https://huggingface.co/stabilityai/sd-vae-ft-mse-original</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[6] https://github.com/google-research/text-to-text-transfer-transformer</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">[7] https://github.com/haotian-liu/LLaVA</span></em></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">[8] https://hpc-ai.com/blog/open-sora-v1.0</span></em></span></p><p style="text-align: center;margin-bottom: 24px;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;outline: 0px;max-width: 100%;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;white-space: normal;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;margin-bottom: 0px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;outline: 0px;max-width: 100%;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;white-space: normal;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;margin-bottom: 0px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;outline: 0px;max-width: 100%;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;white-space: normal;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;margin-bottom: 0px;box-sizing: border-box !important;overflow-wrap: break-word !important;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[首个AI软件工程师Devin完整技术报告出炉，还有人用GPT做出了「复刻版」]]></title>
        <id>2650911243_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650911243&amp;idx=1&amp;sn=62e218accac6da12d062d773f1fdb2f0&amp;chksm=84e47675b393ff6362bf5f76b6ee67a3b8ca55d51fa247f4387d7b6de7d22cb07c5175ac2e53#rd"/>
        <updated>2024-03-17T04:30:19.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">编辑：杜伟、大盘鸡</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="27" data-source-title="" style="color: var(--weui-FG-1);text-wrap: wrap;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);visibility: visible;letter-spacing: 0.578px;">从编码、编译到调试、验证，AI 智能体能做的事情更多了。</span></p></div></blockquote><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 0.034em;">这</span><span style="font-size: 15px;letter-spacing: 0.034em;">周</span><span style="font-size: 15px;letter-spacing: 0.034em;">三，<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910796&amp;idx=2&amp;sn=9ef805a354f29a94d3f06fff25ccff07&amp;chksm=84e47432b393fd2458e0c6f44d55e5cacc3eb11a07d96f12e0e09433339732f7c155bbe31774&amp;scene=21#wechat_redirect" textvalue="Cognition AI 团队发布的首个 AI 软件工程师 Devin 引爆了 AI 社区" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Cognition AI 团队发布的首个 AI 软件工程师 Devin 引爆了 AI 社区</a>，引发了人们对程序员这个职业未来前景的热议。</span><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p class="channels_iframe_wrp wxw_wechannel_card_not_horizontal"><mp-common-videosnap class="js_uneditable custom_select_card channels_iframe videosnap_video_iframe" data-pluginname="mpvideosnap" data-url="https://findermp.video.qq.com/251/20304/stodownload?encfilekey=rjD5jyTuFrIpZ2ibE8T7YmwgiahniaXswqzsqlVPSHW0IQlfcJmdV99ZsibMeWsQicZnJh7UnaNUhVtX5PbJMKk3fh1mRMnjjWXUj14Dia4SkoxEkjOicZfMRnvJw&amp;bizid=1023&amp;dotrans=0&amp;hy=SH&amp;idx=1&amp;m=&amp;scene=0&amp;token=AxricY7RBHdUbOiaGibnvGeOkVQjWIkw2E38Hd7uRluOnw1mqloMtf1YHyBlOIEHSdJQW40QNWg7Og" data-headimgurl="http://wx.qlogo.cn/finderhead/PiajxSqBRaEKs6XzYjCzlSsfrOck5ZdKLHuqicYEiaI62Ty9EQxZmibuCQ/0" data-username="v2_060000231003b20faec8c7e5811fcbd2cc05eb34b077bf43ae33648ee0ea039da9063a594944@finder" data-nickname="机器之心机动组" data-desc="全球首个AI程序员在线接单，只提要求，模型就自动跑好了，程序员的时代结束了吗？今天，一家只有十人的创企Cognition AI发布了智能体Devin。据称，Devin 通过了一家业内领先的人工智能公司的面试，在自由职业平台Upwork上也能成功完成单主要求的任务。也就是说，Devin不仅是横扫基础测试的「做题家」，在就业市场里也有人为它的工作能力买账。#Devin#CognitionAI#编程#程序员#软件工程师#大模型#智能体#agent#AI#人工智能#科技#前沿科技" data-nonceid="14630469220346202382" data-type="video" data-mediatype="undefined" data-authiconurl="https://dldir1v6.qq.com/weixin/checkresupdate/icons_filled_channels_authentication_enterprise_a2658032368245639e666fb11533a600.png" data-from="new" data-width="1080" data-height="1920" data-id="export/UzFfAgtgekIEAQAAAAAAUN86iY07IQAAAAstQy6ubaLX4KHWvLEZgBPE94J4VnYVBeiEzNPgMItwKBoDXqCsq0LEluEPMGku" data-isdisabled="0" data-errortips=""></mp-common-videosnap></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在对 Devin 的评估中，团队使用了 SWE-bench。这是一个由 GitHub 问题和拉取请求组成的软件工程系统的自动化基准测试。他们认为 SWE-bench 是一个不错的选择，它确定性地评估（通过单元测试）系统解决现实世界代码库问题的能力，并与 HumanEval 等仅限于独立功能的基准测试不同。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从结果来看，在 SWE-Bench 基础测试中，无需人类辅助，Devin 就可以解决 13.86% 的问题。而当前 SOTA 模型，在没有人类帮忙的情况下，只能完成 1.96% 的问题。即使提供了要编辑（辅助）的确切文件，当前 SOTA 模型也只能解决 4.80% 的问题。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427565" data-ratio="0.53625" data-type="png" data-w="800" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQYLso1OGle6pxZGARqKjTibrlZeiaKRIFpicSmnhwYoVy6wYvU8LRhl7NQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">数据集</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具体来讲，SWE-bench 是一个包含 2294 个问题和 GitHub 流行开源 Python 存储库中拉取请求（pull request）的数据集，目的是测试系统编写真实代码的能力。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">每个 SWE-bench 实例都包含一个 GitHub 问题和解决该问题的拉取请求。拉取请求必须包含一个单元测试，该测试在代码更改之前失效并在代码更改之后通过（称为「未能通过」（fail to pass）测试）。diff 分为两部分，即 patch 和 test_patch，分别包含代码更改和测试更改。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">接着要求正在评估的系统根据 GitHub 问题描述和存储库（问题发生时）生成 diff。如果在修补（patch）编辑后所有单元测试都通过，则该示例被认为是成功的。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427566" data-ratio="0.2351851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQAV4ObgmeuFVceU8t0Pz7cjPtrK8R11ic2rfHRBOc7k4hrJ1NVUjn7iag/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在 SWE-bench 中，大模型（LLM）要么获得一组正确的文件进行编辑（辅助）或者一个单独的系统根据与问题文本的相似性检索要编辑的文件（无辅助）。作为一个智能体，Devin 不会收到任何文件列表，而是自行导航文件，这与「无辅助」LLM 更具可比性。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">正确解决 SWE-bench 示例具有挑战性，更难的 PR 需要更改数十个文件、保持向后兼容性和 / 或进行大量复杂的推理。即使有辅助，最好的 LLM 也只能达到 4.80% 的成功率。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">方法</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队采用 SWE-bench 来评估智能体，实现了比 LLM 原始评估更通用的设置。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">设置</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队使用标准化 prompt 来端到端地运行智能体，要求它仅在给出 GitHub 问题描述的情况下编辑代码。在运行期间，团队不会向智能体提供任何其他用户输入。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">存储库被克隆到智能体的环境中。团队只在 git 历史记录中保留 base commit 及其 ancestor，以防止信息泄露给智能体。值得注意的是，他们删除了 git Remote，以便 git pull 不起作用。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队在测试开始前搭建了 Python conda 环境。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队将 Devin 的运行时间限制为 45 分钟，因为与大多数智能体不同，它具有无限期运行的能力。如果需要，它可以选择提前终止。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">评估</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">一旦智能体运行退出，团队会将所有测试文件重置为原始状态，以防智能体修改测试。他们获取文件系统中的所有其他 diffs 并将它们提取为补丁。</span></p></li><ul class="list-paddingleft-1" style="list-style-type: square;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 0.034em;">为了确定哪些文件是测试文件，团队采用测试补丁中修改的所有文件的集合。</span><span style="font-size: 15px;letter-spacing: 0.034em;"></span></p></li></ul><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队将智能体的补丁应用到存储库，然后应用测试补丁。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队运行 SWE-bench 提供的 eval 命令并检查是否所有测试都通过。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">结果</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队随机选择了 SWE 基准测试集中 25% 的问题（即 2294 个中的 570 个），对 Devin 进行了评估。这样做是为了减少完成基准测试所需的时间。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">结果显示，Devin 成功解决了 570 个问题中的 79 个问题，成功率为 13.86%。这甚至比之前 SOTA 大模型（Claude 2）的 4.80% 还要高得多。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">下图中的基线在「辅助」设置下进行评估，其中为模型提供需要编辑的确切文件。基线在「无辅助」设置下表现较差，其中单独的检索系统选择 LLM 编辑的文件，最佳的模型是 Claude 2 + BM25 检索，成功率为 1.96%。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427567" data-ratio="0.5518518518518518" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQ2Qr8Bj1aIibe8TT21tezPfmweTYljPmCl4vGwxyib7uyicJaJUCOia0ribA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不过团队表示，辅助或无辅助设置下，其他模型都不能与 Devin 进行严格比较。Devin 获得整个存储库并可以自由浏览文件，因此他们选择更强的数字进行基线比较。团队认为，端到端运行智能体对于 SWE-bench 来说是更自然的设置，这样更类似于现实世界的软件开发。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">分析</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">多步规划</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 可以执行多步规划来接收来自环境的反馈。72% 的通过测试需要 10 分钟以上才能完成，这表明迭代能力有助于 Devin 取得成功。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427568" data-ratio="0.75" data-type="png" data-w="800" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQv543GeWKcnbXUOZG1ZBGz7lLZ1iahaG3olMm3261FnVXqEWp8FJftWg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">定性案例</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队对 Devin 的进行了一些定性分析。这里 Devin 仅获得了问题描述和克隆存储库作为输入。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">示例 1：✅ scikit-learn__scikit-learn-10870</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427569" data-ratio="0.8398148148148148" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQW4oefmoOB4Be1wMv0KUREueM3KIQRFrxqdBGa1h3QHsSG9WNNC1QibA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 最初对描述感到困惑，并按照描述在 return self 之前添加 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">self.lower_bound_ = max_lower_bound before return self</span><span style="font-size: 15px;"> 。这实际上不正确，因为该变量尚未定义。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427570" data-ratio="0.4759259259259259" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQtPmicdvCJdDwJbbaqZzZ4icsPwzalUxibNekeUlx4rzuneib3CYPibUxdkA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">随后，Devin 根据问题描述中提供的测试代码，更新了测试文件。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427571" data-ratio="0.34814814814814815" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQPzicfMtW8rMVyrhr2qRXMoUhIZYgMpB3GAFVzMmWKjgibfWVIibvayiboQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在运行测试并收到错误后，Devin 更正了该文件。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427572" data-ratio="0.09259259259259259" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQ6deicbAKsqjZm9tetQAQ1U81y3xD2LZKR1w8l4ZLkbqthRLDbHD7y6Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427573" data-ratio="0.6861111111111111" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQe9Vsc9NjAh8LKGic6CtqRMY4Qeoc8Rykp6LTMQcvQkk3mGDich4W8rHA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">修复后，Devin 重新运行测试以使其通过并成功退出。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这个例子很有趣，主要有几个原因：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 非常严格地遵循原始问题的指示，尽管不准确。这表明与用户的偏好过度一致。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">得益于能够在自身环境中运行测试，Devin 能够纠正错误。对于软件开发人员来说，能够进行迭代至关重要，智能体也应该能够做到这一点。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">示例 2：✅ django__django-10973</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427574" data-ratio="0.6722222222222223" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQrDoWZTUB1bKGxXjeYtdAAXHLDPlJG0T0H9D0rIHmkKkMiay6eorGsoQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 找到了正确的文件 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">django/db/backends/postgresql/client.py</span><span style="font-size: 15px;">，并进行了完整的编辑：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427575" data-ratio="0.6916666666666667" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQTE9ScQzqiciajvJzROeW6pIibgYvhWjehRrMC5chnJJlCXsaKHhc1AzBg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这里，Devin 能够成功修改一大段代码。在 SWE-bench 中，许多成功的编辑都是单行 diff，但 Devin 却能同时处理多行。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">示例 3：❌sympy__sympy-17313</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这是一个复杂的任务，涉及修改计算机代数系统以正确处理 floor 和 ceiling 对象与可以指定为正或负的值之间的比较运算符。它需要复杂的逻辑推理和多个推导步骤。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427576" data-ratio="0.325" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQib6LTRbKichx9Lj7vYJticDJLMNfW15bYkg604gmB3NPGGrsaBMJ6gNSQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 错选了要编辑的正确类，他编辑的是 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">frac</span><span style="font-size: 15px;"> 类，而不是 floor 类和 ceiling 类。此外，Devin 只编辑了一个比较运算符 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">__gt__</span><span style="font-size: 15px;">，而 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">__lt</span><span style="font-size: 15px;">、</span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">le__ </span><span style="font-size: 15px;">和</span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">__ge__</span><span style="font-size: 15px;">也需要修改。这次的编辑错的很离谱。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">正确的diff可以在这里找到：https://github.com/sympy/sympy/pull/17313/files。diff 相当复杂，包含大量边缘情况处理和大量单元测试，需要深入了解 sympy 代码库。(需要注意的是，每个测试都必须通过才能通过 SWE-bench 实例）。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">示例 4:❌ scikit-learn__scikit-learn-10774</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这项任务包括为 repo 中的所有数据集添加额外的返回选项功能。Devin 成功地对其中几个数据集进行了编辑，示例如下。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427577" data-ratio="0.4703703703703704" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQS6JAsiaEPVgibOpJZPIXiaPrKTVcKk9x2cst0hnGw7goicj1RLjDdEsMGg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427578" data-ratio="0.13333333333333333" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQT0CaVOpZiaSSFcbCC0fqcM9o0dErzuyO4gMDCre6KyicWKI43TZziaK6g/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Devin 对数据集 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">california_housing.py</span><span style="font-size: 15px;">、</span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">covtype.py</span><span style="font-size: 15px;">、</span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">kddcup99.py</span><span style="font-size: 15px;"> 和 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">mldata.py</span><span style="font-size: 15px;"> （最初的 PR 实际上排除了这些数据集）进行了类似的编辑。不过，Devin 漏掉了两个数据集，即 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">lfw.py</span><span style="font-size: 15px;"> 和 </span><span style="font-size: 15px;background-color: rgb(214, 214, 214);">rcv1.py</span><span style="font-size: 15px;">，因此测试最终失败。团队打算改进 Devin 编辑多个文件的功能。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">测试驱动实验</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">团队还进行了一项实验，向 Devin 提供了最终的单元测试和问题陈述。在这种测试驱动开发设置下，100 个抽样测试的成功通过率提高到了 23%。(请注意，对测试本身的任何修改都会在评估前被删除）</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这一结果与 SWE-bench 的其他结果无法相比，因为智能体可以访问真实的测试补丁。不过，测试驱动开发是软件工程中的一种常见模式，因此这种设置是 SWE-bench 的自然扩展。人类向智能体提供一个需要通过的目标测试是人类工程师和智能体合作的一种自然方式，团队期待在未来看到更多测试驱动的智能体。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><strong><span style="font-size: 15px;">Devin 新近通过测试解决的问题示例</span></strong><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">✅django__django-13321：Devin 在函数前添加了打印语句，然后运行单元测试，最后根据打印语句编辑文件，从而解决了这个问题。测试用例的存在使 Devin 能够轻松地进行调试。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427579" data-ratio="0.21666666666666667" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQ1LUS9QWEOSglXhqU8XhCIe06PRPsN6sp6J22icWviafO2u1ecnXf1KRA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427580" data-ratio="0.19166666666666668" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQBm39CBnvIyrslSG0FSPDBJic4Fia9LMl9BeoSr2ibnZvBfBne43P8sPGQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">✅django_django-16983：新单元测试断言会发出 queqie 的错误消息："'filter_horizontal [0]' 的值不能包括 [...]"。如果不知道错误信息的确切措辞，就不可能通过测试。这突显了基准测试的一个问题，并表明如果没有测试补丁，就不可能得到完美的分数。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">完整报告地址：https://www.cognition-labs.com/post/swe-bench-technical-report</span><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">类 Devin 项目</span></strong></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">与此同时，Devin 发布后几天，社区已经出现「复刻版」。推特用户 @antonosika 使用 GPT 和一些开源项目对 Devin 进行复刻，他表示无需代码即可制作 Devin。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具体工作流如下所示：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427581" data-ratio="0.5429362880886427" data-type="png" data-w="722" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQao7CZp2T5J3WmfMNibYczpkVjOf9uiaaEhDUV8ou5waXFP34lhThqTibg/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">获取 Devin 应用界面的截图；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">利用 gptengineer 应用程序与前端界面和 GitHub 代码空间结合；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">克隆 Open Devin 并使用 gptme 作为后端；</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">利用 gptme 的命令行界面（CLI）来连接前后端，从而构建一个完整的应用程序。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">完整视频如下：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3373076887811375109" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicoXico1KbWYKMBrZwic3a4yQjnZFbQje6AXu8EahS0qvLCbH1JrSc6WVoibQvrAdwTJD7AjDDmpZUZg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.5444444444444445" data-w="1112" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3373076887811375109"></iframe></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此外还有 BabelCloud，它也是一个类似于 Devin 的 AI 软件工程师，能够独立完成相对复杂的前后端任务。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427582" data-ratio="0.5760869565217391" data-type="png" data-w="736" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQeEHe5AMMIX5HA2ib3nicBHBicxs8PCuVZA7TMp1bhvo5pWhFxS5HMJq8w/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;"><span style="font-size: 15px;color: rgb(123, 12, 0);">技术博客：https://medium.com/@connect_33559/how-far-from-replacing-humans-ai-programmer-build-complex-software-after-3-hours-of-working-1b61b18a0c0b</span></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">完整视频如下所示：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3373086313989521418" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicoXico1KbWYKMBrZwic3a4yQRoX8y7iam5dicnndUEnvFGibLibzQCqv8ZXIAwXQZEWZy0XTz0CQ7kmRJw%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3373086313989521418"></iframe><span style="font-size: 15px;letter-spacing: 0.034em;"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从博客和视频中可以看到，该项目可以使用 Babel Agent 进行工程软件开发，比如测试 Claude 3 的可用性、编写后端管理系统、集成 Stripe 支付系统等。Babel Agent 的具体功能包括如下：</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">自主任务分解。Babel Agent 可以根据需求文档自主设计和分解任务，并逐一执行。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">自主编码、编译和调试。Babel Agent 不仅可以自主编写、编译代码，还可以根据编译中的问题反馈自主调试，就像人类程序员的工作流程一样。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427583" data-ratio="1.2946954813359528" data-type="png" data-w="1018" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQqofztOUT5anUuu9TmhT207kVulAB0sknBI1EEIZib3QjM7hAjKgAcdw/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">独立问题的自主研究。当任务是集成 Claude 3 时，Babel Agent 会自主搜索 SDK，找到文档，编写代码，然后对其进行测试和验证。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">自主测试。Babel Agent 可以编写自动化测试代码、执行测试并自行纠正问题。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427584" data-ratio="0.9221556886227545" data-type="png" data-w="1002" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQGAmiczKhrLX8TqyY4aC3iaGsRvAoB9wnpXKh4cMdia9dKy1SJJNZwO3Bw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">寻求人类帮助。当遇到不明确的要求或没有提供必要的信息时，Babel Agent 会寻求人工帮助。此外在尝试不同方法后无法完成任务时，它也会做同样的事情。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">迭代开发。Babel Agent 支持对需求的迭代更改和对在线问题的自主纠正。</span></p></li></ul><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427585" data-ratio="1.9238578680203047" data-type="png" data-w="788" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicoXico1KbWYKMBrZwic3a4yQGeCPsr3Kg720iaEuukCNqe5z0e0YQzRYPUonF0AJI0IFiathvX7h5zLA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">未来，AI 智能体在编程行业还会掀起怎样的变革，我们拭目以待。</span></p><p style="margin-bottom: 0px;line-height: 1.75em;"><br></p><div mpa-from-tpl="t" style="margin-bottom: 0px;text-wrap: wrap;"><div mpa-from-tpl="t" style="padding: 10px;"><div mpa-from-tpl="t" style="padding: 15px;box-shadow: rgb(204, 204, 204) 0px 0px 5px;"><div mpa-from-tpl="t" style="padding-top: 0.8em;padding-right: 0.5em;padding-left: 0.5em;"><div data-autoskip="1" mpa-from-tpl="t" style="line-height: 1.5em;text-align: left;"><p style="line-height: 1.75em;margin-bottom: 24px;"><strong><span style="font-size: 15px;">来上海这场大模型技术workshop，一起探讨大模型的重点技术路径</span></strong></p><p style="line-height: 1.75em;margin-bottom: 24px;"><span style="font-size: 15px;">3月22日下午，来自复旦大学、波形智能、亚马逊云科技的专家学者和技术大咖们，将重点分享大模型能力对齐、长文本、Claude 3等议题[机智]</span></p><p style="line-height: 1.75em;margin-bottom: 24px;"><span style="font-size: 15px;">识别海报二维码或点击阅读原文即刻报名！</span></p><p style="margin-top: 15px;font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;"><img class="rich_pages wxw-img js_insertlocalimg" data-cropselx1="0" data-cropselx2="511" data-cropsely1="0" data-cropsely2="1172" data-imgfileid="503427593" data-ratio="1.96" data-s="300,640" data-type="png" data-w="1000" style="height: 1133px;text-align: center;font-size: var(--articleFontsize);outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);width: 578px;visibility: visible !important;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJulxfPxaicdespibBIXBibpz0w5UMdaHVgsojswSYtMWr1e3OT3n46rVLrw/640?wx_fmt=jpeg&amp;from=appmsg"><br></p></div></div></div></div></div><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">©&nbsp;THE END&nbsp;</span></p><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">转载请联系本公众号获得授权</span></p><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">投稿或寻求报道：content@jiqizhixin.com</span><span style="color: rgb(136, 136, 136);font-size: 12px;background-color: rgb(255, 255, 255);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p><a href="https://jiqizhixin.mike-x.com/Ame0q">阅读原文</a>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[流浪地球里的数字生命计划启动了？DeepMind在电脑里造果蝇，网友：能造人吗？]]></title>
        <id>2650911198_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650911198&amp;idx=1&amp;sn=013ff5201966234572a5e727985cc891&amp;chksm=84e475a0b393fcb66812a7ada92c3ebb4eea18216ba2b248dd6fdbb575498d6d0026b6980757#rd"/>
        <updated>2024-03-16T04:30:17.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">编辑：张倩</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="20" data-source-title="" style="outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;line-height: 1.75em;"><span style="color:#888888;">「<span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">质疑图恒宇，理解图恒宇，成为图恒宇。</span>」</span></p></div></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 0.034em;">在《流浪地球 2》中，刘德华饰演的图恒宇是一个令人印象深刻的角色。</span><span style="font-size: 15px;letter-spacing: 0.034em;">为了让在车祸中去世的女儿拥有「完整的一生」，他不顾人类世界对「数字生命计划」的禁令，一直在暗中独自努力完善数字生命的架构，并最终决定公然违规，将女儿的数据上传至量子计算机，之后因此被捕入狱。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427519" data-ratio="0.4185185185185185" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaA0ctAYTOLbnplqWZI4v8NYEQpicw5b0TH5F6NZhQY1LRoiaYZwuIVvCicQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">电影《流浪地球 2》中的数字生命图丫丫。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">电影上映后，有关「数字生命」的话题经过了很多讨论。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">最近，这个话题被再次提起，起因是不少失去亲友的人正在尝试用 AI 技术「复活」逝者，制作出一系列包含逝者形象的虚拟视频。一个「质疑图恒宇，理解图恒宇，成为图恒宇」的评论被赞上万次。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">不过，这些视频大多没有可互动性。按照现在的技术，即使设置了互动功能，屏幕里的人也会因为违背一些物理直觉而显得很假。毕竟，理解物理世界，并按照物理世界的规律去运动、思考依然是一个没有解决的问题，就连最近大火的 Sora 视频生成模型也被认为做不到这一点。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">更何况，能真正以假乱真的「数字生命」被认为是要拥有自主意识的。而意识是什么，如何构建，在科学界都没有达成共识。所以，真正的「数字生命」其实离我们还很遥远。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">不过，人工智能科学家和神经科学家正在联手，朝着这一方向努力。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>神经科学 + 人工智能：在电脑里造一只逼真果蝇</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427520" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="800" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAhG4G7hkCU2QibmmyFSeyQPJnw8TDmoVaAMQ8ibJ7k0KmbOpDlmwtBVww/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这是谷歌 DeepMind 和美国 Janelia 研究园区（霍华德・休斯医学研究所成立的神经科学研究机构）共同研究出的一个虚拟果蝇，它能像真实的果蝇一样行走和飞行。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3371550822211239944" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gW9jIolc3YgoBpALQc0eqsiaAeWsJnCKuU3ib9xk3H12nKdmbDPeCUjYibQtTpDmBR7erhCicPrOVRUXsQ%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3371550822211239944"></iframe></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.034em;">这段视频显示，虚拟果蝇再现了真实果蝇的飞行动作（自发转弯），执行以 2 厘米 / 秒的速度行走的命令，同时向左、向右转弯。模型还模仿了真实果蝇的行走轨迹，包括以不同速度行走、转弯和短暂停止。</span></em></span><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">同时，它也是迄今为止最逼真的果蝇模拟，结合了新的解剖学精确模型、快速物理模拟器和根据果蝇行为训练的人工神经网络，以模仿真实果蝇的动作。</span><span style="font-size: 15px;letter-spacing: 0.034em;">&nbsp;</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">除了在复杂的轨迹上行走和飞行，虚拟果蝇还能用眼睛控制和引导飞行。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「你获取真实的果蝇数据 —— 它们是如何飞行的，如何行走的 —— 训练网络来模仿这些动作，然后让我们训练好的这个网络来控制果蝇，告诉果蝇如何运动，」Janelia Turaga 实验室的机器学习研究员 Roman Vaxenburg 领导了这个项目。他说，「它就像一个小型大脑，控制着果蝇的动作」。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">新模型是该团队虚拟果蝇的首次迭代，他们计划利用更多的解剖和感官特征以及真实的神经网络使其更加逼真。这也是他们所希望实现的一系列逼真动物模型中的第一个。他们和其他研究人员现在可以利用这个通用的开源框架来开发这些模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这些模型可以帮助科学家更全面地了解神经系统、身体和环境是如何共同控制行为的。几十年来，研究人员一直在实验室中用真实动物探究这些问题，而逼真的虚拟模型将使科学家们能够了解所有这些组成部分是如何相互联系的，以及实验室中无法测量的因素 —— 比如飞行时施加在身体上的力是如何影响行为的。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「对身体的模拟可以告诉你神经系统的指令是如何转化为动作和行为的，而这个『如何』与身体的形状以及身体如何与世界互动的物理学有关，」该项目的高级科学家、Janelia 小组组长 Srinivas Turaga 说，「所有这些都编入了这个物理模拟中」。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3371551282963922948" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gW9jIolc3YgoBpALQc0eqsiaAAcIfvQWM4RbJvmKzfBRbI5cxKRaNyWDNS1bZ6wbKNaicJWESqeOvUfg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1280" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3371551282963922948"></iframe></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.034em;">这段视频显示了果蝇模型以 30 厘米 / 秒的速度在固定高度执行直飞指令，随后果蝇模型再现了真实果蝇的飞行动作：躲避感知到的威胁和自发转弯。接下来展示的是果蝇模型以 2 厘米 / 秒的速度执行行走指令，同时向左转和向右转，然后是果蝇模型模拟真实果蝇的行走轨迹，包括以不同速度行走、转弯和短暂停止。</span></em></span><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">新模型建立在以前模拟果蝇行为的工作基础上，包括使用简化的蝇体和手动控制系统来模拟飞行的「大统一果蝇（Grand Unified Fly）」。最近推出的 NeuroMechFly 使用逼真的身体模型和带有学习组件的手动控制系统来模拟行走。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在这项新研究中，Janelia 的研究人员和 DeepMind 的科学家在高级研究科学家 Yuval Tassa 和 Josh Merel 的领导下，着手改进果蝇模型的解剖学、生物力学、物理学和行为学信息，以创造出一种能够执行多种行为的更逼真的果蝇模拟。这项工作是 Janelia 和 DeepMind 之间的多项合作之一，双方利用各自在神经科学和人工智能领域的专业知识共同解决科学问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">DeepMind 高级研究总监 Matthew Botvinick 说：「尽管人们普遍承认，理解大脑功能取决于理解身体及其与其他物理对象的相互作用，但计算神经科学研究很少寻求在这种全局层面上模拟事物。在与 Srini 和这个团队的其他成员进行头脑风暴的过程中，我们意识到有一个令人兴奋的机会，可以在果蝇研究的背景下将所有的碎片整合在一起」。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>未来还会有虚拟小鼠、虚拟斑马鱼、虚拟……&nbsp; &nbsp;</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">整个果蝇的打造过程可以概括为：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">首先，Janelia 研究专家 Igor Siwanowicz 用显微镜对成年雌果蝇的各个部位进行了成像，并使用计算机软件构建了一个解剖学上精确的果蝇体外虚拟模型，其中包括果蝇关节和附肢的运动。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427521" data-ratio="1.3074074074074074" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAWicMe5rrmL41p4yia7rK94yEJhgsNx3yyXR4NQicB8x5UKS3V3T2iblSAQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">DeepMind 的研究人员 —— 包括 Tassa、Merel 和研究工程师 Guido Novati—— 将这一虚拟模型转化为代码，并输入 MuJoCo 模拟器，这是一个专为机器人和生物力学设计的快速开源物理模拟器。该工具使研究人员能够虚拟模拟现实世界中物体的运动和互动方式。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了支持果蝇模型，研究人员对模拟器进行了重大升级，包括附着力致动器，以模拟昆虫脚抓物体表面时产生的力。研究小组还请 Novati 设计了一个新的流体力学模型，用于描述果蝇在空中飞行时所受的力。该项目的资深作者 Tassa 说，该模型可以支持各种空气动力学行为，包括展翅飞翔。这里，他们用到了端到端强化学习。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427522" data-ratio="1.237037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaARK9WAFw56cywN3ZuvvYSEyUfnvlc4Wylf4AzvYZkhPlzT8EkLQ32rQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">飞行模拟。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427523" data-ratio="0.8898148148148148" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAjZCLWbZqLo9Q9NV0DQJkyMbliafSJmfibmGT8yHibO66RBmFVMKnuBJfw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">行走模拟。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427524" data-ratio="1.010185185185185" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAnI5XsEJEPdWnPvVUw3Bjib5dMeejTLAAxha7US7Q6qaGiaibmkXtEQHsw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">视觉引导飞行任务：高度控制和避障。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 0.034em;">&nbsp;</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Tassa 还表示：「由于作用在果蝇身上的力是如此微小，对这样一种小昆虫进行建模非常具有挑战性。」</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">接下来，Vaxenburg 建立了一个人工神经网络，并根据真实的果蝇行为对其进行训练，方法是向该网络提供由果蝇行为专家录制的视频信息，这些专家包括 Janelia 高级小组负责人 Kristin Branson 和 Michael Reiser、HHMI 研究员 Gwyneth Card 和加州理工学院教授 Michael Dickinson。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Vaxenburg 说：「我们的目标是提高逼真度，这通过两个方面的工作来实现：一个是改进解剖细节的捕捉，即果蝇的构造；另一个是捕捉行为，即果蝇的动作和反应。」</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427525" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="800" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAeTkh1XB7Vq2wDA1iaom74gIPoXoojyzQhrmg6xddZicvcH8icSLnlibRGA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">上图展示了果蝇模型的身体结构和自由度。果蝇模型由 67 个身体部分组成，由 66 个关节连接，相当于 102 个自由度。图中显示了所有自由度以正弦波方式运动的序列。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这个新模型只是一个开始。下一步，研究小组希望将果蝇解剖结构的其他部分（如肌肉和肌腱）以及逼真的感觉系统纳入虚拟昆虫中，从而创建一个更加逼真的果蝇模型。他们还希望能够使用真实的神经网络，如果蝇腹侧神经索连接体，为模型提供动力。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">鉴于研究小组已经证明他们能够创建这类逼真的虚拟模型，未来，他们还想创建虚拟小鼠和斑马鱼，这两种生物被神经科学家广泛研究。他们用来创建虚拟果蝇的流程也可供全球研究人员免费使用，使其他人也能创建自己的逼真模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Turaga 说：「我们已经展示了如何做到这一点，我们可以为另一种生物再做一次。」</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">目前，关于这一研究成果的论文已经在 bioRxiv 上发布。作者在论文摘要中写到，「动物的身体决定了神经系统如何产生行为。因此，对感觉运动行为的神经控制进行详细建模需要一个详细的身体模型。」为此，他们在 MuJoCo 物理引擎中提供了一个解剖学上详细的黑腹果蝇全身生物力学模型，也就是我们前面介绍的虚拟果蝇。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427526" data-ratio="0.6324074074074074" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaARXm1s6SRIfUE3HIxpDOmAK5yoqZRDs1ibeoRic0LnfQ3pruJ7GI7iafWw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://www.biorxiv.org/content/10.1101/2024.03.11.584515v1</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在社交媒体上，这项研究吸引了不少网友的关注，有人问：它会进化吗？还有人问：他们也能做个模拟人的模型吗？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427527" data-ratio="0.9592592592592593" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaA3PsmjLfFodoYVnLg19LKu7yknA4uGUt4Tg26ibiaa8xTK7LTnfd0NDFA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这些网友甚至提供了新的研究思路：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427528" data-ratio="0.21481481481481482" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAe4fptpdSuwlcDAsOFREdAasED6AicaXZicpQ07B1XNZo69HmQg5ksefg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="rich_pages wxw-img" data-imgfileid="503427529" data-ratio="0.2518518518518518" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAgJbibFceATwh7hIFxNkREp4cdLApzLoHhUCQs7kfU8IagpX0ukgQcEw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这一切看起来都很有前景。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427530" data-ratio="0.6657407407407407" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaA79rMia2pKPadf7j5Iic8o16USmdbE6jRhtD1IKQPVicgsCyJJsUD9RcmA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">不过，抛开这些尚且比较遥远的联想，这项果蝇研究对当前的医疗保健研究也很有意义，可以助力从药物发现到疫病建模等一系列领域的研究。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503427531" data-ratio="0.6268518518518519" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9jIolc3YgoBpALQc0eqsiaAgOghLSbykQboqJGicLHbu9ibb8DG8QFz1s8pbl5iaWv0hacymfO8YeHPQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">参考链接：https://www.janelia.org/news/artificial-intelligence-brings-a-virtual-fly-to-life</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><div mpa-from-tpl="t" style="margin-bottom: 0px;text-wrap: wrap;"><div mpa-from-tpl="t" style="padding: 10px;"><div mpa-from-tpl="t" style="padding: 15px;box-shadow: rgb(204, 204, 204) 0px 0px 5px;"><div mpa-from-tpl="t" style="padding-top: 0.8em;padding-right: 0.5em;padding-left: 0.5em;"><div data-autoskip="1" mpa-from-tpl="t" style="line-height: 1.5em;text-align: left;"><p style="line-height: 1.75em;margin-bottom: 24px;"><strong><span style="font-size: 15px;">来上海这场大模型技术workshop，一起探讨大模型的重点技术路径</span></strong></p><p style="line-height: 1.75em;margin-bottom: 24px;"><span style="font-size: 15px;">3月22日下午，来自复旦大学、波形智能、亚马逊云科技的专家学者和技术大咖们，将重点分享大模型能力对齐、长文本、Claude 3等议题[机智]</span></p><p style="line-height: 1.75em;margin-bottom: 24px;"><span style="font-size: 15px;">识别海报二维码或点击阅读原文即刻报名！</span></p><p style="margin-top: 15px;font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;"><img class="rich_pages wxw-img js_insertlocalimg" data-cropselx1="0" data-cropselx2="511" data-cropsely1="0" data-cropsely2="1172" data-imgfileid="503427547" data-ratio="1.96" data-s="300,640" data-type="png" data-w="1000" style="height: 1133px;text-align: center;font-size: var(--articleFontsize);outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);width: 578px;visibility: visible !important;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJulxfPxaicdespibBIXBibpz0w5UMdaHVgsojswSYtMWr1e3OT3n46rVLrw/640?wx_fmt=jpeg&amp;from=appmsg"><br></p></div></div></div></div></div><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">©&nbsp;THE END&nbsp;</span></p><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">转载请联系本公众号获得授权</span></p><p style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;text-align: center;"><span style="font-size: 12px;color: rgb(136, 136, 136);">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p><a href="https://jiqizhixin.mike-x.com/Ame0q">阅读原文</a>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[苹果大模型MM1杀入场：300亿参数、多模态、MoE架构，超半数作者是华人]]></title>
        <id>2650911073_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650911073&amp;idx=1&amp;sn=a915a4e1c32154400adeadbeb853925a&amp;chksm=84e4751fb393fc09a702a547b344f7daa5a5d639679f80f35b7333cdf86a44d1901d04444f06#rd"/>
        <updated>2024-03-15T04:42:41.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="47" data-source-title="" style="color: var(--weui-FG-1);text-wrap: wrap;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);">苹果也在搞自己的大型多模态基础模型，未来会不会基于该模型推出相应的文生图产品呢？我们拭目以待。</span></p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今年以来，苹果显然已经加大了对生成式人工智能（GenAI）的重视和投入。此前在 2024 苹果股东大会上，苹果 CEO 蒂姆・库克表示，今年将在 GenAI 领域实现重大进展。此外，苹果宣布放弃 10 年之久的造车项目之后，一部分造车团队成员也开始转向 GenAI。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">如此种种，苹果向外界传达了加注 GenAI 的决心。目前多模态领域的 GenAI 技术和产品非常火爆，尤以 OpenAI 的 Sora 为代表，苹果当然也想要在该领域有所建树。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今日，在一篇由多位作者署名的论文《MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training》中，苹果正式公布自家的多模态大模型研究成果 —— 这是一个具有高达 30B 参数的多模态 LLM 系列。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427409" data-ratio="0.537962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuQHK2kHTGL9eNVZ7UvNcmORXU1aWwQvy5MJPOdic3VZjdCrk0DxxGJHA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.09611.pdf</span><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队在论文中探讨了不同架构组件和数据选择的重要性。并且，通过对图像编码器、视觉语言连接器和各种预训练数据的选择，他们总结出了几条关键的设计准则。具体来讲，本文的贡献主要体现在以下几个方面。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">首先，研究者在模型架构决策和预训练数据选择上进行小规模消融实验，并发现了几个有趣的趋势。<strong><span style="font-size: 15px;color: rgb(61, 170, 214);">建模设计方面的重要性按以下顺序排列：图像分辨率、视觉编码器损失和容量以及视觉编码器预训练数据。</span></strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">其次，研究者使用三种不同类型的预训练数据：图像字幕、交错图像文本和纯文本数据。<strong><span style="font-size: 15px;color: rgb(61, 170, 214);">他们发现，当涉及少样本和纯文本性能时，交错和纯文本训练数据非常重要，而对于零样本性能，字幕数据最重要。</span></strong>这些趋势在监督微调（SFT）之后仍然存在，这表明预训练期间呈现出的性能和建模决策在微调后得以保留。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最后，<strong><span style="font-size: 15px;color: rgb(61, 170, 214);">研究者构建了 MM1，一个参数最高可达 300 亿（其他为 30 亿、70 亿）的多模态模型系列， 它由密集模型和混合专家（MoE）变体组成，不仅在预训练指标中实现 SOTA，在一系列已有多模态基准上监督微调后也能保持有竞争力的性能。</span></strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">具体来讲，预训练模型 MM1 在少样本设置下的字幕和问答任务上，要比 Emu2、Flamingo、IDEFICS 表现更好。监督微调后的 MM1 也在 12 个多模态基准上的结果也颇有竞争力。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">得益于大规模多模态预训练，MM1 在上下文预测、多图像和思维链推理等方面具有不错的表现。同样，MM1 在指令调优后展现出了强大的少样本学习能力。&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427410" data-ratio="0.7351851851851852" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJugkzevlsEC0DzyicAF7GiaTHibW8SZvhNHW4LQw8V873cib2HRiaSVWkEzGg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427411" data-ratio="0.5527777777777778" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJugJicS9LFAfVYzSYrtsT9UibanmIicTOyApH4LMLS8z7fqNWmVBX0LO6Lw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>方法概览：构建 MM1 的秘诀</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">构建高性能的 MLLM（Multimodal Large Language Model，多模态大型语言模型） 是一项实践性极高的工作。尽管高层次的架构设计和训练过程是清晰的，但是具体的实现方法并不总是一目了然。这项工作中，研究者详细介绍了为建立高性能模型而进行的消融。他们探讨了三个主要的设计决策方向：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">架构：研究者研究了不同的预训练图像编码器，并探索了将 LLM 与这些编码器连接起来的各种方法。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">数据：研究者考虑了不同类型的数据及其相对混合权重。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">训练程序：研究者探讨了如何训练 MLLM，包括超参数以及在何时训练模型的哪些部分。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;letter-spacing: 0.034em;">消融设置</span></strong><span style="font-size: 15px;letter-spacing: 0.034em;"></span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">由于训练大型 MLLM 会耗费大量资源，研究者采用了简化的消融设置。消融的基本配置如下：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">图像编码器：在 DFN-5B 和 VeCap-300M 上使用 CLIP loss 训练的 ViT-L/14 模型；图像大小为 336×336。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">视觉语言连接器：C-Abstractor ，含 144 个图像 token。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">预训练数据：混合字幕图像（45%）、交错图像文本文档（45%）和纯文本（10%）数据。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">语言模型：1.2B 变压器解码器语言模型。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了评估不同的设计决策，研究者使用了零样本和少样本（4 个和 8 个样本）在多种 VQA 和图像描述任务上的性能：COCO Cap tioning 、NoCaps 、TextCaps 、VQAv2 、TextVQA 、VizWiz 、GQA 和 OK-VQA。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">模型架构消融试验</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者分析了使 LLM 能够处理视觉数据的组件。具体来说，他们研究了（1）如何以最佳方式预训练视觉编码器，以及（2）如何将视觉特征连接到 LLM 的空间（见图 3 左）。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427412" data-ratio="0.4777777777777778" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuLgXXyZKEA1ooAv2Xjk69PbhskOStLbiaDmPicCn0ngLWp5iaYRlNXkS7w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">图像编码器预训练。在这一过程中，研究者主要消融了图像分辨率和图像编码器预训练目标的重要性。需要注意的是，与其他消融试验不同的是，研究者本次使用了 2.9B LLM（而不是 1.2B），以确保有足够的容量来使用一些较大的图像编码器。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">编码器经验：图像分辨率的影响最大，其次是模型大小和训练数据组成。如表 1 所示，将图像分辨率从 224 提高到 336，所有架构的所有指标都提高了约 3%。将模型大小从 ViT-L 增加到 ViT-H，参数增加了一倍，但性能提升不大，通常不到 1%。最后，加入 VeCap-300M （一个合成字幕数据集）后，在少样本场景中性能提升超过了 1%。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427413" data-ratio="0.5842592592592593" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuTV1Jb1avuN1aL02F3Tj4TMaKnNVib8bibsH1bvyTbOfgIPxpcatP1eQA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">视觉语言连接器和图像分辨率。该组件的目标是将视觉表征转化为 LLM 空间。由于图像编码器是 ViT，因此其输出要么是单一的嵌入，要么是一组与输入图像片段相对应的网格排列嵌入。因此，需要将图像 token 的空间排列转换为 LLM 的顺序排列。与此同时，实际的图像 token 表征也要映射到词嵌入空间。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">VL 连接器经验：视觉 token 数量和图像分辨率最重要，而 VL 连接器的类型影响不大。如图 4 所示，随着视觉 token 数量或 / 和图像分辨率的增加，零样本和少样本的识别率都会提高。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427414" data-ratio="0.3425925925925926" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJusDprLClZSYOjNSq28QP4mlAdrnkopFsttq7CLDBOFdSEIaGDakhJPw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">预训练数据消融试验</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">通常，模型的训练分为两个阶段：预训练和指令调优。前一阶段使用网络规模的数据，后一阶段则使用特定任务策划的数据。下面重点讨论了本文的预训练阶段，并详细说明研究者的数据选择（图 3 右）。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">有两类数据常用于训练 MLLM：由图像和文本对描述组成的字幕数据；以及来自网络的图像 - 文本交错文档。表 2 是数据集的完整列表：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427415" data-ratio="0.2722222222222222" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuEVPqctpiawLLtB4iaccNNQ1L8c2688jUW046Po7kiaIXMfxZAiamQRMALw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">数据经验 1：交错数据有助于提高少样本和纯文本性能，而字幕数据则能提高零样本性能。图 5a 展示了交错数据和字幕数据不同组合的结果。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;letter-spacing: 0.034em;">数据经验 2：</span><span style="font-size: 15px;letter-spacing: 0.034em;">纯文本数据有助于提高少样本和纯文本性能。</span><span style="font-size: 15px;letter-spacing: 0.034em;">如图 5b 所示，将纯文本数据和字幕数据结合在一起可提高少样本性能。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">数据经验 3：谨慎混合图像和文本数据可获得最佳的多模态性能，并保留较强的文本性能。图 5c 尝试了图像（标题和交错）和纯文本数据之间的几种混合比例。</span></p></li><li><p style="margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">数据经验 4：合成数据有助于少样本学习。如图 5d 所示，人工合成数据确实对少数几次学习的性能有不小的提升，绝对值分别为 2.4% 和 4%。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427416" data-ratio="1.0333333333333334" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJufnq9ZjjEp1AtXxFxHZcRgAibRzqj5Rz3ibQ5ribG0DJAl64ErUhClUJKA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong><span style="font-size: 16px;letter-spacing: 0.034em;">最终模型和训练方法</span></strong></span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者收集了之前的消融结果，确定 MM1 多模态预训练的最终配方：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">图像编码器：考虑到图像分辨率的重要性，研究者使用了分辨率为 378x378px 的 ViT-H 模型，并在 DFN-5B 上使用 CLIP 目标进行预训练；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">视觉语言连接器：由于视觉 token 的数量最为重要，研究者使用了一个有 144 个 token 的 VL 连接器。实际架构似乎不太重要，研究者选择了 C-Abstractor；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">数据：为了保持零样本和少样本的性能，研究者使用了以下精心组合的数据：45% 图像 - 文本交错文档、45% 图像 - 文本对文档和 10% 纯文本文档。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了提高模型的性能，研究者将 LLM 的大小扩大到 3B、7B 和 30B 个参数。所有模型都是在序列长度为 4096、每个序列最多 16 幅图像、分辨率为 378×378 的情况下，以 512 个序列的批量大小进行完全解冻预训练的。所有模型均使用 AXLearn 框架进行训练。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们在小规模、9M、85M、302M 和 1.2B 下对学习率进行网格搜索，使用对数空间的线性回归来推断从较小模型到较大模型的变化（见图 6），结果是在给定（非嵌入）参数数量 N 的情况下，预测出最佳峰值学习率 η：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427417" data-ratio="0.06666666666666667" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJu95pCibmKRaqSKcRRe5IgStldenyVpiatubrK8mUvHeRiaU2otVpQ6SEWw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">通过专家混合（MoE）进行扩展。在实验中，研究者进一步探索了通过在语言模型的 FFN 层添加更多专家来扩展密集模型的方法。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">要将密集模型转换为 MoE，只需将密集语言解码器替换为 MoE 语言解码器。为了训练 MoE，研究者采用了与密集骨干 4 相同的训练超参数和相同的训练设置，包括训练数据和训练 token。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">关于多模态预训练结果，研究者通过适当的提示对预先训练好的模型在上限和 VQA 任务上进行评估。表 3 对零样本和少样本进行了评估：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427418" data-ratio="1.1064814814814814" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJu5nRpdO3TicJXBeEnQcRGp1VfNOdfUXAGRZOFcYrsIxf7OCGmy4XBY7w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>监督微调结果</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最后，研究者介绍了预训练模型之上训练的监督微调（SFT）实验。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们遵循 LLaVA-1.5 和 LLaVA-NeXT，从不同的数据集中收集了大约 100 万个 SFT 样本。鉴于直观上，更高的图像分辨率会带来更好的性能，研究者还采用了扩展到高分辨率的 SFT 方法。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">监督微调结果如下：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">表 4 展示了与 SOTA 比较的情况，「-Chat」表示监督微调后的 MM1 模型。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">首先，平均而言，MM1-3B-Chat 和 MM1-7B-Chat 优于所有列出的相同规模的模型。MM1-3B-Chat 和 MM1-7B-Chat 在 VQAv2、TextVQA、ScienceQA、MMBench 以及最近的基准测试（MMMU 和 MathVista）中表现尤为突出。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">其次，研究者探索了两种 MoE 模型：3B-MoE（64 位专家）和 6B-MoE（32 位专家）。在几乎所有基准测试中，苹果的 MoE 模型都比密集模型取得了更好的性能。这显示了 MoE 进一步扩展的巨大潜力。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">第三，对于 30B 大小的模型，MM1-30B-Chat 在 TextVQA、SEED 和 MMMU 上的表现优于 Emu2-Chat37B 和 CogVLM-30B。与 LLaVA-NeXT 相比，MM1 也取得了具有竞争力的全面性能。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，LLaVA-NeXT 不支持多图像推理，也不支持少样本提示，因为每幅图像都表示为 2880 个发送到 LLM 的 token，而 MM1 的 token 总数只有 720 个。这就限制了某些涉及多图像的应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427419" data-ratio="0.9722222222222222" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuU82ECqNMp5p4d02TzEyjvOZFIrCHB53FvBVdp2dMGVdiacZ69ic0ibBcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">图 7b 显示，输入图像分辨率对 SFT 评估指标平均性能的影响，图 7c 显示，随着预训练数据的增加，模型的性能不断提高。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">图像分辨率的影响。图 7b 显示了输入图像分辨率对 SFT 评估指标平均性能的影响。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">预训练的影响：图 7c 显示，随着预训练数据的增加，模型的性能不断提高。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427420" data-ratio="0.5194444444444445" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWiczI7KhvDvztXsNW0bTjJJuT5C4RK0CxvzE5eqdu9ZeLCZUryscLjRLPn53ibkh6TRfo0KNPrDCBtg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">更多研究细节，可参考原论文。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503427421" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[星舰首次进入太空轨道，里程碑式突破：人类离火星旅行又近一步]]></title>
        <id>2650911014_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650911014&amp;idx=1&amp;sn=8d65c0e0f5a088ae8fd0c2dbbfa17e29&amp;chksm=84e47558b393fc4e7c14b3cb95b0e56934e9e2a324e7216927aa3c846f666783769df587be68#rd"/>
        <updated>2024-03-14T16:38:49.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="11" data-source-title="" style="color: var(--weui-FG-1);text-wrap: wrap;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);">它会带我们飞向外星吗？</span></p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;">成功入轨，星舰 Starship 这次向前迈进了一大步。</span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">美国东部时间 3 月 14 日早上 9 点 25 分，随着人们整齐一致的倒数声，星舰在得克萨斯州南部博卡奇卡海滩附近的 Starbase 基地顺利升空。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今天的发射准备工作较为顺利。在短暂推迟后，33 台猛禽发动机均成功点火并顺利升空。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427341" data-ratio="0.5516129032258065" data-s="300,640" data-type="gif" data-w="620" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpw9OUe1fMbpyzibjbx2gfnA3JF1F0mFXuvrcaJibw2MK41JX0Y3aibwvhQ/640?wx_fmt=gif&amp;from=appmsg"></p><p><span style="font-size: 15px;letter-spacing: 0.034em;"></span><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427342" data-ratio="0.5516129032258065" data-s="300,640" data-type="gif" data-w="620" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpe07kwKxtIEYDVceicuQt1PBJIPBG7J0QRCmvFMVFGEVePsSgO70ynibg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">火箭在 52 秒达到最大动压点（Max Q），这是一个航空术语，指飞行器在飞行过程中经历最大动态压力的位置。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427343" data-ratio="0.5368421052631579" data-s="300,640" data-type="gif" data-w="570" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpic0URsicKoSSRbvozNzibzAichsEQbX7XRf8phF1cwVibRoEiczEt04UOTGA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2 分 42 秒后，一二级火箭成功热分离。星舰飞船 SN28 启动发动机把自己推离了 Super Heavy 助推器。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427344" data-ratio="0.539543057996485" data-s="300,640" data-type="gif" data-w="569" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpwiab7Gc8j4RN1g1AgDvYxmaWOPKvOibiagicOqgMBTnw3r3vqZpNd23ZZQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">6 分钟左右，一级助推器开始返回但抖动过大不稳定，最终坠入海中。此时没有多少引擎处于点火状态，看起来有关回收的尝试没有成功。</span></p><p style="text-align: center;"><span style="font-size: 15px;letter-spacing: 0.034em;text-align: justify;"></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427345" data-ratio="0.5368421052631579" data-s="300,640" data-type="gif" data-w="570" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpnpNxZn9LaUAClGrZON3lAzzT6ZaCNaMuuhyD1wVbaIhJaCHbUbYFRQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427347" data-ratio="0.5368421052631579" data-type="gif" data-w="570" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibp4OjmSZXP5AbssMbF0k4z0S2R0kS1hO30QIDm6UibtNzTJq7h3CrCgFg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">而星舰飞船成功完成了发动机的整个燃烧过程，并在启动后 8 分 35 秒关闭了发动机。这足以让它进入轨道，这标志着一个巨大的里程碑，对于 SpaceX 来说无疑是一次胜利。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427348" data-ratio="0.5368421052631579" data-s="300,640" data-type="gif" data-w="570" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpJWjogfdTQAIEv1frl7h5VhASNs19oO0y2eKZcVLNhfDcVDiaSk8QUicg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 通讯经理丹・霍特（Dan Huot) 表示：「我们比以往任何时候都走得更远。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">19 分钟后，星舰飞船开始在距离地球表面超过 200 公里高度的太空滑行。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427350" data-ratio="0.54673721340388" data-s="300,640" data-type="gif" data-w="567" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpY8mL6OoTbICYviaU5s1dAjoXjOwFACwzgDzoFTy8U41Gtiawia85MJu5w/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克表示，星舰飞船已经达到轨道速度（上次没有达到），并向 SpaceX 团队发来祝贺。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427351" data-ratio="0.22087912087912087" data-s="300,640" data-type="png" data-w="910" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpibJMdSTHmPpcyLaINHLB530D541iahzIOCxDLCxnzMFlibcwd2V7Q8g5Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在 28 分钟左右时，<span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">SpaceX 表示在 170 公里的高度上，Starship 完成了有效载荷门测试，该测试旨在验证其将卫星等货物送入太空的能力。</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">但官方表示，仍需要进行一些「数据检查」。</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">这意味着试飞期间实现了另一个重要里程碑。</span></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 展示了星舰内部的特写：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427352" data-ratio="0.5368421052631579" data-s="300,640" data-type="gif" data-w="570" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpa2sQibXOloYsg9iarW4hAar6CGO2IIIRFjdIaNz9q8d5jibP6xYVWgCYw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在发射升空 40 分钟后，SpaceX 宣布本次测试的推进剂转移演示已经完成。根据 NASA 去年 12 月解释该测试的一封电子邮件，该目标是将星舰飞船上的 10 吨液氧加注到另一个液氧舱中。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 工程师设计该演示是为了讨论星际飞船在轨道上运行时如何在未来的任务中补充燃料。星舰可能需要十几次加油飞行才能到达月球。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这是个高难度的技术活。</span><span style="font-size: 15px;letter-spacing: 0.034em;">NASA 还告诉 CNN，SpaceX 可能因完成演示而获得 5340 万美元的「爆点奖」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">但是在再入大气层之前，SpaceX 选择不重新启动 Starship 的引擎，这项测试被取消了，我们尚不知道工程师做出决策的原因。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">46 分钟左右，星舰飞船重新进入地球大气层，返回大气层时的极端高温和压力形成了等离子「火焰」，现场再次响起欢呼声。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">宇宙飞船表面铺设的约 1.8 万块轻质六角形陶瓷砖承受住了再入大气的温度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427353" data-ratio="0.527336860670194" data-s="300,640" data-type="gif" data-w="567" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibp2LN1uglTjVQga7rgrGgWAfrdyWA0ZL6QjAqZSgDNnhiafMG9sXj4amA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: justify;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这应该是我们第一次看到飞行器大气层再入的实时图像。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427354" data-ratio="0.527336860670194" data-s="300,640" data-type="gif" data-w="567" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpxA0XWOwUL52Qiaxn4viahlOaDJpEG33GGK1WWJg8K6dCvKlLkHKrI6LA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">随着高度逐渐下降，Starship 进入了黑障区，在海拔 65 公里高度时视频信号中断。Dan Huot 表示，地面控制中心失去了星链和其他数据流，这意味着我们可能失去了与星舰的联系。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">「我们不知道它是否按照计划溅落在印度洋中，」SpaceX 工程师 Kate Tice 表示。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427355" data-ratio="0.527336860670194" data-s="300,640" data-type="gif" data-w="567" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpyTbumWq1W8jficWj099BxJxCztu4sjDWKQso9nbyHgbusCF8vFyUHXA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">随后，人们推定星舰在这一阶段烧毁。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这次任务宣告提前结束，但显然已取得了重大进步，是人类航天的又一里程碑式突破。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">工程师指出，未来的数小时和数天里，人们还要评估大量数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 这次的发射，让星舰整体入轨，宣告了多发动机捆绑式超大火箭的路线具有可行性。与此同时，在火箭的热分离、姿态控制、回收等方面也有了进步。难怪马斯克在任务结束后心情不错：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427356" data-ratio="1.1064814814814814" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibporTItUFsGQfnXeicWR1j9xicFkHdCZwekR3pGFCzVHoQTsaicAD8xia0XA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">已经迫不及待想要移民火星了？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Starship 的第三次全堆叠配置发射测试是迄今为止最雄心勃勃的一次。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427357" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpib0GJ74u0DErvHoL5v2xgyBTaBdLib7uTJRsovVzO8icbxg9ticHJMkeoA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">SpaceX 的第三艘 Starship 飞船竖立在该公司位于德克萨斯州南部的 Starbase 基地发射架上。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">上次的 Starship 试飞是在去年的 11 月，火箭在飞行后约 10 分钟发生爆炸。当时马斯克预测发射成功的概率是：像掷硬币一样，50% 左右。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这次飞行成功的几率要高得多。「我不想立 flag，但我认为到达轨道的可能性很大——有 80%，」马斯克说。「当然，第三次飞行是比前两次飞行更好的火箭。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">看起来，马斯克的预测很靠谱。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong><span style="font-size: 16px;letter-spacing: 0.034em;">不断尝试，走向成功</span></strong></span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这是星舰的第三次试飞，前面两次试飞都以大部分失败告终。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">星舰（Starship）是由 SpaceX 开发的重型运载火箭，于 2017 年 9 月由埃隆・马斯克首次公布，它的终极目标是打造一个完全可重复使用的运输系统，能够将乘员和货物送入地球轨道，帮助人类重返月球，并最终前往火星及更远的地方。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2023 年 4 月 20 日，<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650874709&amp;idx=1&amp;sn=dc84689d774ac939312d4d91a2f3ebe8&amp;chksm=84e4e72bb3936e3d5c986ab35d5dc03b74137b4d376a71fe8d46ce4f0d2ba63ebd59cf19e0c4&amp;scene=21#wechat_redirect" textvalue="星舰的完整版本首次尝试发射" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">星舰的完整版本首次尝试发射</a>，火箭飞行几分钟后在空中爆炸，严重破坏了地面基础设施并引发了环境问题。美国联邦航空局与美国鱼类和野生动物管理局甚至为此进行了协调，在为第二次尝试颁发新的飞行许可证之前启动了安全审查。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427359" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="800" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibphzQib7wZHGjYpgcxqUrYk5WAoWSkKiaI58d8v8eeP9JK8x6R0rD1RheQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><span style="display: none;line-height: 0px;">‍</span>去年 11 月<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650897635&amp;idx=1&amp;sn=18aec578d8ee9f0c57ca0f879df830ae&amp;chksm=84e4b89db393318b0b8a4fe17718368754676d1a0bec5844c7f73366d90d31780dc2ae56dfbe&amp;scene=21#wechat_redirect" textvalue="第二次试飞，星舰实现了成功的一二级分离" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">第二次试飞，星舰实现了成功的一二级分离</a>，但一级火箭在分离之后一直无法启动足够多的发动机控制姿态，在空中爆炸了，二级因速度不够导致切入轨道失败。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427358" data-ratio="0.5546875" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibp7ac5kQZJ9la3hDAM4QqsrbQGIrETiaSn7cw07sjKGJhhhLNS6Qp0J4A/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，这就是 SpaceX 快速迭代技术的方式，前两次的「暂时失败」提供了很多宝贵的经验。在第三次发射之前，工程师们早已为今天的发射设定了大量新的目标。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在全球范围内，星舰的每次发射都备受关注，因为这艘火箭和马斯克的火星梦紧紧地绑定在一起。沃尔特・艾萨克森写作的《埃隆・马斯克传》中记录了这一切。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">和之前的很多次试验一样，星舰的第一次发射以「爆炸」收尾：不仅对发射台造成了严重损坏，并「留下了 385 英亩的碎片场，将混凝土块抛到距离发射台 2680 英尺远的地方，引发了 3.5 英亩的火灾。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">但在马斯克的标准里，这次发射并不算失败。因为他事先声明，如果火箭能够顺利点火，升到足够高度，以至于即便爆炸也是发生在人们的视野外，还能给 SpaeX 留下大量有价值的新信息和新数据，他就认定这次试验性发射是成功的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">回看第一次发射视频时，SpaceX 发现，猛禽发动机的爆炸震碎了发射台的基座，将大块混凝土抛向空中，其中一些发动机可能被碎块击中了。而基座混凝土之所以被震碎并炸飞，是因为马斯克在建造发射台时决定不在发射架下方挖导焰槽（导焰槽可以容纳发动机点火后产生的爆炸冲击波），再加上发射时用来发挥替代作用的钢板没有准备好。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427360" data-ratio="0.6708074534161491" data-s="300,640" data-type="png" data-w="966" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibp341xxaf4LsPvQsIWybibTSib1mqib0k2b4mMojpqZI2RcJ2Okf6hibFBbA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);letter-spacing: 0.034em;">星舰第一次发射过程中炸飞的混凝土块。扬起的泥土和沙子像雨一样落到了 8 公里外，与火山爆发相当。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">「马斯克根据静态火灾试验的数据计算出高密度混凝土可以承受发射时的爆炸冲击波，就像他在早期版本的猎鹰 1 号中决定抛弃防晃隔板一样，事后证明冒这种险是错误的决定。」《埃隆・马斯克传》中写道。当然，几十台发动机并联这一高难度做法也可能是导致第一次发射失败的重要原因。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在第一次发射失败后，SpaceX 对星舰做出了数千次调整。其中一个升级是对发射台的升级。2023 年 6 月，在 X 上接受记者采访时，马斯克表示，除了用高强度混凝土加固，他们还给发射台安了一个「水冷钢板三明治」，即让两块厚钢板容纳发动机点火后产生的爆炸冲击波，并用一个像「巨大的倒置淋浴头」一样运行的冲水系统来冷却它们。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在第二次发射失败后，SpaceX 认为超重型助推器的损坏应归咎于过滤器堵塞，而液氧泄漏则是上级故障的原因。由 SpaceX 牵头的 FAA 调查总共确定了第三次发射之前需要采取的 17 项纠正措施，其中 7 项与超重型助推器有关，包括重新设计车辆硬件和改进控制系统模型，其余 10 项纠正措施与上级相关。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当时，对发射台的升级（包括水抑制系统）使其能够在起飞时的猛烈攻击中幸存下来，火箭的 33 个第一级发动机全部成功点火。飞行器成功完成了阶段分离，上级发动机也点火成功。但当助推器开始点燃 13 个发动机以将火箭带回地球时，其中一个发动机发生故障，SpaceX 的形容是「迅速引发意外的快速拆卸」，该航天器在泄漏导致火灾后失踪，自主机载飞行终止系统摧毁了航天器。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 解释说，在阶段分离之后，助推器开始下降阶段时，「几个发动机开始关闭，然后一个发动机出现能量故障」，由此引发了一系列事件，最终导致「超重型助推器」的毁灭。SpaceX 确定过滤器堵塞是罪魁祸首，发动机中的液氧堵塞了过滤器。这导致几个发动机关闭，并在飞行约三分半钟后发生爆炸，当时星舰正在墨西哥湾上空飞行 56 英里（90 公里）。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">至于第二级，它成功地从助推器中解脱出来，并飞行了近 7 分钟。然后，作为测试的一部分，它排出了为收集数据而装载的额外液氧（这通常不会在发射过程中进行）。然而，在排气过程中发生了泄漏，导致起火和与飞行计算机失去通信。这导致发动机提前关闭，自毁系统结束了任务。当上级达到 93 英里（150 千米）的高度时，测试结束。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了防止类似事件再次发生，SpaceX 公司「对即将推出的星舰飞行器进行了硬件改造，以改进减少泄漏、防火措施，并改进与推进剂排放口相关的操作，以提高可靠性」。此外，SpaceX 还补充说，猛禽发动机的液压转向系统将完全改为电动系统，这也消除了「潜在的可燃性来源」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>人类飞往火星的希望</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克认为，人类应该成为一个跨星际物种，以防止地球上可能发生的灾难性事件，如小行星撞击地球等，导致人类灭绝。他的终极梦想是让人类有朝一日能够移居火星，而他自己可以在「火星上退休」，甚至是「死在火星上」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，在查阅了 NASA 官网之后，他失望地发现：NASA 根本没有登陆火星的计划。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">所以在 2002 年，马斯克创建了自己的第三个公司：太空探索技术公司「Space X」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在经营 SpaceX 的过程中，马斯克做了很多打破常规的事情，比如采取一种迭代式的设计方法：迅速制成火箭和发动机原型，进行测试，炸毁，修改，再次尝试，直到最后做出能用的东西。他们希望通过这种方式尽快找出问题，然后解决问题。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这也是为什么我们在浏览 SpaceX 相关的新闻时，经常能够听到爆炸的消息。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">「最有可能的结果无非就是，我会因此倾家荡产。但我有别的选择吗？眼睁睁看着太空探索没有丝毫进展？我们必须试一试，否则人类将永远被困在地球上。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">SpaceX 的星舰飞行器及其超重型助推器是世界上最高、最强大的火箭。当它们堆叠在一起时，高度为 400 英尺（122 米）， 重约 7500 吨，由两部分组成：&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">底部 70 米高、配备 33 台「猛禽」发动机的「超重型推进器（Super Heavy）」，即一级。这一级主要负责将整个星舰系统推出地球大气层。按照计划，在完成这个任务后，一级火箭会分离，然后返回地球进行着陆，以便再次使用。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">顶部 50 米高、搭载 6 台「猛禽」发动机的星舰（Starship）飞船，即二级。这一级在一级火箭分离后，会继续前往太空执行任务。在长远规划中，这些任务可能包括将货物或乘客送到地球轨道、月球、火星，甚至是太阳系的其他地方。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427361" data-ratio="0.7481481481481481" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpicgE90QaPzJou8M45PEf1qx152cibjtSb6icEC5vcq6kgFKXKFOHx3rNQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">本次发射后，马斯克和 SpaceX 已经把最强运载火箭的名号从波音手里夺了回来。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">按照设计，两级都可以重复利用，发射一次消耗 5000 吨推进剂，是液氧和甲烷，采用全流程燃烧循环技术，燃料成本不到 50 万美元，设计运载能力 150 吨。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">除此之外，星舰主要使用不锈钢制成。这些特性带来了一系列技术挑战和限制，也降低了火箭发射的成本。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">星舰最终的成功，会承载起人类探索月球和火星的希望。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">纵观人类火箭发射的发展历程，SpaceX 从猎鹰到星舰的发展，开启了重复使用火箭的新时代，也在姿态感知、姿态控制、制导等方面验证了大量新技术。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">NASA 已承诺在星际飞船上投资 40 亿美元。根据美国宇航局目前的路线图，堆叠在超重型助推器顶部的星舰飞船计划在 2026 年进行的 NASA 阿尔忒弥斯 III 月球任务中发挥关键作用，完成该机构载人登月任务的最后一段，将宇航员从月球轨道上的航天器上载到月球表面。它最终还必须证明可以将多个「星际飞船加油机」送入轨道，为主星际飞船补充燃料，以继续其前往月球的旅程。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">因此，这次发射尝试的进展可能会对美国航天局的月球探索目标产生深远影响。现在看来，我们已获得了很多乐观的结果：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">Super Heavy 完成上升段飞行并在墨西哥湾动力溅落（部分成功）</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">Starship 达到轨道速度（成功）</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">开启、关闭 Starship 有效载荷舱门（部分成功）</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">在 Starship 滑行阶段进行推进剂转移演示（成功）</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">首次在太空中重新点燃猛禽发动机（未尝试）</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">控制星舰再入大气层（失败）</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427362" data-ratio="0.5222222222222223" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpDeSc6zlejtqIMjjNK65NJldDdPA3RWYKVDbX5nL2d9vbFPB37zmc4Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">星舰第三次发射的计划。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">技术的迭代还在继续，预计本年度 SpaceX 还会尝试发射 4-5 次星舰。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最后，3 月 14 日也是 SpaceX 成立的日子。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427363" data-ratio="0.5101851851851852" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpiamoSkrso7feI3m5LOIS9lOslFxHvJve4iaHRoQOV07xQdTEYrFiabDyQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今天这家公司在过 22 岁生日。</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;outline: 0px;max-width: 100%;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;white-space: normal;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;box-sizing: border-box !important;overflow-wrap: break-word !important;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503427365" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><div style="margin-bottom: 0px;"><p style="display: none;line-height: 1.75em;"><br></p></div><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[能说会看会行动，OpenAI机器人，一出手就是王炸]]></title>
        <id>2650910924_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910924&amp;idx=1&amp;sn=b1094de7dd507c8d885575408952bb63&amp;chksm=84e474b2b393fda4dbe888719f90fafbf0649c7ae191f76406d8f362febfbd729176bd286232#rd"/>
        <updated>2024-03-14T03:46:40.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之能报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);outline: 0px;font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="30" data-source-title=""><div class="js_blockquote_digest"><p>网友：波士顿动力要整点新舞步，才能让Figure 01下热搜。</p></div></blockquote><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">「借助 OpenAI 的能力，Figure 01 现在可以与人全面对话了！」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">本周三，半个硅谷都在投的明星机器人创业公司 Figure，发布了全新 OpenAI 大模型加持的机器人 demo。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="649" data-backw="578" data-imgfileid="100034333" data-ratio="1.1229166666666666" data-type="png" data-w="960" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOzoRxTUeNHD6iaY4pWd4aruxSQJKX5RBDCZ4vzXn2Z0PIDDvUm2tyC9g/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">这家公司在 3 月 1 日刚刚宣布获得 OpenAI 等公司的投资，才十几天就直接用上了 OpenAI 的多模态大模型。<br><br>如你所见，得到 OpenAI 大模型能力加持的 Figure 01 现在是这个样子的。</span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034352" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOHkD8sgKuI5XJoGt01MMTibD0icibYHRFu305K25kmHkdyBG23gFNVtZ0Q/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">它可以为听从人类的命令，递给人类苹果。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034353" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2x7ykukibqvLa5R1O043AnchoWmicO9842NvMGiaylGsiaN0wUibQnibBpSA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将黑色塑料袋收拾进框子里。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><span style="display: none;line-height: 0px;">‍</span><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034354" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOajOA1ZcURHMySSlY7icIRb5XDV7VOFLjjtibiaLMQrIGv8ZicNmdm755TA/640?wx_fmt=gif&amp;from=appmsg"><span style="display: none;line-height: 0px;">‍</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将杯子和盘子归置放在沥水架上。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="324" data-backw="578" data-imgfileid="100034355" data-ratio="0.5602941176470588" data-s="300,640" data-type="gif" data-w="680" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJFAUS6ge8wBNvBKicyoVegmKic88ZDW40hgr4VkxW6hicfyiaBgTA3ckuw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">需要强调的是：你看到的这一切，只用到了一个神经网络。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">完整的demo视频如下所示：<br></span></p><p class="channels_iframe_wrp wxw_wechannel_card_not_horizontal" style="margin-bottom: 24px;"><mp-common-videosnap class="js_uneditable custom_select_card channels_iframe videosnap_video_iframe" data-pluginname="mpvideosnap" data-url="https://findermp.video.qq.com/251/20304/stodownload?encfilekey=rjD5jyTuFrIpZ2ibE8T7YmwgiahniaXswqz4ibeYJVb2zMwibArvFLicdykibYRYMec9AGuE82B3jgVc7fPUherD5f11W6MI2KD5mDRe1fOYT7jYC0ib2FcevbgAag&amp;bizid=1023&amp;dotrans=0&amp;hy=SH&amp;idx=1&amp;m=&amp;scene=0&amp;token=x5Y29zUxcibChDEOZcZ62Tb4Faiar0GeqykGIu5HNrdaPkFB5AF74W0KaXnb5kSHLU3JcTtgbpKwE" data-headimgurl="http://wx.qlogo.cn/finderhead/PiajxSqBRaEKs6XzYjCzlSsfrOck5ZdKLHuqicYEiaI62Ty9EQxZmibuCQ/0" data-username="v2_060000231003b20faec8c7e5811fcbd2cc05eb34b077bf43ae33648ee0ea039da9063a594944@finder" data-nickname="机器之心机动组" data-desc="OpenAI机器人来了：给ChatGPT造个身体，能说会看会行动。机器人创业公司 Figure AI，发布了升级版机器人demo。得益于OpenAI的能力，Figure 01 现在可以与人全面对话，行动自如，条理清晰。机器人的进化越来越快了。
#OpenAI#FigureAI#机器人#具身智能#AI#人工智能#科技#前沿科技
" data-nonceid="11498428280630186292" data-type="video" data-mediatype="undefined" data-authiconurl="https://dldir1v6.qq.com/weixin/checkresupdate/icons_filled_channels_authentication_enterprise_a2658032368245639e666fb11533a600.png" data-from="new" data-width="1080" data-height="1920" data-id="export/UzFfAgtgekIEAQAAAAAACSkm_Xn48AAAAAstQy6ubaLX4KHWvLEZgBPE44I4Vhx9SeuEzNPgMIvc-PknJ5zpsQiMnOso10FC" data-isdisabled="0" data-errortips=""></mp-common-videosnap></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">广大网友在看到如此惊艳的 demo 后，对机器人的发展速度感到震惊，我们似乎正处在这场汹涌的进化浪潮中。甚至有人感叹，已经准备好迎接更多的机器人了。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="75" data-backw="578" data-imgfileid="100034338" data-ratio="0.13037974683544304" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOniaU3nXVUicrr4FQ7wxCLSwt37ybd3sHQ7qBerBwpBxVvKExicLqqqWiaw/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="90" data-backw="578" data-imgfileid="100034342" data-ratio="0.15569620253164557" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOGr8kjcWW4ViaKvhiaR1Y1LQUcjRCZvQOicibl0aibehjh89MLterKyna0xg/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="78" data-backw="578" data-imgfileid="100034339" data-ratio="0.1341772151898734" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOBP05bibn0lXOU6P8KAz15BOLdwRHXpDiaFR89Y1UQ6pqQm5wNXaQOicfg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">还有网友调侃道：「波士顿动力：好的，伙计们，这是一场真正的竞争。让我们回到实验室，设计更多舞蹈套路。」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="70" data-backw="578" data-imgfileid="100034340" data-ratio="0.12080536912751678" data-type="png" data-w="894" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOcpTp06Qz8NqYTD6IRyufvkjZn7zT6jleCM6L8H4KLwI2symlWP6SHg/640?wx_fmt=png&amp;from=appmsg"></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">所有这些，全是机器人自学的！</span></strong><span style="font-size: 15px;"></span></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure创始人Brett Adcock表示，视频中Figure 01展示了端到端神经网络框架下与人类的对话，没有任何远程操作。并且，机器人的速度有了显著的提升，开始接近人类的速度。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="300" data-backw="578" data-imgfileid="100034341" data-ratio="0.5189873417721519" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2oRgOAzEbmh1mFGKdmRCPmeyITMrU2rP503ywGOTkCqicuiaOtVp8EEg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure机器人操作高级AI工程师Corey Lynch介绍了此次Figure 01的技术原理。他表示，Figure 01现在可以做到以下这些：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其视觉体验</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">规划未来的行动</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">反思自己的记忆</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">口头解释推理过程</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="292" data-backw="578" data-imgfileid="100034343" data-ratio="0.5050632911392405" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOiaJ7537xTLEOxJBdljFK8dhWD9xyAJh6PcaymjlCVlDLV4zvdJT62xA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>他接着解释道，视频中机器人的所有行为都是学到的（再次强调不是远程操作），并以正常速度（1.0x）运行。<br><br>在具体实现过程中，他们将机器人摄像头中的图像输入，并将机载麦克风捕获的语音文本转录到由 OpenAI训练的大型多模态模型中，该模型可以理解图像和文本。该模型对整个对话记录进行处理，包括过去的图像，从而获得语言响应，然后通过文本到语音的方式将其回复给人类。<br><br>此外，该模型负责决定在机器人上运行哪些学习到的闭环行为以完成给定的命令，从而将特定的神经网络权重加载到GPU上并执行策略。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="325" data-backw="578" data-imgfileid="100034347" data-ratio="0.562037037037037" data-type="png" data-w="1080" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJSduLlQ3Tq4RZCUAzd7oqdZytLSguriceWHLuRyvKOYhX4UK31sicdOA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>将Figure 01 连接到大型预训练多模态模型为其提供了一些有趣的新功能。Figure 01 + OpenAI 现在可以：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其周围环境。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">使用常识推理做出决定。例如，「桌子上的盘子和杯子等餐具接下来可能需要放进沥水架」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将「我饿了」等模棱两可的高级请求转化为一些适合上下文的行为，例如「递给对方一个苹果」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">用简单的英语描述为什么它执行特定的操作。例如，「这是我可以从桌子上为您提供的唯一可食用物品」。</span></p></li></ul><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="320" data-backw="578" data-imgfileid="100034358" data-ratio="0.553125" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOC0qGfofubbiciaBN98AqeRdKUicmBqtXNLfDto9vN0YF3LiazHKx0HKaJw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">理解对话历史的大型预训练模型为Figure 01提供了强大的短期记忆。<br><br>考虑一个简单的问题：「你能把它们放在那里吗？」<br><br>其中 「它们」指的是什么？「那里」又是哪里？正确回答这个问题需要反思记忆的能力。<br><br>通过预训练模型分析对话的图像和文本历史记录，Figure 01快速形成并执行计划：1）将杯子放在沥水架上，2）将盘子放在沥水架上。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100034359" data-ratio="0.5485294117647059" data-s="300,640" data-type="gif" data-w="680" style="width: 578px;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOFjWbfhQ0ayhXgpEzCep3MqvHOuicxOoVvvibULBXtDRnAcLoJPay2ReQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">关于学到的低级双手操作，所有行为均由神经网络视觉运动transformer策略驱动，将像素直接映射到动作。这些网络以10hz 的频率接收机载图像，并以200hz的频率生成 24-DOF 动作（手腕姿势和手指关节角度）。<br><br>这些动作充当高速「设定点」，以供更高速率的全身控制器跟踪。这是一个有用的关注点分离，其中：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">互联网预训练模型对图像和文本进行常识推理，以得出高级规划。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">学习到的视觉运动策略执行计划，执行难以手动指定的快速反应行为，例如在任何位置操纵可变形的袋子。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">全身控制器确保安全、稳定的动力，例如保持平衡。</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最后他表示，即使在几年前，自己还认为人形机器人规划和执行自身完全学得行为的同时与人类进行完整的对话是几十年后才能看到的事情。显然，现在已经发生了太多变化。</span></p><p><img class="rich_pages wxw-img" data-backh="246" data-backw="578" data-imgfileid="100034344" data-ratio="0.4253164556962025" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOWFybJb0PvbKM01y3Cx6cukMBksYabr4Rp77s691pa4bDNF6WBtTjJw/640?wx_fmt=png&amp;from=appmsg"><br></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">至于声音方面，大家都在猜机器人金属感十足的声音源自谁？有猜乔布斯的、Sam Altman的，也有猜演员 Rob Lowe 的，你认为呢？</span></h3><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427199" data-ratio="0.28703703703703703" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpn1icDIkzvxdqnpib88gDMiavEnCibeH9cf47vcIlGD31giaCibfZf2wgNHIQ/640?wx_fmt=png&amp;from=appmsg"></p><p><strong><span style="font-size: 15px;"></span></strong></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">Figure，具身智能时代最热创业公司</span></strong></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最近，生成式 AI 的竞争正在走向长文本、多模态，各家科技公司和机构也没有忘记投资下个热点——具身智能。<br><br>具身智能，对于计算机视觉、机器人等领域来说是一个很有挑战的目标：假设 AI 智能体（机器人）不仅能接收来自数据集的静态图像，还能在三维虚拟世界甚至真实环境中四处移动，并与周围环境交互，那我们就会迎来技术的一次重大突破，从识别图像等机器学习的简单能力，转变到学习如何通过多个步骤执行复杂的类人任务。<br><br><a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzUyODA3MDUwMA==&amp;mid=2247517811&amp;idx=1&amp;sn=2293011e21dec9ba484fea7e1167883a&amp;chksm=fa772478cd00ad6e4e81d102158615f05a105f0a4465a89116ac8f56f039b4399b818c6c825b&amp;scene=21#wechat_redirect" textvalue="被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。</a><br><br>3月1日，Figure 宣布完成惊人的 6.75 亿美元 B 轮融资，公司估值达到 26 亿美元。一眼望去，感觉半个硅谷都投了它：微软、英特尔、OpenAI Startup Fund、Amazon Industrial Innovation Fund 、英伟达、贝索斯、「木头姐」的方舟投资、Parkway Venture Capital、Align Ventures 等。<br><br>该公司的产品 Figure 01，据称是世界上第一个具有商业可行性的自主人形机器人，身高 1.5 米，体重 60 公斤，可承载 20 公斤货物，采用电机驱动。它的可工作时长是 5 小时，行走速度每秒 1.2 米，可以说很多指标已经接近人类。<br><br>自 2023 年 1 月以来，人们对 Figure 的关注度一直在上升。虽然到目前为止，公司一共才发布过四个 demo 视频。其中的一个展示了 Figure 01 是如何制作咖啡的：<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="299" data-backw="400" data-imgfileid="100034360" data-ratio="0.7475" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO7WwYaEqtkIgPePCz7EBpPlXoKUDuYqxtOjYwqmaVaKI46V6QyX0ibbg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">据Figure表示，机器人练习这些动作的方法是端到端的，神经网络的训练时间是10小时。<br><br>在 2 月 27 日的视频里，Figure 01 自主完成了一个典型的物流环节任务——搬运空箱。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="236" data-backw="400" data-imgfileid="100034361" data-ratio="0.59" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO0iblsjb3ibxWAyZic6exC96GdrfvWib23N1glvz6mOBVvux6hWEuI4L5jw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">当然，速度还是比人类慢了很多。不过在这些任务中，Figure 01 都是完全自主地执行任务。所谓「完全自主」，是指只需将机器人放在地面上（无论放在屋里什么地方），在没有其他用户输入的情况下，直接按开始就行。<br><br>在训练过的大型视觉语言模型( VLM )帮助下，人形机器人会先识别、定位目标箱子，然后推理合适的拿放姿势。接下来，Figure 01 会导航自己到目标跟前，检测抓取点和手部力量，尝试抓取成功并将箱子放到传送带上。<br><br>这些技术亮点也是 Figure 和一直希望回归机器人领域的 OpenAI 达成合作协议的重要原因之一——将 OpenAI 的研究与 Figure 的机器人经验结合起来，为人形机器人开发下一代 AI 模型。OpenAI 也希望将自己的高性能多模态大模型扩展到机器人领域。<br><br>除了接受大笔风投之外，Figure 也在积极拓展落地场景。目前，Figure 01 已经开始在宝马位于南卡罗来纳州斯帕坦堡的汽车工厂接受测试，人们计划让机器人替代人类从事一些危险度高的任务。<br><br></span><span style="font-size: 12px;color: rgb(136, 136, 136);"><em>参考链接：<br>https://twitter.com/i/status/1767913661253984474<br>https://www.figure.ai/</em></span><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="480" data-backw="578" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="480" data-imgfileid="100034328" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: auto;width: 100%;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sJmFS59bKcLo002wiazYfxKSDOX9w6AXE4xF7eJ34ibNKbeLjVFg88WNNIhZiaexbXz8iaTyK45MCkHOQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p class="mp_profile_iframe_wrp" style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="2" data-is_biz_ban="0"></mp-common-profile></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="margin-bottom: 24px;"><br></p><p style="display: none;margin-bottom: 24px;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注生成AI用例的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI机器人，一出手就是王炸]]></title>
        <id>2650910844_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910844&amp;idx=1&amp;sn=232554de8c9b7dec1e7147e6be7861ac&amp;chksm=84e47402b393fd141d688acc561a009aa2fafa47e6a4d11e7604de3fe3c137e935f6745a0cce#rd"/>
        <updated>2024-03-13T17:26:12.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之能报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);outline: 0px;font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">作者：机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="30" data-source-title=""><div class="js_blockquote_digest"><p>网友：波士顿动力要整点新舞步，才能让Figure 01下热搜。</p></div></blockquote><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">「借助 OpenAI 的能力，Figure 01 现在可以与人全面对话了！」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">本周三，半个硅谷都在投的明星机器人创业公司 Figure，发布了全新 OpenAI 大模型加持的机器人 demo。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="649" data-backw="578" data-imgfileid="100034333" data-ratio="1.1229166666666666" data-type="png" data-w="960" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOzoRxTUeNHD6iaY4pWd4aruxSQJKX5RBDCZ4vzXn2Z0PIDDvUm2tyC9g/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">这家公司在 3 月 1 日刚刚宣布获得 OpenAI 等公司的投资，才十几天就直接用上了 OpenAI 的多模态大模型。<br><br>如你所见，得到 OpenAI 大模型能力加持的 Figure 01 现在是这个样子的。</span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034352" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOHkD8sgKuI5XJoGt01MMTibD0icibYHRFu305K25kmHkdyBG23gFNVtZ0Q/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">它可以为听从人类的命令，递给人类苹果。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034353" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2x7ykukibqvLa5R1O043AnchoWmicO9842NvMGiaylGsiaN0wUibQnibBpSA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将黑色塑料袋收拾进框子里。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><span style="display: none;line-height: 0px;">‍</span><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034354" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOajOA1ZcURHMySSlY7icIRb5XDV7VOFLjjtibiaLMQrIGv8ZicNmdm755TA/640?wx_fmt=gif&amp;from=appmsg"><span style="display: none;line-height: 0px;">‍</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将杯子和盘子归置放在沥水架上。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="324" data-backw="578" data-imgfileid="100034355" data-ratio="0.5602941176470588" data-s="300,640" data-type="gif" data-w="680" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJFAUS6ge8wBNvBKicyoVegmKic88ZDW40hgr4VkxW6hicfyiaBgTA3ckuw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">需要强调的是：你看到的这一切，只用到了一个神经网络。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">完整的demo视频如下所示：<br></span></p><p class="channels_iframe_wrp wxw_wechannel_card_not_horizontal" style="margin-bottom: 24px;"><mp-common-videosnap class="js_uneditable custom_select_card channels_iframe videosnap_video_iframe" data-pluginname="mpvideosnap" data-url="https://findermp.video.qq.com/251/20304/stodownload?encfilekey=rjD5jyTuFrIpZ2ibE8T7YmwgiahniaXswqz4ibeYJVb2zMwibArvFLicdykibYRYMec9AGuE82B3jgVc7fPUherD5f11W6MI2KD5mDRe1fOYT7jYC0ib2FcevbgAag&amp;bizid=1023&amp;dotrans=0&amp;hy=SH&amp;idx=1&amp;m=&amp;scene=0&amp;token=AxricY7RBHdVRjxoxDtf5SfxGib71c6kv6S0eSdxQtu1T0m6BibUFlevxjYI5qHGomWappHTQsXgoE" data-headimgurl="http://wx.qlogo.cn/finderhead/PiajxSqBRaEKs6XzYjCzlSsfrOck5ZdKLHuqicYEiaI62Ty9EQxZmibuCQ/0" data-username="v2_060000231003b20faec8c7e5811fcbd2cc05eb34b077bf43ae33648ee0ea039da9063a594944@finder" data-nickname="机器之心机动组" data-desc="OpenAI机器人来了：给ChatGPT造个身体，能说会看会行动。机器人创业公司 Figure AI，发布了升级版机器人demo。得益于OpenAI的能力，Figure 01 现在可以与人全面对话，行动自如，条理清晰。机器人的进化越来越快了。
#OpenAI#FigureAI#机器人#具身智能#AI#人工智能#科技#前沿科技
" data-nonceid="11498428280630186292" data-type="video" data-mediatype="undefined" data-authiconurl="https://dldir1v6.qq.com/weixin/checkresupdate/icons_filled_channels_authentication_enterprise_a2658032368245639e666fb11533a600.png" data-from="new" data-width="1080" data-height="1920" data-id="export/UzFfAgtgekIEAQAAAAAACSkm_Xn48AAAAAstQy6ubaLX4KHWvLEZgBPE44I4Vhx9SeuEzNPgMIvc-PknJ5zpsQiMnOso10FC" data-isdisabled="0" data-errortips=""></mp-common-videosnap></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">广大网友在看到如此惊艳的 demo 后，对机器人的发展速度感到震惊，我们似乎正处在这场汹涌的进化浪潮中。甚至有人感叹，已经准备好迎接更多的机器人了。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="75" data-backw="578" data-imgfileid="100034338" data-ratio="0.13037974683544304" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOniaU3nXVUicrr4FQ7wxCLSwt37ybd3sHQ7qBerBwpBxVvKExicLqqqWiaw/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="90" data-backw="578" data-imgfileid="100034342" data-ratio="0.15569620253164557" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOGr8kjcWW4ViaKvhiaR1Y1LQUcjRCZvQOicibl0aibehjh89MLterKyna0xg/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="78" data-backw="578" data-imgfileid="100034339" data-ratio="0.1341772151898734" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOBP05bibn0lXOU6P8KAz15BOLdwRHXpDiaFR89Y1UQ6pqQm5wNXaQOicfg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">还有网友调侃道：「波士顿动力：好的，伙计们，这是一场真正的竞争。让我们回到实验室，设计更多舞蹈套路。」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="70" data-backw="578" data-imgfileid="100034340" data-ratio="0.12080536912751678" data-type="png" data-w="894" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOcpTp06Qz8NqYTD6IRyufvkjZn7zT6jleCM6L8H4KLwI2symlWP6SHg/640?wx_fmt=png&amp;from=appmsg"></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">所有这些，全是机器人自学的！</span></strong><span style="font-size: 15px;"></span></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure创始人Brett Adcock表示，视频中Figure 01展示了端到端神经网络框架下与人类的对话，没有任何远程操作。并且，机器人的速度有了显著的提升，开始接近人类的速度。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="300" data-backw="578" data-imgfileid="100034341" data-ratio="0.5189873417721519" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2oRgOAzEbmh1mFGKdmRCPmeyITMrU2rP503ywGOTkCqicuiaOtVp8EEg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure机器人操作高级AI工程师Corey Lynch介绍了此次Figure 01的技术原理。他表示，Figure 01现在可以做到以下这些：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其视觉体验</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">规划未来的行动</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">反思自己的记忆</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">口头解释推理过程</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="292" data-backw="578" data-imgfileid="100034343" data-ratio="0.5050632911392405" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOiaJ7537xTLEOxJBdljFK8dhWD9xyAJh6PcaymjlCVlDLV4zvdJT62xA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>他接着解释道，视频中机器人的所有行为都是学到的（再次强调不是远程操作），并以正常速度（1.0x）运行。<br><br>在具体实现过程中，他们将机器人摄像头中的图像输入，并将机载麦克风捕获的语音文本转录到由 OpenAI训练的大型多模态模型中，该模型可以理解图像和文本。该模型对整个对话记录进行处理，包括过去的图像，从而获得语言响应，然后通过文本到语音的方式将其回复给人类。<br><br>此外，该模型负责决定在机器人上运行哪些学习到的闭环行为以完成给定的命令，从而将特定的神经网络权重加载到GPU上并执行策略。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="325" data-backw="578" data-imgfileid="100034347" data-ratio="0.562037037037037" data-type="png" data-w="1080" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJSduLlQ3Tq4RZCUAzd7oqdZytLSguriceWHLuRyvKOYhX4UK31sicdOA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>将Figure 01 连接到大型预训练多模态模型为其提供了一些有趣的新功能。Figure 01 + OpenAI 现在可以：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其周围环境。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">使用常识推理做出决定。例如，「桌子上的盘子和杯子等餐具接下来可能需要放进沥水架」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将「我饿了」等模棱两可的高级请求转化为一些适合上下文的行为，例如「递给对方一个苹果」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">用简单的英语描述为什么它执行特定的操作。例如，「这是我可以从桌子上为您提供的唯一可食用物品」。</span></p></li></ul><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="320" data-backw="578" data-imgfileid="100034358" data-ratio="0.553125" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOC0qGfofubbiciaBN98AqeRdKUicmBqtXNLfDto9vN0YF3LiazHKx0HKaJw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">理解对话历史的大型预训练模型为Figure 01提供了强大的短期记忆<br><br>考虑一个简单的问题：「你能把它们放在那里吗？」<br><br>其中 「它们」指的是什么？「那里」又是哪里？正确回答这个问题需要反思记忆的能力。<br><br>通过预训练模型分析对话的图像和文本历史记录，Figure 01快速形成并执行计划：1）将杯子放在沥水架上，2）将盘子放在沥水架上。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100034359" data-ratio="0.5485294117647059" data-s="300,640" data-type="gif" data-w="680" style="width: 578px;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOFjWbfhQ0ayhXgpEzCep3MqvHOuicxOoVvvibULBXtDRnAcLoJPay2ReQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">关于学到的低级双手操作，所有行为均由神经网络视觉运动transformer策略驱动，将像素直接映射到动作。这些网络以10hz 的频率接收机载图像，并以200hz的频率生成 24-DOF 动作（手腕姿势和手指关节角度）。<br><br>这些动作充当高速「设定点」，以供更高速率的全身控制器跟踪。这是一个有用的关注点分离，其中：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">互联网预训练模型对图像和文本进行常识推理，以得出高级规划。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">学习到的视觉运动策略执行计划，执行难以手动指定的快速反应行为，例如在任何位置操纵可变形的袋子。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">全身控制器确保安全、稳定的动力，例如保持平衡。</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最后他表示，即使在几年前，自己还认为人形机器人规划和执行自身完全学得行为的同时与人类进行完整的对话是几十年后才能看到的事情。显然，现在已经发生了太多变化。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="246" data-backw="578" data-imgfileid="100034344" data-ratio="0.4253164556962025" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOWFybJb0PvbKM01y3Cx6cukMBksYabr4Rp77s691pa4bDNF6WBtTjJw/640?wx_fmt=png&amp;from=appmsg"><br></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">Figure，具身智能时代最热创业公司</span></strong></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最近，生成式 AI 的竞争正在走向长文本、多模态，各家科技公司和机构也没有忘记投资下个热点——具身智能。<br><br>具身智能，对于计算机视觉、机器人等领域来说是一个很有挑战的目标：假设 AI 智能体（机器人）不仅能接收来自数据集的静态图像，还能在三维虚拟世界甚至真实环境中四处移动，并与周围环境交互，那我们就会迎来技术的一次重大突破，从识别图像等机器学习的简单能力，转变到学习如何通过多个步骤执行复杂的类人任务。<br><br><a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzUyODA3MDUwMA==&amp;mid=2247517811&amp;idx=1&amp;sn=2293011e21dec9ba484fea7e1167883a&amp;chksm=fa772478cd00ad6e4e81d102158615f05a105f0a4465a89116ac8f56f039b4399b818c6c825b&amp;scene=21#wechat_redirect" textvalue="被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。</a><br><br>3月1日，Figure 宣布完成惊人的 6.75 亿美元 B 轮融资，公司估值达到 26 亿美元。一眼望去，感觉半个硅谷都投了它：微软、英特尔、OpenAI Startup Fund、Amazon Industrial Innovation Fund 、英伟达、贝索斯、「木头姐」的方舟投资、Parkway Venture Capital、Align Ventures 等。<br><br>该公司的产品 Figure 01，据称是世界上第一个具有商业可行性的自主人形机器人，身高 1.5 米，体重 60 公斤，可承载 20 公斤货物，采用电机驱动。它的可工作时长是 5 小时，行走速度每秒 1.2 米，可以说很多指标已经接近人类。<br><br>自 2023 年 1 月以来，人们对 Figure 的关注度一直在上升。虽然到目前为止，公司一共才发布过四个 demo 视频。其中的一个展示了 Figure 01 是如何制作咖啡的：<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="299" data-backw="400" data-imgfileid="100034360" data-ratio="0.7475" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO7WwYaEqtkIgPePCz7EBpPlXoKUDuYqxtOjYwqmaVaKI46V6QyX0ibbg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">据Figure表示，机器人练习这些动作的方法是端到端的，神经网络的训练时间是10小时。<br><br>在 2 月 27 日的视频里，Figure 01 自主完成了一个典型的物流环节任务——搬运空箱。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="236" data-backw="400" data-imgfileid="100034361" data-ratio="0.59" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO0iblsjb3ibxWAyZic6exC96GdrfvWib23N1glvz6mOBVvux6hWEuI4L5jw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">当然，速度还是比人类慢了很多。不过在这些任务中，Figure 01 都是完全自主地执行任务。所谓「完全自主」，是指只需将机器人放在地面上（无论放在屋里什么地方），在没有其他用户输入的情况下，直接按开始就行。<br><br>在训练过的大型视觉语言模型( VLM )帮助下，人形机器人会先识别、定位目标箱子，然后推理合适的拿放姿势。接下来，Figure 01 会导航自己到目标跟前，检测抓取点和手部力量，尝试抓取成功并将箱子放到传送带上。<br><br>这些技术亮点也是 Figure 和一直希望回归机器人领域的 OpenAI 达成合作协议的重要原因之一——将 OpenAI 的研究与 Figure 的机器人经验结合起来，为人形机器人开发下一代 AI 模型。OpenAI 也希望将自己的高性能多模态大模型扩展到机器人领域。<br><br>除了接受大笔风投之外，Figure 也在积极拓展落地场景。目前，Figure 01 已经开始在宝马位于南卡罗来纳州斯帕坦堡的汽车工厂接受测试，人们计划让机器人替代人类从事一些危险度高的任务。<br><br></span><span style="font-size: 12px;color: rgb(136, 136, 136);"><em>参考链接：<br>https://twitter.com/i/status/1767913661253984474<br>https://www.figure.ai/</em></span><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="480" data-backw="578" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="480" data-imgfileid="100034328" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: auto;width: 100%;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sJmFS59bKcLo002wiazYfxKSDOX9w6AXE4xF7eJ34ibNKbeLjVFg88WNNIhZiaexbXz8iaTyK45MCkHOQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p class="mp_profile_iframe_wrp" style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="2" data-is_biz_ban="0"></mp-common-profile></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="margin-bottom: 24px;"><br></p><p style="display: none;margin-bottom: 24px;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注生成AI用例的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[向数字世界AGI迈进！智能体已经从头开玩「荒野大镖客 2」了]]></title>
        <id>2650910796_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910796&amp;idx=1&amp;sn=405be9dd0b0d200e4fad13a91b2c2a98&amp;chksm=84e47432b393fd24e2a980e131d265b5e9575557335c95250d22ff77d581a7a59e226f91c1aa#rd"/>
        <updated>2024-03-13T04:25:08.000Z</updated>
        <summary type="html"><![CDATA[<p style="text-align: justify;line-height: 1.75em;"><br></p><div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 29.75px;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;"><span style="display: none;line-height: 0px;">‍</span>机器之心发布</span></p><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 29.75px;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><p style="text-align: justify;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3366566767409479682" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWic8lfCWeamg5rKlyJibKe87nD6hw0PJH9ejlGIXcAs0D6MYH4eicuiaaic9HRDPKaribe8mibI83vQ74qgg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1920" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3366566767409479682"></iframe></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 12px;color: rgb(136, 136, 136);">Video: Cradle从头开始完成主线任务<strong><br></strong></span></p><p style="text-align: center;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">通用计算机控制</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">信息革命产生了数字世界，数字世界为大模型的诞生提供了数据，也最容易实现通用人工智能（AGI）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin: 3pt 0pt;text-align: center;font-family: 等线;font-size: 12pt;line-height: 16px;"><span style="display: inline-block;overflow: hidden;transform: rotate(0deg);width: 300.111px;height: 242.774px;"><img class="rich_pages wxw-img" data-imgfileid="503427048" data-ratio="0.8092592592592592" data-type="png" data-w="1080" height="242.774" style="width: 300.111px;height: 242.774px;" width="300.111" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nplE4aOBzfQP1yIPibghyibUXQibicaOGZwpPhABHmNUQ21xLg8ibfmMPUjw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">向数字世界 AGI 迈进，北京智源人工智能研究院、新加坡南洋理工大学、北京大学携手提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制 General Computer Control (GCC)</strong></span>，即智能体需要像人一样<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>看屏幕，通过键盘、鼠标完成计算机上的所有任务。</strong></span>在过去很长一段时间里，人工智能研究以游戏为场景，而 GCC 将为通用人工智能研究提供场景，也将进一步促进大模型和 AI Agents 的落地与产业化。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为此，研究团队提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体框架 Cradle</strong></span>，使智能体不依赖任何内部 API 直接控制键盘、鼠标和任何软件交互，无论开源还是闭源，甚至能玩《荒野大镖客 2》这样的商业 3A 游戏大作！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427082" data-ratio="0.4546296296296296" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87niaDSpOeuxk906Zw1g5ymo3jCafo6bqBX4SF6Ghxl4cVibGlAmdnOh9VA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2403.03186</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://baai-agents.github.io/Cradle/</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">代码链接：https://github.com/BAAI-Agents/Cradle</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">随着大模型的发展，越来越多的智能体（AI Agents）研究关注计算机控制，包括浏览网页、操作智能手机、玩游戏等。然而，已有研究依赖内部 API 获取输入，并输出预先定义好的动作。要构建能完成计算机上一切任务的<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用智能体</strong></span>，必须使用最通用和最标准的输入输出与计算机进行交互。因此，通用计算机控制使用统一的输入和输出，从而让智能体的通用性变为可能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">但通用性带来了操作上的难度：（1）使用计算机屏幕作为输入对智能体的视频理解能力提出了更高的要求，例如由于没有内部 API，需要通过视觉信息判断动作是否执行成功；（2）使用键盘和鼠标操作作为输出使得智能体需要更高的时空操作精度，比如键盘按键和鼠标点击通常额外涉及时间维度。如何解决这些难题是构建<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体 (GCC Agents) </strong></span>的挑战！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">Cradle：操控一切软件</span></strong></p><p style="text-align: center;line-height: 1.75em;"><span style="font-family: 等线;font-size: 12pt;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427074" data-ratio="0.32599118942731276" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nKiaWLObjvy0phjEalIrEcDaHL0ibM4cPdNFyW63TibicYyox6zia9tt2nDA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「计算机指任何以用户为中心的计算设备，包括 PC、智能手机和平板电脑等。尽管 Cradle 着重于键盘和鼠标操作，但可以很容易扩展到控制手柄和触摸屏等」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通用计算机控制智能体框架 Cradle 主要由 6 个模块组成：信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块。Cradle 高度的通用性来源于其对和计算机交互过程中的原始输入输出的合理封装和抽象。以从屏幕中显示的视频作为输入，提取其中的文本和视觉信息进行决策，并且输出底层操作系统中控制键盘和鼠标的信号去和计算机交互，使得其可以不依赖于任何假设与所有软件进行交互。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427056" data-ratio="0.4889867841409692" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nIvWAwI0IFCvRZJCgqtJlpPLycCm0mPvOfdOHqmictf4r6f3BVtOK0fQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「Cradle 主要由信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块等 6 个模块组成，其强大的决策推理来自于 “反思过去，总结现在，规划未来”」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">同时 Cradle 强大的决策推理模块让其得以自发和软件进行交互并且完成任务，这个过程可以被简单地总结为：<span style="color: rgb(61, 170, 214);"><strong>反思过去，总结现在，规划未来</strong></span>。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">反思过去</span></strong></span><span style="font-size: 15px;">：使用执行过往动作过程的视频作为输入，分别提取出其中关键的文本和视觉信息，通过反思来判断上一步动作是否执行成功、任务是否完成以及如何改进。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">总结现在</span></strong></span><span style="font-size: 15px;">：反思完之后，总结当前情况，并且以此为依据来决定是否更换任务目标或是修改任务内容。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">规划未来</span></strong></span><span style="font-size: 15px;">：最后根据当前任务和现状生成或者更新技能，并且从已学会的技能中检索与当前任务相关的技能作为备选，然后从中选取合适的技能实例化为动作去执行。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在决策推理的同时，Cradle 会周期性地总结和维护储存在情境记忆中的历史信息以及储存在长期记忆中的技能。这一过程的大脑是多模态大模型，如 GPT-4V，但是 Cradle 为其添加了总结、反思以及记忆等功能，形成了完整的面向通用计算机控制的智能体框架，有效解决了通用性所带来的难题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>Cradle：带你从头开始探索《荒野大镖客 2》</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了证明框架的通用性和强大的决策能力，研究团队选择将 Cradle 部署到最为困难以及鲜有人探索的的商业 3A 游戏大作《荒野大镖客 2》。他们认为作为操作最为困难的软件，假如 Cradle 能够在 3A 游戏上自由探索甚至完成主线剧情，那么说明该框架有巨大潜力泛化到其他游戏和软件上。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427057" data-ratio="0.5418502202643172" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n7vyhvoWFaQFBpOjZBmhbJEZ8Z9f5PpQ48icfO0HUfRG1YQvZ7iakf4qA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「与 Minecraft 这样的开源游戏不同，大多数商业游戏特别是 3A 游戏并不提供内部 API 接口，使得类似 Voyager 这样的依赖内部 API 获取输入并输出预定义动作的框架无法迁移到其他游戏中」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以 GPT-4V 为基础，Cradle 能直接根据游戏内的提示和教程生成对应的可执行代码作为技能，一步步丰富自己的技能库， 并在之后的游戏中重复使用这些技能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427058" data-ratio="0.2753303964757709" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nNW7n247IjrvyEWsM1ZTxYRARsDoGDV1T99aF6GhYfRNDUmW17LuVmA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在执行了错误动作之后，Cradle 能够有效地通过反思来发现并且纠正错误。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427059" data-ratio="1.4933920704845816" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87njcTXfOkrjF8nyeibUMVlKjUnVDLicT1X2cWtoIHrLeD1Z01iaXCiblagNw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Cradle 不仅能从头开始跟随游戏指引生成相应技能，完成长达 40 分钟时的主线剧情，还能在开放世界自由探索，骑马，打猎，战斗，与 NPC 对话，使用道具，操作地图，甚至商店购物，均不在话下。这是首个能长时间游玩商业 3A 游戏的智能体。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427075" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nePKbttiaiaXwBfkQxeNMfE4UeZv0BBcSqiar1UiccAstTF8EdkLribib6nRA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427076" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nb8cJkMTbVXx2NGjPbQ3TGYj0OlOceHDnUOKVibm27foeO00HyPAtaHw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427077" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nicFnjjatKpYgC0A9riakWm7LYZyzyeuCpU0cCTLbH48GhAKmJiafOsUUQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427078" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n5K8uySbPlRXEfPdnKCuhynVAGSS4CaQg3lnShHQdvUzysua3bicNMQg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;">结束语</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">开源的 Cradle 代码可以很容易扩展到其他软件和游戏。研究团队表示，为了能够实现真正的通用计算机控制，后续 Cradle 还将移植到更多软件和游戏上，也鼓励相关研究团队 / 工业界开展进一步研究与探索。目标是让智能体可以与无论是开源还是闭源的所有软件进行交互并持续自我提升，实现通用性，最终成为通用人工智能诞生的摇篮。</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">"GCC is a cradle for AGI."&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;—The Cradle team</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>One more thing：Cradle 技术解读直播</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">3 月 14 日 14:30-15:30，论文一作新加坡南洋理工大学博士生谭伟豪进行线上解读报告。点击「阅读原文」报名或扫描下图二维码报名。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427062" data-ratio="1.777533039647577" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nLGo0G61d60wCybpNGx9OgCOnaiaf9BA6ib17HagP6iaUOHy0Y91IolzxQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: justify;margin-bottom: 0px;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p><a href="https://event.baai.ac.cn/activities/766">阅读原文</a>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[被误解的「中文版Sora」背后，字节跳动有哪些技术？]]></title>
        <id>2650910630_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910630&amp;idx=1&amp;sn=6ff041f26968b98f62f3eb4fe6b80be5&amp;chksm=84e46bd8b393e2ced5de69d15fc1761261ecd41dae13d930805d3a068ed76eaef09c39ce9bb0#rd"/>
        <updated>2024-03-12T04:10:57.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><strong>作者：蛋酱</strong></span></p></div></div></div></div></div><p style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><br style="outline: 0px;visibility: visible;"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">2024 开年，OpenAI 就在生成式 AI 领域扔下了重磅炸弹：Sora。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这几年，视频生成领域的技术迭代持续加速，很多科技公司也公布了相关技术进展和落地成果。在此之前，Pika、Runway 都曾推出过类似产品，但 Sora 放出的 Demo，显然以一己之力抬高了视频生成领域的标准。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今后的这场竞争中，哪家公司将率先打造出超越 Sora 的产品，仍是未知数。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">国内这边，目光聚集于一众科技大厂。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此前有消息称，字节跳动在 Sora 发布之前就研发出了一款名为 Boximator 的视频生成模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Boximator 提供了一种能够精确控制视频中物体的生成方法。用户无需编写复杂的文本提示，可以直接在参考图像中通过在物体周围画方框来选择目标，然后添加一些方框和线条来定义目标的结束位置或跨帧的整个运动路径，如下图所示：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426699" data-ratio="1" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5Wsqs8iax8wdJu52OpJUjqVwMbk0W1e4IfYa1dX3Mc0RlYU44d7Umt3g/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对此，字节跳动保持了低调的态度：相关人士回复媒体，Boximator 是视频生成领域控制对象运动的技术方法研究项目。目前还无法作为完善的产品落地，距离国外领先的视频生成模型在画面质量、保真率、视频时长等方面还有很大差距。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">在对应的技术论文介绍（<span style="color: rgb(123, 12, 0);">https://arxiv.org/abs/2402.01566</span>）中，我们也能看到，Boximator 是以插件的形式运行，可与现有的视频生成模型无缝集成，在保持视频质量的同时，增加运动控制功能。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">视频生成背后的技术涉及多个细分方向，与图像 / 视频理解、图像生成、超分辨率等技术都有关系。深挖之后，我们发现在众多分支领域，字节跳动已公开发表了一些研究成果。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇文章将介绍来自字节跳动智能创作团队的 9 项研究，涉及文生图、文生视频、图生视频、视频理解等多项最新成果。我们不妨从这些研究中，追踪探索视觉生成类模型的技术进展。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">关于视频生成，</span></strong><strong style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;"><span style="font-size: 16px;">字节有哪些成果？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今年 1 月上旬，字节跳动就发布过一个视频生成模型 MagicVideo-V2，一度引发社区热议。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426700" data-ratio="0.27685185185185185" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5jVLt4KVbYcn4yLMKMibbIG23vaK3PxnZic0iad0zUcw1a4sLEjlRn6OtQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2401.04468</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://magicvideov2.github.io/</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicVideo-V2 的创新在于将文本到图像模型、视频运动生成器、参考图像嵌入模块和帧插值模块集成到端到端视频生成 pipeline 中。得益于这一架构设计，MagicVideo-V2 在「审美」上能够保持着稳定的高水平表现，不仅生成美观的高分辨率视频，还兼具比较好的保真度和流畅度。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具体而言，研究者首先使用 T2I 模块创建一个 1024×1024 的图像，封装所描述的场景。随后，I2V 模块对该静态图像进行动画处理，生成 600×600×32 的帧序列，之前的潜在噪声确保了初始帧的连续性。V2V 模块将这些帧增强到 1048×1048 分辨率，同时完善视频内容。最后，插值模块将序列扩展到 94 个帧，得到 1048×1048 分辨率的视频，所生成视频具有较高的美学质量和时间平滑性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426701" data-ratio="0.6027777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5TKEabqn35fUibpP2NzdLBDeP56Rg3dwmHP4ovJAgM8nH5SmNeqcIdlA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者进行的大规模用户评估证明：MagicVideo-V2 比一些知名的 T2V 方法更受青睐（绿色、灰色和粉色条分别代表 MagicVideo-V2 被评为较好、相当或较差）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426705" data-ratio="0.46296296296296297" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG50MAyQGIKBjJOcj1PbV5wyr0ia0GNEkXfbeBFRnVRDeicl8BpaA1YblsQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426706" data-ratio="0.6712962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ozBavx1n1r9eds5h7V9qqe4MmPsqwaecuqGjUYlEgohcxwImGSQkjg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">高质量视频生成背后</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">统一视觉和语言学习的研究范式</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 MagicVideo-V2 的论文中，我们可以看出，视频生成技术的进展，离不开文生图、图生视频等 AIGC 技术的铺路。而生成高审美水准内容的基础在于理解，特别是模型对于视觉和语言两种模态学习、融合能力的进步。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">近年来，大语言模型的可扩展性和通用能力，催生出了统一视觉和语言学习的研究范式。为了跨越「视觉」和「语言」两种模态之间的天然鸿沟，研究者们将预训练好的大语言模型和视觉模型的表征连接起来，提取跨模态特性，完成如视觉问题解答、图像字幕、视觉知识推理和对话等任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这些方向上，字节跳动也有相关探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如，针对开放世界视觉任务中的多目标推理分割挑战，字节跳动联合北京交通大学、北京科技大学的研究者提出了高效像素级推理大模型 PixelLM，并将其开源。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426707" data-ratio="0.2657407407407407" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5zvbnueM6QY05fmzJUaN4EBg96RWHVfPxiapsfV0ymqDYe6MiaiaX2Qjcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：PixelLM:Pixel Reasoning with Large Multimodal Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.02228.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://pixellm.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 能够熟练地处理具有任意数量的开放集目标和不同推理复杂性的任务，下图展示了 PixelLM 在各种分割任务中生成高质量目标掩码的能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426708" data-ratio="0.5972222222222222" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5SVv18alibpVRaB37xtuU0d322YIWVurnm45rAEA46AQnNWgZ3eu8RQg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 的核心是一个新颖的像素解码器和一个分割 codebook：codebook 包含了可学习的 token，这些 token 编码了与不同视觉尺度目标参考相关的上下文和知识，像素解码器根据 codebook token 的隐藏嵌入和图像特征生成目标掩码。在保持 LMM 基本结构的同时，PixelLM 可以在没有额外的、昂贵的视觉分割模型的情况下生成高质量的掩码，从而提高了效率和向不同应用程序的可迁移性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426709" data-ratio="0.42962962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OTmDhz7tAQRlMfCriaTz7Y9w9aWsG2FVL6JURGSs23eicjRloibmncicmg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">值得关注的是，研究者构建了一个全面的多目标推理分割数据集 MUSE。他们从 LVIS 数据集中选取了共 910k 个高质量实例分割掩码以及基于图像内容的详细文本描述，利用这些构建了 246k 个问题 - 答案对。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">相比于图像，如果涉及视频内容，模型遭遇的挑战难度就又增加了不少。因为视频不仅包含丰富多变的视觉信息，还涉及时间序列的动态变化。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">现有的多模态大模型在处理视频内容时，通常将视频帧转化为一系列的视觉 token，并与语言 token 结合以生成文本。但随着生成文本长度的增加，视频内容的影响会逐渐减弱，导致生成的文本越来越多地偏离原视频内容，产生所谓的「幻觉」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">面对这一问题，字节跳动联合浙江大学提出了专门针对视频内容的复杂性设计的多模态大模型 Vista-LLaMA。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426710" data-ratio="0.1824074074074074" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5VtWmwwAIctKEUzZ8Fwx85m8HZtu9ic2jUyGE3NESujxhyOLiaq1bPVGA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Vista-LLaMA:Reliable Video Narrator via Equal Distance to Visual Tokens</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.08870.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://jinxxian.github.io/Vista-LLaMA/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Vista-LLaMA 采用了一种改良的注意力机制 —— 视觉等距离 token 注意力（EDVT），在处理视觉与文本 token 时去除了传统的相对位置编码，同时保留了文本与文本之间的相对位置编码。这种方法大幅提高了语言模型对视频内容的理解深度和准确性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">特别是，Vista-LLaMA 引入的序列化视觉投影器为视频中的时间序列分析问题提供了新的视角，它通过线性投影层编码视觉 token 的时间上下文，增强了模型对视频动态变化的理解能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426711" data-ratio="0.5481481481481482" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OaDTMNu8Hfvha9f7GTsQrFHNCSRIbtWaP7wBJuiaH0mdZVObf6KwYcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在最近被 ICLR 2024 接收的一项研究中，字节跳动的研究者还探讨了一种提升模型对视频内容学习能力的预训练方法。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于视频 - 文本训练语料的规模和质量有限，大多数视觉语言基础模型都采用图像 - 文本数据集进行预训练，并主要关注视觉语义表征建模，而忽略了时间语义表征和相关性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决这个问题，他们提出了 COSA，一种串联样本预训练视觉语言基础模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426712" data-ratio="0.43796296296296294" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ib5ahY8Tqe16OTfhSsTdJVic6k02NcrSvKGFWRwpPCdZOC0YcYaPguPA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：COSA: Concatenated Sample Pretrained Vision-Language Foundation Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2306.09085.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://github.com/TXH-mercury/COSA</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">COSA 仅使用图像 - 文本语料库对视觉内容和事件级时间线索进行联合建模。研究者将多个图像 - 文本对按顺序串联起来，作为预训练的输入。这种转换能有效地将现有的图像 - 文本语料库转换成伪长格式视频 - 段落语料库，从而实现更丰富的场景转换和明确的事件 - 描述对应关系。实验证明，COSA 能够持续提高各种下游任务的性能，包括长 / 短视频 - 文本任务和图像 - 文本任务（如检索、字幕和问题解答）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426713" data-ratio="0.6305555555555555" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG50G7Orb0e6gHgaqQmjx5CEDKcibb3F4P6R1T274DPNpuGh8NERlWJpHA/640?wx_fmt=png&amp;from=appmsg"></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426714" data-ratio="0.7157407407407408" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5pUc1p7JqV5peicYoxBLiaFahpDggPPIeDjVfPW85Kgw3DXkMobmjb5ibg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">从图像到视频</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">被重新认识的「扩散模型」</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在视觉 - 语言模型之外，扩散模型同样是大部分视频生成模型采用的技术。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">通过在大量图像 - 文本配对数据集上进行严格训练，扩散模型能够完全根据文本信息生成细节丰富的图像。除了图片生成，扩散模型还可用于音频生成、时间序列生成、3D 点云生成等等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如在一些短视频应用中，用户只需要提供一张图片，就能生成一段以假乱真的动作视频。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">数百年来保持神秘微笑的蒙娜丽莎，都能马上跑起来：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426715" data-ratio="0.340129749768304" data-s="300,640" data-type="gif" data-w="1079" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5YGBUu20483zxUbDuOVDVXefpHGFFdeHMuw00gWB9Nn57BKcJM03tGg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这项有趣应用背后的技术，是新加坡国立大学和字节跳动的研究者联合推出的「MagicAnimate」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 是一个基于扩散的人类图像动画框架，在根据特定的运动序列生成视频的任务中，能够很好地保证整个动画的时间一致性并提升动画保真度。而且，MagicAnimate 项目是开源的。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426716" data-ratio="0.30462962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57T0nyfyXZYeACstbcLqmWlsmOCX2kKK07h9ZDuoeeLjagEGvUogdEQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicAnimate:Temporally Consistent Human Image Animation using Diffusion Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2311.16498.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://showlab.github.io/magicanimate/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决生成动画普遍存在的「闪烁」问题，研究者通过将时间注意力（temporal attention）块合并到扩散主干网络中，来构建用于时间建模的视频扩散模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 将整个视频分解为重叠的片段，并简单地对重叠帧的预测进行平均。最后，研究者还引入图像 - 视频联合训练策略，以进一步增强参考图像保留能力和单帧保真度。虽然仅接受了真实人类数据的训练，MagicAnimate 却展现出了泛化到各种应用场景的能力，包括对未见过的领域数据进行动画处理、与文本 - 图像扩散模型的集成以及多人动画等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426717" data-ratio="0.4287037037037037" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5HEYz6EbqbpkEMIjm5BRhaciaRZcKqXRX1KoDFXp0N9GubpE2lS0l7NA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">另一项基于扩散模型思想的研究「DREAM-Talk」，则解决了从单张肖像图像生成会说话的情绪化人脸的任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426718" data-ratio="0.2361111111111111" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5EicnXYGAw14p2LJDc8WHs4kQiaVmGOGic6CCT7BYwlEWYKKRS6y0CJ65w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：DREAM-Talk:Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.13578.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://dreamtalkemo.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们知道，在这项任务中，很难同时实现富有表现力的情感对话和准确的唇语同步，通常为了保证唇语同步的准确性，表现力往往会大打折扣。&nbsp;</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">「DREAM-Talk」是一个基于扩散的音频驱动框架，分为两个阶段：首先，研究者提出了一个新颖的扩散模块 EmoDiff，可根据音频和参考情绪风格生成多种高度动态的情绪表情和头部姿势。鉴于唇部动作与音频之间的强相关性，研究者随后利用音频特征和情感风格对动态进行了改进，从而提高了唇部同步的准确性，此外还部署了一个视频到视频渲染模块，实现了将表情和唇部动作转移到任意肖像。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从效果上看，DREAM-Talk 在表现力、唇部同步准确性和感知质量方面的确不错：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426719" data-ratio="0.6333333333333333" data-s="300,640" data-type="png" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ia69w7QzS5gQQic1hgVIJicdzaF6GHqxC148zxibGmd4dHicoDbexcBSTQQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但不管是图像生成还是视频生成，当前基于扩散模型路线的研究都还有一些基础挑战需要解决。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如很多人关心生成内容的质量问题（对应 SAG、DREAM-Talk），这可能与扩散模型的生成过程中的一些步骤有关，比如引导采样。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">扩散模型中的引导采样大致可分为两类：需要训练的和无需训练的。免训练引导采样是利用现成的预训练网络（如美学评估模型）来引导生成过程，旨在以更少的步骤和更高的精度从预训练的模型中获取知识。当前的训练无指导采样算法基于对干净图像的一步估计来获得指导能量函数。然而，由于预训练网络是针对干净图像进行训练的，因此干净图像的一步估计过程可能不准确，尤其是在扩散模型的早期阶段，导致早期时间步骤的指导不准确。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">针对该问题，字节跳动和新加坡国立大学的研究者共同提出了 Symplectic Adjoint Guidance (SAG)。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426720" data-ratio="0.3592592592592593" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5yW6kEBibOQsCBJicryLHkM8ibBCqtXehF5vWibJWbImHehsvScFOYqYyuA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.12030.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">SAG 通过两个内阶段计算梯度引导：首先，SAG 通过 n 个函数调用估计干净图像，其中 n 作为一个灵活的参数，可以根据特定的图像质量要求进行调整。其次，SAG 使用对称偶方法精确高效地获得关于内存需求的梯度。这种方法可支持各种图像和视频生成任务，包括风格引导图像生成、美学改进和视频风格化，并有效提升了生成内容的质量。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最近入选 ICLR 2024 的一篇论文，则着重讨论了「扩散概率模型梯度反向传播的临界灵敏度方法」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426721" data-ratio="0.6240740740740741" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5QhkTtVM6kiaiazFO7FjBp9yJib36RQaQxIVuhunRNM1aElcoib6dnXIx6w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文标题：Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2307.10711.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于扩散概率模型的采样过程涉及对去噪 U-Net 的递归调用，因此 naïve 梯度反向传播需要存储所有迭代的中间状态，从而导致极高的内存消耗。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这篇论文中，研究者提出的 AdjointDPM 首先通过求解相应的概率流 ODE 从扩散模型中生成新样本。然后，通过求解另一个增强的 ODE，使用邻接灵敏度方法反向传播模型参数（包括调节信号、网络权重和初始噪声）损失的梯度。为了减少前向生成和梯度反向传播过程中的数值误差，研究者使用指数积分进一步将概率流 ODE 和增强型 ODE 重新参数化为简单的非刚性 ODE。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者指出，AdjointDPM 在三个任务中极具价值：将视觉效果转换为识别文本嵌入、针对特定类型的风格化对扩散概率模型进行微调，以及优化初始噪声以生成用于安全审计的对抗样本，以减少优化工作中的成本。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对于视觉类的感知任务，采用文本到图像的扩散模型作为特征提取器的方法也受到越来越多的关注。在这一方向上，字节跳动的研究者在论文中提出了一种简单而有效的方案。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426722" data-ratio="0.25277777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5gucl3XWVs3zIPcFf14Wkx9rgdtOJFcfNMwk0NyFyLCSAbnb5UfsUsw/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题；Harnessing Diffusion Models for Visual Perception with Meta Prompts</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.14733.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇论文的核心创新是在预训练的扩散模型中引入可学习的嵌入（元提示）以提取感知特征，不依赖额外的多模态模型来生成图像标题，也不使用数据集中的类别标签。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">元提示有两方面的作用：首先，作为 T2I 模型中文本嵌入的直接替代物，它可以在特征提取过程中激活与任务相关的特征；其次，它将用于重新排列提取的特征，以确保模型专注于与手头任务最相关的特征。此外，研究者还设计了一种循环细化训练策略，充分利用扩散模型的特性，从而获得更强的视觉特征。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">「中文版 Sora」诞生之前</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">还有多远的路要走？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这几篇新论文中，我们已经了解到字节跳动这样的国内科技公司，在视频生成技术上的一系列积极的探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但是与 Sora 相比，无论是字节跳动，还是 AI 视频生成领域的一众明星公司，都存在肉眼可见的差距。Sora 的优势建立在对 Scaling Law 的信仰和突破性的技术创新上：通过 patchs 统一视频数据，依托 Diffusion Transformer 等技术架构和 DALL・E 3 的语义理解能力，真正做到了「遥遥领先」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 2022 年文生图的大爆发，到 2024 年 Sora 的横空出世，人工智能领域的技术迭代速度，已经超过了大家的想象。2024 年，相信这一领域还会出现更多的「爆款」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">字节显然也在加紧投入技术研发。近期，谷歌 VideoPoet 项目负责人蒋路，开源多模态大模型 LLaVA 团队成员之一、前微软研究院首席研究员 Chunyuan Li 均被曝出已加入字节跳动智能创作团队。该团队还在大力招聘，官网上已放出多个大模型算法相关岗位。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不仅仅是字节，BAT 等老牌巨头也放出众多令人瞩目的视频生成研究成果，一众大模型创业公司更是极具冲劲。文生视频技术又将出现哪些新的突破？我们拭目以待。</span></p><p><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[前端不存在了？盲测64%的人更喜欢GPT-4V的设计，杨笛一等团队新作]]></title>
        <id>2650910318_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910318&amp;idx=1&amp;sn=79915af15817ce7b9c5ccbf45d6e662f&amp;chksm=84e46a10b393e3060f6ef83e9dab5fdcc9c1750115c4f79929af8a14275c35f43944bbdd7081#rd"/>
        <updated>2024-03-11T04:10:56.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：Panda</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="14" data-source-title=""><div class="js_blockquote_digest"><p>前端工程师是不是开始慌了？</p></div></blockquote><p><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">3 月 9 日央视的一档节目上，百度创始人、董事长兼 CEO 李彦宏指出，以后不会存在「程序员」这种职业了，因为只要会说话，人人都会具备程序员的能力。「未来的编程语言只会剩下两种，一种叫做英文，一种叫做中文。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426623" data-ratio="0.562962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5l0KeweYovnibpvA2k0NwknH9vQfLt9xhPmWChCicU6aFqiarjZQQdwqBA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">自大模型技术突破以来，越来越多的行业拥有了自动化的趋势，这其中进度最快的领域似乎是软件开发本身。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">根据你的自然语言指令，ChatGPT 这样的工具可以和你边聊边生成代码，结果逐渐靠谱且速度很快。在最近多模态技术进步以后，甚至截个图让 AI 自行领会意图也能生成你想要的设计：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426624" data-ratio="0.6972222222222222" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5fKM8WnLFbWAFBtzkvpBH0Rn4ptLl1R8xI7MD9UjibnhlZcSKlcIIKTA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这种方法是装装样子还是来真的？AI 距离「替代程序员」还有多远？有研究告诉我们：已经很可怕了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;"><span style="display: none;line-height: 0px;">‍</span>我们离自动化前端工程还有多远？</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">将视觉设计实现成执行功能的代码是一项颇具挑战性的任务，因为这需要理解视觉元素和它们的布局，然后将它们翻译成结构化的代码。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这个过程需要复杂的技能，也因此让很多普通人无法构建自己的网络应用，即便他们已经有了非常具体的构建或设计思路。不仅如此，由于这个过程需要不同领域的专业知识，因此往往需要具备不同技能的人互相合作，这就会让整个网页构建过程更加复杂，甚至可能导致目标设计与实际实现之间出现偏差。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">如果能基于视觉设计有效地自动生成功能性代码，那么势必有望实现前端网页应用开发的大众化，也就是让非专家人士也能轻松快捷地构建应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近些年，基于自然语言的代码生成领域发展迅速，但少有人研究基于用户界面（UI）设计来自动生成代码实现，原因包括用户界面存在多样化的视觉和文本信号、结果代码的搜索空间巨大等。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最近，多模态 LLM 进入了新的发展时代，大规模预训练模型可以针对多种基于视觉的任务通过处理视觉和文本输入来生成文本输出，其中代表性的模型包括 Flamingo、GPT-4V 和 Gemini。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这样的进展为上述任务带来了全新的解决方案范式：取一张用户网站设计的截图并将其提供给系统，就能得到完整的代码实现，然后这些代码又可以被渲染成用户想要的网页。整个过程是完全端到端式的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近日，斯坦福大学、佐治亚理工学院等机构的一个联合团队评估了当前的多模态模型在这一任务上的表现。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426625" data-ratio="0.44814814814814813" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5GCKeI78rQWZ4PrFxWfNMehP6aicTKEG8zy2q0lcPZKBHUPxQyP9L6ibw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Design2Code: How Far Are We From Automating Front-End Engineering?</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.03163.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://salt-nlp.github.io/Design2Code/</span><span style="font-size: 15px;"></span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们将这个任务称为 Design2Code。通过一系列的基准评测，我们可以从这些结果中了解自动化前端工程已经发展到哪一步了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了实现系统化和严格的基准评测，该团队为 Design2Code 任务构建了首个真实世界基准。表 1 给出了一些示例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426626" data-ratio="0.7787037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5FRUOWyrwI02A21t4o6QjKo8TSjklNnTNMtIEKMnWFSwO1I3w9MCSsg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了最好地反映真实用例，他们使用了真实世界的网页，而非用生成方法得到合成网页。他们收集了 C4 验证集中的网页，并对所有样本进行了仔细的人工调整，最终得到了 484 个高质量、高难度和多样化的网页。它们可代表不同复杂度的多种真实世界用例。他们执行了定性和定量分析，证明这个基准数据集覆盖了广泛的 HTML 标签用法、领域和复杂度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此外，为了促进高效的评估和模型开发，该团队还为这个任务开发了一些评估指标 —— 可自动比较生成网页的截图与给定的截图输入。这些新指标考虑的维度很全面，包括边界框匹配、文本内容、位置和所有已匹配视觉元素的颜色。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">然后，该团队调查了 GPT-4V 和 Gemini 等当前的多模态 LLM 在这一任务上的表现。为了让这些模型能展现出自己的最优能力，该团队使用了一些不同的 prompt 设计方案，包括文本增强式 prompt 设计和自我修正式 prompt 设计。其中文本增强式 prompt 设计是为视觉输入提供文本元素作为补充，从而可以降低光学字符识别（OCR）的任务负载；自我修正式 prompt 设计则是让模型比较之前的生成结果与输入的网页截图，让其自我改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者发现，在 GPT-4V 和 Gemini Pro 上，相比于使用直接 prompt 设计法，文本增强式 prompt 设计都能带来提升，但自我修正式方法只能为 GPT-4V 带来积极影响。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">尽管这些商用模型的表现是当前最佳的，但它们都是缺乏透明度的黑箱。因此，该团队还为这一任务贡献了一个开源的 18B 参数的已微调模型：Design2Code-18B。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">具体来说，该模型基于当前最佳的开源模型 CogAgent 构建，并使用合成的 Design2Code 数据进行了微调。令人惊讶的是，在新提出的基准上，尽管合成的训练数据与真实的测试数据之间存在差异，但这个「小型」开源模型的表现依然颇具竞争力 —— 足以媲美 Gemini Pro Vision。这说明专用型的「小型」开放模型是有发展潜力的，并且模型也可以从合成数据中学习获取技能。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">Design2Code</span></strong><strong><span style="font-size: 16px;"> 基准</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到基准数据，该团队首先收集了 C4 验证集中的所有网站链接。然后他们将所有 CSS 代码嵌入到了 HTML 文件中，从而让每个网页都只有一个代码实现文件。这样得到了共计 12.79 万个网页。然后他们又执行了进一步的过滤和处理，包括自动调整和人工调节。最终他们得到了包含 484 个测试样本的基准。下表 1 比较了新提出的 Design2Code 与 Huggingface 的 WebSight 数据集。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426627" data-ratio="0.37777777777777777" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5kBvb3KhwVFAxTS1OWk1qyQGNqtg9GBxpfHv9Yoe0lGicboTyU1S77kw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">图 2 总结了 Design2Code 的主要主题。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426628" data-ratio="1.0699815837937385" data-type="png" data-w="543" style="width: 308px;height: 330px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57Xdjz1S1pPSakictPMPBH2ohf9m6SaqXqSeY5bKSGqnrfOf6D18sMEA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">至于评估指标，该团队提出了一种高层级的视觉相似度指标，即比较参考网页和生成网页的相似度。另外他们还使用了一组低层级的元素匹配指标，包括块元素、位置、文本和颜色等的匹配程度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">结果自动评估和人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">表 2 和图 3 给出了自动评估的结果。请注意，这里的比较并不是公平的，因为不同模型有不同的模型大小和训练数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426633" data-ratio="0.5768518518518518" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5CW5Hbsj3w9gnL7GzM32pXxCWrFocYb3GMwkENctibxPGPm7Hk4tic9Pw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426634" data-ratio="0.9929577464788732" data-type="png" data-w="568" style="width: 278px;height: 276px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5N8x1qQ9HEdiaEylxrp8ialFw9WVK9KO7BDuOyUAYDMEUN59QS4IibLMJQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">可以观察到：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 在颜色之外的所有维度上都表现最好，而在颜色维度上领先的是 WebSight VLM-8B。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于 GPT-4V 和 Gemini Pro Vision，文本增强式 prompt 设计均可以成功提升块元素匹配分数和文本相似度分数，这说明提供提取出的文本元素是有用的。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对 GPT-4V 而言，自我修正式 prompt 设计可以为块元素匹配和位置相似度带来少量提升，但对 Gemini Pro Vision 来说却并无提升。可能的原因是：在没有外部反馈的前提下，LLM 执行内部自我校正的能力有限。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">通过比较 Design2Code-18B 和基础版本的 CogAgent-18B，可以看出微调能为所有维度带来显著提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">相比于 WebSight VLM-8B，该团队微调得到的 Design2Code-18B 在块元素匹配和文本相似度指标上表现更好，但在位置相似度和颜色相似度指标上表现更差。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队表示，前两个观察可以归因于更强更大的基础模型，而后两个则可归功于更大量的微调数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也进行了人类评估。下面是主要的评估协议和结果。每一个问题都由 5 位人类标注者给出评估意见，最终结果遵从多数意见。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">成对模型比较：也就是让标注者给一对生成的网页排名（一个来自基线方法，另一个来自受测方法），以决定哪一个与参考网页更相似。这里的基线是对 Gemini Pro Vision 采用直接 prompt 设计，收集的数据是其它七种方法与这种基线方法的胜 / 平 / 负的比例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426635" data-ratio="0.587037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG55FQge4TFcozW0iac3wbRvoMuG7DuLRwcNnSwFRtDR9lKdibC2ibXcUyOA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果见图 4，可以看出：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 显著优于其它基线，而且文本增强式 prompt 设计和自我修正式 prompt 设计能在直接 prompt 设计的基础上进一步提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文本增强式 prompt 设计可以少量提升 Gemini，但进一步增加自我修正方法却没有帮助。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WebSight VLM-8B 优于 Gemini 直接 prompt 设计方法（54% 的胜率和 35% 的败率），这说明在大量数据上进行微调可以在特定领域比肩商用模型。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">新模型 Design2Code-18B 的表现与 Gemini Pro Vision 直接 prompt 设计方法相当（38% 的胜率和 37% 的败率）。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">直接评估：尽管有这些比较，但读者可能还是会问：「我们离自动化前端工程还有多远？」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到一个更直观的答案，该团队进一步让人类标注者比较了参考网页与最佳的 AI 生成网页（使用了 GPT-4V 自我修正式 prompt 设计）。他们从两个方面进行了直接评估：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1.AI 生成的网页能否替代原始网页？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">人类标注者认为：AI 生成的网页中，49% 可与参考网页互换。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2. 参考网页和 AI 生成的网页哪个更好？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果有点出人意料：在 64% 的案例中，人类标注者更偏爱 GPT-4V 生成的网页，也就是说他们认为 AI 生成的网页比原始参考图像的设计更好！</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估 vs 人类评估</span></strong><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也研究了自动指标与人类配对偏好之间的相关性。结果发现，人类通常更关注高层级的视觉效果和布局，而不是细节内容，这说明人类的思考方式是自上而下的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，针对论文给出的结果，有人提出了不同意见，认为前端的工作流程远比表面看上去复杂，因此真正实现「自动化前端工程」还需要一段时间。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426636" data-ratio="0.6222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5IicWnRN4I4GopXnYKapwLPzKiaZpZpzZibLiaa67a7WXQ7Cf503MuaTJzg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426637" data-ratio="0.25462962962962965" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ibSYgvvIrlFsOUdBhwGIticKYia4DrUviaOQIbyIKQgQPG7gzxXvtzpH0w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于这个问题，你怎么看？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426638" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;color: rgba(0, 0, 0, 0.9);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
</feed>