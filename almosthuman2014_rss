<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>almosthuman2014</id>
    <title>机器之心</title>
    <updated>2024-03-14T08:00:18.586Z</updated>
    <generator>awesome</generator>
    <author>
        <name>机器之心</name>
    </author>
    <subtitle>专业的人工智能媒体和产业服务平台</subtitle>
    <logo>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</logo>
    <icon>http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Hw3m9nYrAsOLx3ZicPxogLrGibnMYybTBN7EGzEhCVulznVbDob2ib3mwdMMQXtOhO6bqCdSz9kX7w/0?wx_fmt=png</icon>
    <entry>
        <title type="html"><![CDATA[能说会看会行动，OpenAI机器人，一出手就是王炸]]></title>
        <id>2650910924_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910924&amp;idx=1&amp;sn=b1094de7dd507c8d885575408952bb63&amp;chksm=84e474b2b393fda4dbe888719f90fafbf0649c7ae191f76406d8f362febfbd729176bd286232#rd"/>
        <updated>2024-03-14T03:46:40.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之能报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);outline: 0px;font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="30" data-source-title=""><div class="js_blockquote_digest"><p>网友：波士顿动力要整点新舞步，才能让Figure 01下热搜。</p></div></blockquote><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">「借助 OpenAI 的能力，Figure 01 现在可以与人全面对话了！」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">本周三，半个硅谷都在投的明星机器人创业公司 Figure，发布了全新 OpenAI 大模型加持的机器人 demo。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="649" data-backw="578" data-imgfileid="100034333" data-ratio="1.1229166666666666" data-type="png" data-w="960" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOzoRxTUeNHD6iaY4pWd4aruxSQJKX5RBDCZ4vzXn2Z0PIDDvUm2tyC9g/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">这家公司在 3 月 1 日刚刚宣布获得 OpenAI 等公司的投资，才十几天就直接用上了 OpenAI 的多模态大模型。<br><br>如你所见，得到 OpenAI 大模型能力加持的 Figure 01 现在是这个样子的。</span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034352" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOHkD8sgKuI5XJoGt01MMTibD0icibYHRFu305K25kmHkdyBG23gFNVtZ0Q/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">它可以为听从人类的命令，递给人类苹果。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034353" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2x7ykukibqvLa5R1O043AnchoWmicO9842NvMGiaylGsiaN0wUibQnibBpSA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将黑色塑料袋收拾进框子里。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><span style="display: none;line-height: 0px;">‍</span><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034354" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOajOA1ZcURHMySSlY7icIRb5XDV7VOFLjjtibiaLMQrIGv8ZicNmdm755TA/640?wx_fmt=gif&amp;from=appmsg"><span style="display: none;line-height: 0px;">‍</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将杯子和盘子归置放在沥水架上。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="324" data-backw="578" data-imgfileid="100034355" data-ratio="0.5602941176470588" data-s="300,640" data-type="gif" data-w="680" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJFAUS6ge8wBNvBKicyoVegmKic88ZDW40hgr4VkxW6hicfyiaBgTA3ckuw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">需要强调的是：你看到的这一切，只用到了一个神经网络。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">完整的demo视频如下所示：<br></span></p><p class="channels_iframe_wrp wxw_wechannel_card_not_horizontal" style="margin-bottom: 24px;"><mp-common-videosnap class="js_uneditable custom_select_card channels_iframe videosnap_video_iframe" data-pluginname="mpvideosnap" data-url="https://findermp.video.qq.com/251/20304/stodownload?encfilekey=rjD5jyTuFrIpZ2ibE8T7YmwgiahniaXswqz4ibeYJVb2zMwibArvFLicdykibYRYMec9AGuE82B3jgVc7fPUherD5f11W6MI2KD5mDRe1fOYT7jYC0ib2FcevbgAag&amp;bizid=1023&amp;dotrans=0&amp;hy=SH&amp;idx=1&amp;m=&amp;scene=0&amp;token=x5Y29zUxcibChDEOZcZ62Tb4Faiar0GeqykGIu5HNrdaPkFB5AF74W0KaXnb5kSHLU3JcTtgbpKwE" data-headimgurl="http://wx.qlogo.cn/finderhead/PiajxSqBRaEKs6XzYjCzlSsfrOck5ZdKLHuqicYEiaI62Ty9EQxZmibuCQ/0" data-username="v2_060000231003b20faec8c7e5811fcbd2cc05eb34b077bf43ae33648ee0ea039da9063a594944@finder" data-nickname="机器之心机动组" data-desc="OpenAI机器人来了：给ChatGPT造个身体，能说会看会行动。机器人创业公司 Figure AI，发布了升级版机器人demo。得益于OpenAI的能力，Figure 01 现在可以与人全面对话，行动自如，条理清晰。机器人的进化越来越快了。
#OpenAI#FigureAI#机器人#具身智能#AI#人工智能#科技#前沿科技
" data-nonceid="11498428280630186292" data-type="video" data-mediatype="undefined" data-authiconurl="https://dldir1v6.qq.com/weixin/checkresupdate/icons_filled_channels_authentication_enterprise_a2658032368245639e666fb11533a600.png" data-from="new" data-width="1080" data-height="1920" data-id="export/UzFfAgtgekIEAQAAAAAACSkm_Xn48AAAAAstQy6ubaLX4KHWvLEZgBPE44I4Vhx9SeuEzNPgMIvc-PknJ5zpsQiMnOso10FC" data-isdisabled="0" data-errortips=""></mp-common-videosnap></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">广大网友在看到如此惊艳的 demo 后，对机器人的发展速度感到震惊，我们似乎正处在这场汹涌的进化浪潮中。甚至有人感叹，已经准备好迎接更多的机器人了。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="75" data-backw="578" data-imgfileid="100034338" data-ratio="0.13037974683544304" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOniaU3nXVUicrr4FQ7wxCLSwt37ybd3sHQ7qBerBwpBxVvKExicLqqqWiaw/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="90" data-backw="578" data-imgfileid="100034342" data-ratio="0.15569620253164557" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOGr8kjcWW4ViaKvhiaR1Y1LQUcjRCZvQOicibl0aibehjh89MLterKyna0xg/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="78" data-backw="578" data-imgfileid="100034339" data-ratio="0.1341772151898734" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOBP05bibn0lXOU6P8KAz15BOLdwRHXpDiaFR89Y1UQ6pqQm5wNXaQOicfg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">还有网友调侃道：「波士顿动力：好的，伙计们，这是一场真正的竞争。让我们回到实验室，设计更多舞蹈套路。」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="70" data-backw="578" data-imgfileid="100034340" data-ratio="0.12080536912751678" data-type="png" data-w="894" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOcpTp06Qz8NqYTD6IRyufvkjZn7zT6jleCM6L8H4KLwI2symlWP6SHg/640?wx_fmt=png&amp;from=appmsg"></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">所有这些，全是机器人自学的！</span></strong><span style="font-size: 15px;"></span></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure创始人Brett Adcock表示，视频中Figure 01展示了端到端神经网络框架下与人类的对话，没有任何远程操作。并且，机器人的速度有了显著的提升，开始接近人类的速度。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="300" data-backw="578" data-imgfileid="100034341" data-ratio="0.5189873417721519" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2oRgOAzEbmh1mFGKdmRCPmeyITMrU2rP503ywGOTkCqicuiaOtVp8EEg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure机器人操作高级AI工程师Corey Lynch介绍了此次Figure 01的技术原理。他表示，Figure 01现在可以做到以下这些：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其视觉体验</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">规划未来的行动</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">反思自己的记忆</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">口头解释推理过程</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="292" data-backw="578" data-imgfileid="100034343" data-ratio="0.5050632911392405" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOiaJ7537xTLEOxJBdljFK8dhWD9xyAJh6PcaymjlCVlDLV4zvdJT62xA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>他接着解释道，视频中机器人的所有行为都是学到的（再次强调不是远程操作），并以正常速度（1.0x）运行。<br><br>在具体实现过程中，他们将机器人摄像头中的图像输入，并将机载麦克风捕获的语音文本转录到由 OpenAI训练的大型多模态模型中，该模型可以理解图像和文本。该模型对整个对话记录进行处理，包括过去的图像，从而获得语言响应，然后通过文本到语音的方式将其回复给人类。<br><br>此外，该模型负责决定在机器人上运行哪些学习到的闭环行为以完成给定的命令，从而将特定的神经网络权重加载到GPU上并执行策略。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="325" data-backw="578" data-imgfileid="100034347" data-ratio="0.562037037037037" data-type="png" data-w="1080" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJSduLlQ3Tq4RZCUAzd7oqdZytLSguriceWHLuRyvKOYhX4UK31sicdOA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>将Figure 01 连接到大型预训练多模态模型为其提供了一些有趣的新功能。Figure 01 + OpenAI 现在可以：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其周围环境。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">使用常识推理做出决定。例如，「桌子上的盘子和杯子等餐具接下来可能需要放进沥水架」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将「我饿了」等模棱两可的高级请求转化为一些适合上下文的行为，例如「递给对方一个苹果」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">用简单的英语描述为什么它执行特定的操作。例如，「这是我可以从桌子上为您提供的唯一可食用物品」。</span></p></li></ul><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="320" data-backw="578" data-imgfileid="100034358" data-ratio="0.553125" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOC0qGfofubbiciaBN98AqeRdKUicmBqtXNLfDto9vN0YF3LiazHKx0HKaJw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">理解对话历史的大型预训练模型为Figure 01提供了强大的短期记忆。<br><br>考虑一个简单的问题：「你能把它们放在那里吗？」<br><br>其中 「它们」指的是什么？「那里」又是哪里？正确回答这个问题需要反思记忆的能力。<br><br>通过预训练模型分析对话的图像和文本历史记录，Figure 01快速形成并执行计划：1）将杯子放在沥水架上，2）将盘子放在沥水架上。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100034359" data-ratio="0.5485294117647059" data-s="300,640" data-type="gif" data-w="680" style="width: 578px;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOFjWbfhQ0ayhXgpEzCep3MqvHOuicxOoVvvibULBXtDRnAcLoJPay2ReQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">关于学到的低级双手操作，所有行为均由神经网络视觉运动transformer策略驱动，将像素直接映射到动作。这些网络以10hz 的频率接收机载图像，并以200hz的频率生成 24-DOF 动作（手腕姿势和手指关节角度）。<br><br>这些动作充当高速「设定点」，以供更高速率的全身控制器跟踪。这是一个有用的关注点分离，其中：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">互联网预训练模型对图像和文本进行常识推理，以得出高级规划。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">学习到的视觉运动策略执行计划，执行难以手动指定的快速反应行为，例如在任何位置操纵可变形的袋子。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">全身控制器确保安全、稳定的动力，例如保持平衡。</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最后他表示，即使在几年前，自己还认为人形机器人规划和执行自身完全学得行为的同时与人类进行完整的对话是几十年后才能看到的事情。显然，现在已经发生了太多变化。</span></p><p><img class="rich_pages wxw-img" data-backh="246" data-backw="578" data-imgfileid="100034344" data-ratio="0.4253164556962025" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOWFybJb0PvbKM01y3Cx6cukMBksYabr4Rp77s691pa4bDNF6WBtTjJw/640?wx_fmt=png&amp;from=appmsg"><br></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">至于声音方面，大家都在猜机器人金属感十足的声音源自谁？有猜乔布斯的、Sam Altman的，也有猜演员 Rob Lowe 的，你认为呢？</span></h3><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427199" data-ratio="0.28703703703703703" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWib5TrenoS2xDfp7tFqzcmibpn1icDIkzvxdqnpib88gDMiavEnCibeH9cf47vcIlGD31giaCibfZf2wgNHIQ/640?wx_fmt=png&amp;from=appmsg"></p><p><strong><span style="font-size: 15px;"></span></strong></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">Figure，具身智能时代最热创业公司</span></strong></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最近，生成式 AI 的竞争正在走向长文本、多模态，各家科技公司和机构也没有忘记投资下个热点——具身智能。<br><br>具身智能，对于计算机视觉、机器人等领域来说是一个很有挑战的目标：假设 AI 智能体（机器人）不仅能接收来自数据集的静态图像，还能在三维虚拟世界甚至真实环境中四处移动，并与周围环境交互，那我们就会迎来技术的一次重大突破，从识别图像等机器学习的简单能力，转变到学习如何通过多个步骤执行复杂的类人任务。<br><br><a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzUyODA3MDUwMA==&amp;mid=2247517811&amp;idx=1&amp;sn=2293011e21dec9ba484fea7e1167883a&amp;chksm=fa772478cd00ad6e4e81d102158615f05a105f0a4465a89116ac8f56f039b4399b818c6c825b&amp;scene=21#wechat_redirect" textvalue="被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。</a><br><br>3月1日，Figure 宣布完成惊人的 6.75 亿美元 B 轮融资，公司估值达到 26 亿美元。一眼望去，感觉半个硅谷都投了它：微软、英特尔、OpenAI Startup Fund、Amazon Industrial Innovation Fund 、英伟达、贝索斯、「木头姐」的方舟投资、Parkway Venture Capital、Align Ventures 等。<br><br>该公司的产品 Figure 01，据称是世界上第一个具有商业可行性的自主人形机器人，身高 1.5 米，体重 60 公斤，可承载 20 公斤货物，采用电机驱动。它的可工作时长是 5 小时，行走速度每秒 1.2 米，可以说很多指标已经接近人类。<br><br>自 2023 年 1 月以来，人们对 Figure 的关注度一直在上升。虽然到目前为止，公司一共才发布过四个 demo 视频。其中的一个展示了 Figure 01 是如何制作咖啡的：<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="299" data-backw="400" data-imgfileid="100034360" data-ratio="0.7475" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO7WwYaEqtkIgPePCz7EBpPlXoKUDuYqxtOjYwqmaVaKI46V6QyX0ibbg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">据Figure表示，机器人练习这些动作的方法是端到端的，神经网络的训练时间是10小时。<br><br>在 2 月 27 日的视频里，Figure 01 自主完成了一个典型的物流环节任务——搬运空箱。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="236" data-backw="400" data-imgfileid="100034361" data-ratio="0.59" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO0iblsjb3ibxWAyZic6exC96GdrfvWib23N1glvz6mOBVvux6hWEuI4L5jw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">当然，速度还是比人类慢了很多。不过在这些任务中，Figure 01 都是完全自主地执行任务。所谓「完全自主」，是指只需将机器人放在地面上（无论放在屋里什么地方），在没有其他用户输入的情况下，直接按开始就行。<br><br>在训练过的大型视觉语言模型( VLM )帮助下，人形机器人会先识别、定位目标箱子，然后推理合适的拿放姿势。接下来，Figure 01 会导航自己到目标跟前，检测抓取点和手部力量，尝试抓取成功并将箱子放到传送带上。<br><br>这些技术亮点也是 Figure 和一直希望回归机器人领域的 OpenAI 达成合作协议的重要原因之一——将 OpenAI 的研究与 Figure 的机器人经验结合起来，为人形机器人开发下一代 AI 模型。OpenAI 也希望将自己的高性能多模态大模型扩展到机器人领域。<br><br>除了接受大笔风投之外，Figure 也在积极拓展落地场景。目前，Figure 01 已经开始在宝马位于南卡罗来纳州斯帕坦堡的汽车工厂接受测试，人们计划让机器人替代人类从事一些危险度高的任务。<br><br></span><span style="font-size: 12px;color: rgb(136, 136, 136);"><em>参考链接：<br>https://twitter.com/i/status/1767913661253984474<br>https://www.figure.ai/</em></span><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="480" data-backw="578" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="480" data-imgfileid="100034328" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: auto;width: 100%;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sJmFS59bKcLo002wiazYfxKSDOX9w6AXE4xF7eJ34ibNKbeLjVFg88WNNIhZiaexbXz8iaTyK45MCkHOQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p class="mp_profile_iframe_wrp" style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="2" data-is_biz_ban="0"></mp-common-profile></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="margin-bottom: 24px;"><br></p><p style="display: none;margin-bottom: 24px;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注生成AI用例的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI机器人，一出手就是王炸]]></title>
        <id>2650910844_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910844&amp;idx=1&amp;sn=232554de8c9b7dec1e7147e6be7861ac&amp;chksm=84e47402b393fd141d688acc561a009aa2fafa47e6a4d11e7604de3fe3c137e935f6745a0cce#rd"/>
        <updated>2024-03-13T17:26:12.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之能报道</span></p><p style="outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);outline: 0px;font-size: 12px;visibility: visible;"><strong style="outline: 0px;visibility: visible;">作者：机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="30" data-source-title=""><div class="js_blockquote_digest"><p>网友：波士顿动力要整点新舞步，才能让Figure 01下热搜。</p></div></blockquote><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">「借助 OpenAI 的能力，Figure 01 现在可以与人全面对话了！」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">本周三，半个硅谷都在投的明星机器人创业公司 Figure，发布了全新 OpenAI 大模型加持的机器人 demo。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="649" data-backw="578" data-imgfileid="100034333" data-ratio="1.1229166666666666" data-type="png" data-w="960" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOzoRxTUeNHD6iaY4pWd4aruxSQJKX5RBDCZ4vzXn2Z0PIDDvUm2tyC9g/640?wx_fmt=png&amp;from=appmsg"><br></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">这家公司在 3 月 1 日刚刚宣布获得 OpenAI 等公司的投资，才十几天就直接用上了 OpenAI 的多模态大模型。<br><br>如你所见，得到 OpenAI 大模型能力加持的 Figure 01 现在是这个样子的。</span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034352" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOHkD8sgKuI5XJoGt01MMTibD0icibYHRFu305K25kmHkdyBG23gFNVtZ0Q/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">它可以为听从人类的命令，递给人类苹果。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034353" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2x7ykukibqvLa5R1O043AnchoWmicO9842NvMGiaylGsiaN0wUibQnibBpSA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将黑色塑料袋收拾进框子里。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><span style="display: none;line-height: 0px;">‍</span><img class="rich_pages wxw-img js_insertlocalimg" data-backh="317" data-backw="578" data-imgfileid="100034354" data-ratio="0.5484375" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOajOA1ZcURHMySSlY7icIRb5XDV7VOFLjjtibiaLMQrIGv8ZicNmdm755TA/640?wx_fmt=gif&amp;from=appmsg"><span style="display: none;line-height: 0px;">‍</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将杯子和盘子归置放在沥水架上。<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="324" data-backw="578" data-imgfileid="100034355" data-ratio="0.5602941176470588" data-s="300,640" data-type="gif" data-w="680" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJFAUS6ge8wBNvBKicyoVegmKic88ZDW40hgr4VkxW6hicfyiaBgTA3ckuw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">需要强调的是：你看到的这一切，只用到了一个神经网络。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">完整的demo视频如下所示：<br></span></p><p class="channels_iframe_wrp wxw_wechannel_card_not_horizontal" style="margin-bottom: 24px;"><mp-common-videosnap class="js_uneditable custom_select_card channels_iframe videosnap_video_iframe" data-pluginname="mpvideosnap" data-url="https://findermp.video.qq.com/251/20304/stodownload?encfilekey=rjD5jyTuFrIpZ2ibE8T7YmwgiahniaXswqz4ibeYJVb2zMwibArvFLicdykibYRYMec9AGuE82B3jgVc7fPUherD5f11W6MI2KD5mDRe1fOYT7jYC0ib2FcevbgAag&amp;bizid=1023&amp;dotrans=0&amp;hy=SH&amp;idx=1&amp;m=&amp;scene=0&amp;token=AxricY7RBHdVRjxoxDtf5SfxGib71c6kv6S0eSdxQtu1T0m6BibUFlevxjYI5qHGomWappHTQsXgoE" data-headimgurl="http://wx.qlogo.cn/finderhead/PiajxSqBRaEKs6XzYjCzlSsfrOck5ZdKLHuqicYEiaI62Ty9EQxZmibuCQ/0" data-username="v2_060000231003b20faec8c7e5811fcbd2cc05eb34b077bf43ae33648ee0ea039da9063a594944@finder" data-nickname="机器之心机动组" data-desc="OpenAI机器人来了：给ChatGPT造个身体，能说会看会行动。机器人创业公司 Figure AI，发布了升级版机器人demo。得益于OpenAI的能力，Figure 01 现在可以与人全面对话，行动自如，条理清晰。机器人的进化越来越快了。
#OpenAI#FigureAI#机器人#具身智能#AI#人工智能#科技#前沿科技
" data-nonceid="11498428280630186292" data-type="video" data-mediatype="undefined" data-authiconurl="https://dldir1v6.qq.com/weixin/checkresupdate/icons_filled_channels_authentication_enterprise_a2658032368245639e666fb11533a600.png" data-from="new" data-width="1080" data-height="1920" data-id="export/UzFfAgtgekIEAQAAAAAACSkm_Xn48AAAAAstQy6ubaLX4KHWvLEZgBPE44I4Vhx9SeuEzNPgMIvc-PknJ5zpsQiMnOso10FC" data-isdisabled="0" data-errortips=""></mp-common-videosnap></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">广大网友在看到如此惊艳的 demo 后，对机器人的发展速度感到震惊，我们似乎正处在这场汹涌的进化浪潮中。甚至有人感叹，已经准备好迎接更多的机器人了。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="75" data-backw="578" data-imgfileid="100034338" data-ratio="0.13037974683544304" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOniaU3nXVUicrr4FQ7wxCLSwt37ybd3sHQ7qBerBwpBxVvKExicLqqqWiaw/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="90" data-backw="578" data-imgfileid="100034342" data-ratio="0.15569620253164557" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOGr8kjcWW4ViaKvhiaR1Y1LQUcjRCZvQOicibl0aibehjh89MLterKyna0xg/640?wx_fmt=png&amp;from=appmsg"><img class="rich_pages wxw-img" data-backh="78" data-backw="578" data-imgfileid="100034339" data-ratio="0.1341772151898734" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOBP05bibn0lXOU6P8KAz15BOLdwRHXpDiaFR89Y1UQ6pqQm5wNXaQOicfg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">还有网友调侃道：「波士顿动力：好的，伙计们，这是一场真正的竞争。让我们回到实验室，设计更多舞蹈套路。」</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="70" data-backw="578" data-imgfileid="100034340" data-ratio="0.12080536912751678" data-type="png" data-w="894" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOcpTp06Qz8NqYTD6IRyufvkjZn7zT6jleCM6L8H4KLwI2symlWP6SHg/640?wx_fmt=png&amp;from=appmsg"></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">所有这些，全是机器人自学的！</span></strong><span style="font-size: 15px;"></span></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure创始人Brett Adcock表示，视频中Figure 01展示了端到端神经网络框架下与人类的对话，没有任何远程操作。并且，机器人的速度有了显著的提升，开始接近人类的速度。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="300" data-backw="578" data-imgfileid="100034341" data-ratio="0.5189873417721519" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllO2oRgOAzEbmh1mFGKdmRCPmeyITMrU2rP503ywGOTkCqicuiaOtVp8EEg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">Figure机器人操作高级AI工程师Corey Lynch介绍了此次Figure 01的技术原理。他表示，Figure 01现在可以做到以下这些：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其视觉体验</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">规划未来的行动</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">反思自己的记忆</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">口头解释推理过程</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="292" data-backw="578" data-imgfileid="100034343" data-ratio="0.5050632911392405" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOiaJ7537xTLEOxJBdljFK8dhWD9xyAJh6PcaymjlCVlDLV4zvdJT62xA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>他接着解释道，视频中机器人的所有行为都是学到的（再次强调不是远程操作），并以正常速度（1.0x）运行。<br><br>在具体实现过程中，他们将机器人摄像头中的图像输入，并将机载麦克风捕获的语音文本转录到由 OpenAI训练的大型多模态模型中，该模型可以理解图像和文本。该模型对整个对话记录进行处理，包括过去的图像，从而获得语言响应，然后通过文本到语音的方式将其回复给人类。<br><br>此外，该模型负责决定在机器人上运行哪些学习到的闭环行为以完成给定的命令，从而将特定的神经网络权重加载到GPU上并执行策略。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="325" data-backw="578" data-imgfileid="100034347" data-ratio="0.562037037037037" data-type="png" data-w="1080" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOJSduLlQ3Tq4RZCUAzd7oqdZytLSguriceWHLuRyvKOYhX4UK31sicdOA/640?wx_fmt=png&amp;from=appmsg"><span style="font-size: 15px;"><br><br>将Figure 01 连接到大型预训练多模态模型为其提供了一些有趣的新功能。Figure 01 + OpenAI 现在可以：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">描述其周围环境。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">使用常识推理做出决定。例如，「桌子上的盘子和杯子等餐具接下来可能需要放进沥水架」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">将「我饿了」等模棱两可的高级请求转化为一些适合上下文的行为，例如「递给对方一个苹果」。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">用简单的英语描述为什么它执行特定的操作。例如，「这是我可以从桌子上为您提供的唯一可食用物品」。</span></p></li></ul><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="320" data-backw="578" data-imgfileid="100034358" data-ratio="0.553125" data-s="300,640" data-type="gif" data-w="640" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOC0qGfofubbiciaBN98AqeRdKUicmBqtXNLfDto9vN0YF3LiazHKx0HKaJw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">理解对话历史的大型预训练模型为Figure 01提供了强大的短期记忆<br><br>考虑一个简单的问题：「你能把它们放在那里吗？」<br><br>其中 「它们」指的是什么？「那里」又是哪里？正确回答这个问题需要反思记忆的能力。<br><br>通过预训练模型分析对话的图像和文本历史记录，Figure 01快速形成并执行计划：1）将杯子放在沥水架上，2）将盘子放在沥水架上。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100034359" data-ratio="0.5485294117647059" data-s="300,640" data-type="gif" data-w="680" style="width: 578px;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllOFjWbfhQ0ayhXgpEzCep3MqvHOuicxOoVvvibULBXtDRnAcLoJPay2ReQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">关于学到的低级双手操作，所有行为均由神经网络视觉运动transformer策略驱动，将像素直接映射到动作。这些网络以10hz 的频率接收机载图像，并以200hz的频率生成 24-DOF 动作（手腕姿势和手指关节角度）。<br><br>这些动作充当高速「设定点」，以供更高速率的全身控制器跟踪。这是一个有用的关注点分离，其中：<br></span></p><ul class="list-paddingleft-1"><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">互联网预训练模型对图像和文本进行常识推理，以得出高级规划。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">学习到的视觉运动策略执行计划，执行难以手动指定的快速反应行为，例如在任何位置操纵可变形的袋子。</span></p></li><li style="font-size: 15px;"><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">全身控制器确保安全、稳定的动力，例如保持平衡。</span></p></li></ul><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最后他表示，即使在几年前，自己还认为人形机器人规划和执行自身完全学得行为的同时与人类进行完整的对话是几十年后才能看到的事情。显然，现在已经发生了太多变化。</span></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><img class="rich_pages wxw-img" data-backh="246" data-backw="578" data-imgfileid="100034344" data-ratio="0.4253164556962025" data-type="png" data-w="790" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sKumV3L9z2vGyQ30FSYUllOWFybJb0PvbKM01y3Cx6cukMBksYabr4Rp77s691pa4bDNF6WBtTjJw/640?wx_fmt=png&amp;from=appmsg"><br></p><h3 style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 15px;">Figure，具身智能时代最热创业公司</span></strong></h3><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">最近，生成式 AI 的竞争正在走向长文本、多模态，各家科技公司和机构也没有忘记投资下个热点——具身智能。<br><br>具身智能，对于计算机视觉、机器人等领域来说是一个很有挑战的目标：假设 AI 智能体（机器人）不仅能接收来自数据集的静态图像，还能在三维虚拟世界甚至真实环境中四处移动，并与周围环境交互，那我们就会迎来技术的一次重大突破，从识别图像等机器学习的简单能力，转变到学习如何通过多个步骤执行复杂的类人任务。<br><br><a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzUyODA3MDUwMA==&amp;mid=2247517811&amp;idx=1&amp;sn=2293011e21dec9ba484fea7e1167883a&amp;chksm=fa772478cd00ad6e4e81d102158615f05a105f0a4465a89116ac8f56f039b4399b818c6c825b&amp;scene=21#wechat_redirect" textvalue="被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">被生成式 AI 龙头 OpenAI 看好的具身智能，最有希望通向具身智能的公司，似乎就是这家 Figure。</a><br><br>3月1日，Figure 宣布完成惊人的 6.75 亿美元 B 轮融资，公司估值达到 26 亿美元。一眼望去，感觉半个硅谷都投了它：微软、英特尔、OpenAI Startup Fund、Amazon Industrial Innovation Fund 、英伟达、贝索斯、「木头姐」的方舟投资、Parkway Venture Capital、Align Ventures 等。<br><br>该公司的产品 Figure 01，据称是世界上第一个具有商业可行性的自主人形机器人，身高 1.5 米，体重 60 公斤，可承载 20 公斤货物，采用电机驱动。它的可工作时长是 5 小时，行走速度每秒 1.2 米，可以说很多指标已经接近人类。<br><br>自 2023 年 1 月以来，人们对 Figure 的关注度一直在上升。虽然到目前为止，公司一共才发布过四个 demo 视频。其中的一个展示了 Figure 01 是如何制作咖啡的：<br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="299" data-backw="400" data-imgfileid="100034360" data-ratio="0.7475" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO7WwYaEqtkIgPePCz7EBpPlXoKUDuYqxtOjYwqmaVaKI46V6QyX0ibbg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">据Figure表示，机器人练习这些动作的方法是端到端的，神经网络的训练时间是10小时。<br><br>在 2 月 27 日的视频里，Figure 01 自主完成了一个典型的物流环节任务——搬运空箱。<span style="display: none;line-height: 0px;">‍</span><br></span></p><p style="text-align: center;margin-bottom: 24px;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="236" data-backw="400" data-imgfileid="100034361" data-ratio="0.59" data-s="300,640" data-type="gif" data-w="400" style="width: 100%;height: auto;" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/DT8udUick9sKumV3L9z2vGyQ30FSYUllO0iblsjb3ibxWAyZic6exC96GdrfvWib23N1glvz6mOBVvux6hWEuI4L5jw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 24px;margin-bottom: 24px;line-height: 1.75em;"><span style="font-size: 15px;">当然，速度还是比人类慢了很多。不过在这些任务中，Figure 01 都是完全自主地执行任务。所谓「完全自主」，是指只需将机器人放在地面上（无论放在屋里什么地方），在没有其他用户输入的情况下，直接按开始就行。<br><br>在训练过的大型视觉语言模型( VLM )帮助下，人形机器人会先识别、定位目标箱子，然后推理合适的拿放姿势。接下来，Figure 01 会导航自己到目标跟前，检测抓取点和手部力量，尝试抓取成功并将箱子放到传送带上。<br><br>这些技术亮点也是 Figure 和一直希望回归机器人领域的 OpenAI 达成合作协议的重要原因之一——将 OpenAI 的研究与 Figure 的机器人经验结合起来，为人形机器人开发下一代 AI 模型。OpenAI 也希望将自己的高性能多模态大模型扩展到机器人领域。<br><br>除了接受大笔风投之外，Figure 也在积极拓展落地场景。目前，Figure 01 已经开始在宝马位于南卡罗来纳州斯帕坦堡的汽车工厂接受测试，人们计划让机器人替代人类从事一些危险度高的任务。<br><br></span><span style="font-size: 12px;color: rgb(136, 136, 136);"><em>参考链接：<br>https://twitter.com/i/status/1767913661253984474<br>https://www.figure.ai/</em></span><span style="font-size: 15px;"></span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-backh="480" data-backw="578" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="480" data-imgfileid="100034328" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: auto;width: 100%;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/DT8udUick9sJmFS59bKcLo002wiazYfxKSDOX9w6AXE4xF7eJ34ibNKbeLjVFg88WNNIhZiaexbXz8iaTyK45MCkHOQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p class="mp_profile_iframe_wrp" style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="2" data-is_biz_ban="0"></mp-common-profile></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;outline: 0px;visibility: visible;"><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="margin-bottom: 24px;"><br></p><p style="display: none;margin-bottom: 24px;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注生成AI用例的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[向数字世界AGI迈进！智能体已经从头开玩「荒野大镖客 2」了]]></title>
        <id>2650910796_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910796&amp;idx=1&amp;sn=405be9dd0b0d200e4fad13a91b2c2a98&amp;chksm=84e47432b393fd24e2a980e131d265b5e9575557335c95250d22ff77d581a7a59e226f91c1aa#rd"/>
        <updated>2024-03-13T04:25:08.000Z</updated>
        <summary type="html"><![CDATA[<p style="text-align: justify;line-height: 1.75em;"><br></p><div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 29.75px;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;"><span style="display: none;line-height: 0px;">‍</span>机器之心发布</span></p><p mp-original-font-size="17" mp-original-line-height="29.75" style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 29.75px;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><p style="text-align: justify;line-height: 1.75em;"><br></p><p><iframe class="video_iframe rich_pages" data-vidtype="2" data-mpvid="wxv_3366566767409479682" data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWic8lfCWeamg5rKlyJibKe87nD6hw0PJH9ejlGIXcAs0D6MYH4eicuiaaic9HRDPKaribe8mibI83vQ74qgg%2F0%3Fwx_fmt%3Djpeg" allowfullscreen="" frameborder="0" data-ratio="1.7777777777777777" data-w="1920" style="border-radius: 4px;" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_3366566767409479682"></iframe></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 12px;color: rgb(136, 136, 136);">Video: Cradle从头开始完成主线任务<strong><br></strong></span></p><p style="text-align: center;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">通用计算机控制</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">信息革命产生了数字世界，数字世界为大模型的诞生提供了数据，也最容易实现通用人工智能（AGI）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="margin: 3pt 0pt;text-align: center;font-family: 等线;font-size: 12pt;line-height: 16px;"><span style="display: inline-block;overflow: hidden;transform: rotate(0deg);width: 300.111px;height: 242.774px;"><img class="rich_pages wxw-img" data-imgfileid="503427048" data-ratio="0.8092592592592592" data-type="png" data-w="1080" height="242.774" style="width: 300.111px;height: 242.774px;" width="300.111" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nplE4aOBzfQP1yIPibghyibUXQibicaOGZwpPhABHmNUQ21xLg8ibfmMPUjw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">向数字世界 AGI 迈进，北京智源人工智能研究院、新加坡南洋理工大学、北京大学携手提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制 General Computer Control (GCC)</strong></span>，即智能体需要像人一样<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>看屏幕，通过键盘、鼠标完成计算机上的所有任务。</strong></span>在过去很长一段时间里，人工智能研究以游戏为场景，而 GCC 将为通用人工智能研究提供场景，也将进一步促进大模型和 AI Agents 的落地与产业化。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为此，研究团队提出<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体框架 Cradle</strong></span>，使智能体不依赖任何内部 API 直接控制键盘、鼠标和任何软件交互，无论开源还是闭源，甚至能玩《荒野大镖客 2》这样的商业 3A 游戏大作！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427082" data-ratio="0.4546296296296296" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87niaDSpOeuxk906Zw1g5ymo3jCafo6bqBX4SF6Ghxl4cVibGlAmdnOh9VA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2403.03186</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://baai-agents.github.io/Cradle/</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">代码链接：https://github.com/BAAI-Agents/Cradle</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">随着大模型的发展，越来越多的智能体（AI Agents）研究关注计算机控制，包括浏览网页、操作智能手机、玩游戏等。然而，已有研究依赖内部 API 获取输入，并输出预先定义好的动作。要构建能完成计算机上一切任务的<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用智能体</strong></span>，必须使用最通用和最标准的输入输出与计算机进行交互。因此，通用计算机控制使用统一的输入和输出，从而让智能体的通用性变为可能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">但通用性带来了操作上的难度：（1）使用计算机屏幕作为输入对智能体的视频理解能力提出了更高的要求，例如由于没有内部 API，需要通过视觉信息判断动作是否执行成功；（2）使用键盘和鼠标操作作为输出使得智能体需要更高的时空操作精度，比如键盘按键和鼠标点击通常额外涉及时间维度。如何解决这些难题是构建<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>通用计算机控制智能体 (GCC Agents) </strong></span>的挑战！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">Cradle：操控一切软件</span></strong></p><p style="text-align: center;line-height: 1.75em;"><span style="font-family: 等线;font-size: 12pt;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427074" data-ratio="0.32599118942731276" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nKiaWLObjvy0phjEalIrEcDaHL0ibM4cPdNFyW63TibicYyox6zia9tt2nDA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「计算机指任何以用户为中心的计算设备，包括 PC、智能手机和平板电脑等。尽管 Cradle 着重于键盘和鼠标操作，但可以很容易扩展到控制手柄和触摸屏等」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通用计算机控制智能体框架 Cradle 主要由 6 个模块组成：信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块。Cradle 高度的通用性来源于其对和计算机交互过程中的原始输入输出的合理封装和抽象。以从屏幕中显示的视频作为输入，提取其中的文本和视觉信息进行决策，并且输出底层操作系统中控制键盘和鼠标的信号去和计算机交互，使得其可以不依赖于任何假设与所有软件进行交互。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427056" data-ratio="0.4889867841409692" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nIvWAwI0IFCvRZJCgqtJlpPLycCm0mPvOfdOHqmictf4r6f3BVtOK0fQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「Cradle 主要由信息收集、自我反思、任务推断、技能管理、行动计划以及记忆模块等 6 个模块组成，其强大的决策推理来自于 “反思过去，总结现在，规划未来”」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">同时 Cradle 强大的决策推理模块让其得以自发和软件进行交互并且完成任务，这个过程可以被简单地总结为：<span style="color: rgb(61, 170, 214);"><strong>反思过去，总结现在，规划未来</strong></span>。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">反思过去</span></strong></span><span style="font-size: 15px;">：使用执行过往动作过程的视频作为输入，分别提取出其中关键的文本和视觉信息，通过反思来判断上一步动作是否执行成功、任务是否完成以及如何改进。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">总结现在</span></strong></span><span style="font-size: 15px;">：反思完之后，总结当前情况，并且以此为依据来决定是否更换任务目标或是修改任务内容。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(61, 170, 214);"><strong><span style="font-size: 15px;">规划未来</span></strong></span><span style="font-size: 15px;">：最后根据当前任务和现状生成或者更新技能，并且从已学会的技能中检索与当前任务相关的技能作为备选，然后从中选取合适的技能实例化为动作去执行。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在决策推理的同时，Cradle 会周期性地总结和维护储存在情境记忆中的历史信息以及储存在长期记忆中的技能。这一过程的大脑是多模态大模型，如 GPT-4V，但是 Cradle 为其添加了总结、反思以及记忆等功能，形成了完整的面向通用计算机控制的智能体框架，有效解决了通用性所带来的难题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>Cradle：带你从头开始探索《荒野大镖客 2》</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了证明框架的通用性和强大的决策能力，研究团队选择将 Cradle 部署到最为困难以及鲜有人探索的的商业 3A 游戏大作《荒野大镖客 2》。他们认为作为操作最为困难的软件，假如 Cradle 能够在 3A 游戏上自由探索甚至完成主线剧情，那么说明该框架有巨大潜力泛化到其他游戏和软件上。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427057" data-ratio="0.5418502202643172" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n7vyhvoWFaQFBpOjZBmhbJEZ8Z9f5PpQ48icfO0HUfRG1YQvZ7iakf4qA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">「与 Minecraft 这样的开源游戏不同，大多数商业游戏特别是 3A 游戏并不提供内部 API 接口，使得类似 Voyager 这样的依赖内部 API 获取输入并输出预定义动作的框架无法迁移到其他游戏中」</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以 GPT-4V 为基础，Cradle 能直接根据游戏内的提示和教程生成对应的可执行代码作为技能，一步步丰富自己的技能库， 并在之后的游戏中重复使用这些技能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427058" data-ratio="0.2753303964757709" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nNW7n247IjrvyEWsM1ZTxYRARsDoGDV1T99aF6GhYfRNDUmW17LuVmA/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在执行了错误动作之后，Cradle 能够有效地通过反思来发现并且纠正错误。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427059" data-ratio="1.4933920704845816" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87njcTXfOkrjF8nyeibUMVlKjUnVDLicT1X2cWtoIHrLeD1Z01iaXCiblagNw/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Cradle 不仅能从头开始跟随游戏指引生成相应技能，完成长达 40 分钟时的主线剧情，还能在开放世界自由探索，骑马，打猎，战斗，与 NPC 对话，使用道具，操作地图，甚至商店购物，均不在话下。这是首个能长时间游玩商业 3A 游戏的智能体。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427075" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nePKbttiaiaXwBfkQxeNMfE4UeZv0BBcSqiar1UiccAstTF8EdkLribib6nRA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427076" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nb8cJkMTbVXx2NGjPbQ3TGYj0OlOceHDnUOKVibm27foeO00HyPAtaHw/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427077" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nicFnjjatKpYgC0A9riakWm7LYZyzyeuCpU0cCTLbH48GhAKmJiafOsUUQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"></span></strong></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503427078" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWic8lfCWeamg5rKlyJibKe87n5K8uySbPlRXEfPdnKCuhynVAGSS4CaQg3lnShHQdvUzysua3bicNMQg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 16px;">结束语</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">开源的 Cradle 代码可以很容易扩展到其他软件和游戏。研究团队表示，为了能够实现真正的通用计算机控制，后续 Cradle 还将移植到更多软件和游戏上，也鼓励相关研究团队 / 工业界开展进一步研究与探索。目标是让智能体可以与无论是开源还是闭源的所有软件进行交互并持续自我提升，实现通用性，最终成为通用人工智能诞生的摇篮。</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">"GCC is a cradle for AGI."&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p><p style="text-align: right;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;—The Cradle team</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>One more thing：Cradle 技术解读直播</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">3 月 14 日 14:30-15:30，论文一作新加坡南洋理工大学博士生谭伟豪进行线上解读报告。点击「阅读原文」报名或扫描下图二维码报名。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503427062" data-ratio="1.777533039647577" data-s="300,640" data-type="png" data-w="908" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic8lfCWeamg5rKlyJibKe87nLGo0G61d60wCybpNGx9OgCOnaiaf9BA6ib17HagP6iaUOHy0Y91IolzxQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: justify;margin-bottom: 0px;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p><a href="https://event.baai.ac.cn/activities/766">阅读原文</a>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[被误解的「中文版Sora」背后，字节跳动有哪些技术？]]></title>
        <id>2650910630_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910630&amp;idx=1&amp;sn=6ff041f26968b98f62f3eb4fe6b80be5&amp;chksm=84e46bd8b393e2ced5de69d15fc1761261ecd41dae13d930805d3a068ed76eaef09c39ce9bb0#rd"/>
        <updated>2024-03-12T04:10:57.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><strong>作者：蛋酱</strong></span></p></div></div></div></div></div><p style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><br style="outline: 0px;visibility: visible;"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">2024 开年，OpenAI 就在生成式 AI 领域扔下了重磅炸弹：Sora。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这几年，视频生成领域的技术迭代持续加速，很多科技公司也公布了相关技术进展和落地成果。在此之前，Pika、Runway 都曾推出过类似产品，但 Sora 放出的 Demo，显然以一己之力抬高了视频生成领域的标准。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今后的这场竞争中，哪家公司将率先打造出超越 Sora 的产品，仍是未知数。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">国内这边，目光聚集于一众科技大厂。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">此前有消息称，字节跳动在 Sora 发布之前就研发出了一款名为 Boximator 的视频生成模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Boximator 提供了一种能够精确控制视频中物体的生成方法。用户无需编写复杂的文本提示，可以直接在参考图像中通过在物体周围画方框来选择目标，然后添加一些方框和线条来定义目标的结束位置或跨帧的整个运动路径，如下图所示：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426699" data-ratio="1" data-s="300,640" data-type="gif" data-w="640" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5Wsqs8iax8wdJu52OpJUjqVwMbk0W1e4IfYa1dX3Mc0RlYU44d7Umt3g/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对此，字节跳动保持了低调的态度：相关人士回复媒体，Boximator 是视频生成领域控制对象运动的技术方法研究项目。目前还无法作为完善的产品落地，距离国外领先的视频生成模型在画面质量、保真率、视频时长等方面还有很大差距。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;">在对应的技术论文介绍（<span style="color: rgb(123, 12, 0);">https://arxiv.org/abs/2402.01566</span>）中，我们也能看到，Boximator 是以插件的形式运行，可与现有的视频生成模型无缝集成，在保持视频质量的同时，增加运动控制功能。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">视频生成背后的技术涉及多个细分方向，与图像 / 视频理解、图像生成、超分辨率等技术都有关系。深挖之后，我们发现在众多分支领域，字节跳动已公开发表了一些研究成果。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇文章将介绍来自字节跳动智能创作团队的 9 项研究，涉及文生图、文生视频、图生视频、视频理解等多项最新成果。我们不妨从这些研究中，追踪探索视觉生成类模型的技术进展。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">关于视频生成，</span></strong><strong style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;"><span style="font-size: 16px;">字节有哪些成果？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在今年 1 月上旬，字节跳动就发布过一个视频生成模型 MagicVideo-V2，一度引发社区热议。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426700" data-ratio="0.27685185185185185" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5jVLt4KVbYcn4yLMKMibbIG23vaK3PxnZic0iad0zUcw1a4sLEjlRn6OtQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/2401.04468</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://magicvideov2.github.io/</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicVideo-V2 的创新在于将文本到图像模型、视频运动生成器、参考图像嵌入模块和帧插值模块集成到端到端视频生成 pipeline 中。得益于这一架构设计，MagicVideo-V2 在「审美」上能够保持着稳定的高水平表现，不仅生成美观的高分辨率视频，还兼具比较好的保真度和流畅度。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">具体而言，研究者首先使用 T2I 模块创建一个 1024×1024 的图像，封装所描述的场景。随后，I2V 模块对该静态图像进行动画处理，生成 600×600×32 的帧序列，之前的潜在噪声确保了初始帧的连续性。V2V 模块将这些帧增强到 1048×1048 分辨率，同时完善视频内容。最后，插值模块将序列扩展到 94 个帧，得到 1048×1048 分辨率的视频，所生成视频具有较高的美学质量和时间平滑性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426701" data-ratio="0.6027777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5TKEabqn35fUibpP2NzdLBDeP56Rg3dwmHP4ovJAgM8nH5SmNeqcIdlA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者进行的大规模用户评估证明：MagicVideo-V2 比一些知名的 T2V 方法更受青睐（绿色、灰色和粉色条分别代表 MagicVideo-V2 被评为较好、相当或较差）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426705" data-ratio="0.46296296296296297" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG50MAyQGIKBjJOcj1PbV5wyr0ia0GNEkXfbeBFRnVRDeicl8BpaA1YblsQ/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426706" data-ratio="0.6712962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ozBavx1n1r9eds5h7V9qqe4MmPsqwaecuqGjUYlEgohcxwImGSQkjg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;"><br></span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">高质量视频生成背后</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">统一视觉和语言学习的研究范式</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 MagicVideo-V2 的论文中，我们可以看出，视频生成技术的进展，离不开文生图、图生视频等 AIGC 技术的铺路。而生成高审美水准内容的基础在于理解，特别是模型对于视觉和语言两种模态学习、融合能力的进步。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">近年来，大语言模型的可扩展性和通用能力，催生出了统一视觉和语言学习的研究范式。为了跨越「视觉」和「语言」两种模态之间的天然鸿沟，研究者们将预训练好的大语言模型和视觉模型的表征连接起来，提取跨模态特性，完成如视觉问题解答、图像字幕、视觉知识推理和对话等任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这些方向上，字节跳动也有相关探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如，针对开放世界视觉任务中的多目标推理分割挑战，字节跳动联合北京交通大学、北京科技大学的研究者提出了高效像素级推理大模型 PixelLM，并将其开源。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426707" data-ratio="0.2657407407407407" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5zvbnueM6QY05fmzJUaN4EBg96RWHVfPxiapsfV0ymqDYe6MiaiaX2Qjcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：PixelLM:Pixel Reasoning with Large Multimodal Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.02228.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://pixellm.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 能够熟练地处理具有任意数量的开放集目标和不同推理复杂性的任务，下图展示了 PixelLM 在各种分割任务中生成高质量目标掩码的能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426708" data-ratio="0.5972222222222222" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5SVv18alibpVRaB37xtuU0d322YIWVurnm45rAEA46AQnNWgZ3eu8RQg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">PixelLM 的核心是一个新颖的像素解码器和一个分割 codebook：codebook 包含了可学习的 token，这些 token 编码了与不同视觉尺度目标参考相关的上下文和知识，像素解码器根据 codebook token 的隐藏嵌入和图像特征生成目标掩码。在保持 LMM 基本结构的同时，PixelLM 可以在没有额外的、昂贵的视觉分割模型的情况下生成高质量的掩码，从而提高了效率和向不同应用程序的可迁移性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426709" data-ratio="0.42962962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OTmDhz7tAQRlMfCriaTz7Y9w9aWsG2FVL6JURGSs23eicjRloibmncicmg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">值得关注的是，研究者构建了一个全面的多目标推理分割数据集 MUSE。他们从 LVIS 数据集中选取了共 910k 个高质量实例分割掩码以及基于图像内容的详细文本描述，利用这些构建了 246k 个问题 - 答案对。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">相比于图像，如果涉及视频内容，模型遭遇的挑战难度就又增加了不少。因为视频不仅包含丰富多变的视觉信息，还涉及时间序列的动态变化。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">现有的多模态大模型在处理视频内容时，通常将视频帧转化为一系列的视觉 token，并与语言 token 结合以生成文本。但随着生成文本长度的增加，视频内容的影响会逐渐减弱，导致生成的文本越来越多地偏离原视频内容，产生所谓的「幻觉」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">面对这一问题，字节跳动联合浙江大学提出了专门针对视频内容的复杂性设计的多模态大模型 Vista-LLaMA。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426710" data-ratio="0.1824074074074074" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5VtWmwwAIctKEUzZ8Fwx85m8HZtu9ic2jUyGE3NESujxhyOLiaq1bPVGA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Vista-LLaMA:Reliable Video Narrator via Equal Distance to Visual Tokens</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.08870.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://jinxxian.github.io/Vista-LLaMA/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">Vista-LLaMA 采用了一种改良的注意力机制 —— 视觉等距离 token 注意力（EDVT），在处理视觉与文本 token 时去除了传统的相对位置编码，同时保留了文本与文本之间的相对位置编码。这种方法大幅提高了语言模型对视频内容的理解深度和准确性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">特别是，Vista-LLaMA 引入的序列化视觉投影器为视频中的时间序列分析问题提供了新的视角，它通过线性投影层编码视觉 token 的时间上下文，增强了模型对视频动态变化的理解能力。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426711" data-ratio="0.5481481481481482" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5OaDTMNu8Hfvha9f7GTsQrFHNCSRIbtWaP7wBJuiaH0mdZVObf6KwYcg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在最近被 ICLR 2024 接收的一项研究中，字节跳动的研究者还探讨了一种提升模型对视频内容学习能力的预训练方法。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于视频 - 文本训练语料的规模和质量有限，大多数视觉语言基础模型都采用图像 - 文本数据集进行预训练，并主要关注视觉语义表征建模，而忽略了时间语义表征和相关性。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决这个问题，他们提出了 COSA，一种串联样本预训练视觉语言基础模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426712" data-ratio="0.43796296296296294" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ib5ahY8Tqe16OTfhSsTdJVic6k02NcrSvKGFWRwpPCdZOC0YcYaPguPA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：COSA: Concatenated Sample Pretrained Vision-Language Foundation Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2306.09085.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://github.com/TXH-mercury/COSA</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">COSA 仅使用图像 - 文本语料库对视觉内容和事件级时间线索进行联合建模。研究者将多个图像 - 文本对按顺序串联起来，作为预训练的输入。这种转换能有效地将现有的图像 - 文本语料库转换成伪长格式视频 - 段落语料库，从而实现更丰富的场景转换和明确的事件 - 描述对应关系。实验证明，COSA 能够持续提高各种下游任务的性能，包括长 / 短视频 - 文本任务和图像 - 文本任务（如检索、字幕和问题解答）。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426713" data-ratio="0.6305555555555555" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG50G7Orb0e6gHgaqQmjx5CEDKcibb3F4P6R1T274DPNpuGh8NERlWJpHA/640?wx_fmt=png&amp;from=appmsg"></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426714" data-ratio="0.7157407407407408" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5pUc1p7JqV5peicYoxBLiaFahpDggPPIeDjVfPW85Kgw3DXkMobmjb5ibg/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">从图像到视频</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">被重新认识的「扩散模型」</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在视觉 - 语言模型之外，扩散模型同样是大部分视频生成模型采用的技术。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">通过在大量图像 - 文本配对数据集上进行严格训练，扩散模型能够完全根据文本信息生成细节丰富的图像。除了图片生成，扩散模型还可用于音频生成、时间序列生成、3D 点云生成等等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如在一些短视频应用中，用户只需要提供一张图片，就能生成一段以假乱真的动作视频。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">数百年来保持神秘微笑的蒙娜丽莎，都能马上跑起来：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426715" data-ratio="0.340129749768304" data-s="300,640" data-type="gif" data-w="1079" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5YGBUu20483zxUbDuOVDVXefpHGFFdeHMuw00gWB9Nn57BKcJM03tGg/640?wx_fmt=gif&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这项有趣应用背后的技术，是新加坡国立大学和字节跳动的研究者联合推出的「MagicAnimate」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 是一个基于扩散的人类图像动画框架，在根据特定的运动序列生成视频的任务中，能够很好地保证整个动画的时间一致性并提升动画保真度。而且，MagicAnimate 项目是开源的。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426716" data-ratio="0.30462962962962964" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57T0nyfyXZYeACstbcLqmWlsmOCX2kKK07h9ZDuoeeLjagEGvUogdEQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：MagicAnimate:Temporally Consistent Human Image Animation using Diffusion Model</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2311.16498.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://showlab.github.io/magicanimate/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">为了解决生成动画普遍存在的「闪烁」问题，研究者通过将时间注意力（temporal attention）块合并到扩散主干网络中，来构建用于时间建模的视频扩散模型。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">MagicAnimate 将整个视频分解为重叠的片段，并简单地对重叠帧的预测进行平均。最后，研究者还引入图像 - 视频联合训练策略，以进一步增强参考图像保留能力和单帧保真度。虽然仅接受了真实人类数据的训练，MagicAnimate 却展现出了泛化到各种应用场景的能力，包括对未见过的领域数据进行动画处理、与文本 - 图像扩散模型的集成以及多人动画等。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426717" data-ratio="0.4287037037037037" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5HEYz6EbqbpkEMIjm5BRhaciaRZcKqXRX1KoDFXp0N9GubpE2lS0l7NA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">另一项基于扩散模型思想的研究「DREAM-Talk」，则解决了从单张肖像图像生成会说话的情绪化人脸的任务。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426718" data-ratio="0.2361111111111111" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5EicnXYGAw14p2LJDc8WHs4kQiaVmGOGic6CCT7BYwlEWYKKRS6y0CJ65w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：DREAM-Talk:Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.13578.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目地址：https://dreamtalkemo.github.io/&nbsp;</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">我们知道，在这项任务中，很难同时实现富有表现力的情感对话和准确的唇语同步，通常为了保证唇语同步的准确性，表现力往往会大打折扣。&nbsp;</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">「DREAM-Talk」是一个基于扩散的音频驱动框架，分为两个阶段：首先，研究者提出了一个新颖的扩散模块 EmoDiff，可根据音频和参考情绪风格生成多种高度动态的情绪表情和头部姿势。鉴于唇部动作与音频之间的强相关性，研究者随后利用音频特征和情感风格对动态进行了改进，从而提高了唇部同步的准确性，此外还部署了一个视频到视频渲染模块，实现了将表情和唇部动作转移到任意肖像。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从效果上看，DREAM-Talk 在表现力、唇部同步准确性和感知质量方面的确不错：</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426719" data-ratio="0.6333333333333333" data-s="300,640" data-type="png" data-w="960" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ia69w7QzS5gQQic1hgVIJicdzaF6GHqxC148zxibGmd4dHicoDbexcBSTQQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但不管是图像生成还是视频生成，当前基于扩散模型路线的研究都还有一些基础挑战需要解决。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">比如很多人关心生成内容的质量问题（对应 SAG、DREAM-Talk），这可能与扩散模型的生成过程中的一些步骤有关，比如引导采样。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">扩散模型中的引导采样大致可分为两类：需要训练的和无需训练的。免训练引导采样是利用现成的预训练网络（如美学评估模型）来引导生成过程，旨在以更少的步骤和更高的精度从预训练的模型中获取知识。当前的训练无指导采样算法基于对干净图像的一步估计来获得指导能量函数。然而，由于预训练网络是针对干净图像进行训练的，因此干净图像的一步估计过程可能不准确，尤其是在扩散模型的早期阶段，导致早期时间步骤的指导不准确。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">针对该问题，字节跳动和新加坡国立大学的研究者共同提出了 Symplectic Adjoint Guidance (SAG)。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426720" data-ratio="0.3592592592592593" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5yW6kEBibOQsCBJicryLHkM8ibBCqtXehF5vWibJWbImHehsvScFOYqYyuA/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.12030.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">SAG 通过两个内阶段计算梯度引导：首先，SAG 通过 n 个函数调用估计干净图像，其中 n 作为一个灵活的参数，可以根据特定的图像质量要求进行调整。其次，SAG 使用对称偶方法精确高效地获得关于内存需求的梯度。这种方法可支持各种图像和视频生成任务，包括风格引导图像生成、美学改进和视频风格化，并有效提升了生成内容的质量。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">最近入选 ICLR 2024 的一篇论文，则着重讨论了「扩散概率模型梯度反向传播的临界灵敏度方法」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426721" data-ratio="0.6240740740740741" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5QhkTtVM6kiaiazFO7FjBp9yJib36RQaQxIVuhunRNM1aElcoib6dnXIx6w/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文标题：Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2307.10711.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">由于扩散概率模型的采样过程涉及对去噪 U-Net 的递归调用，因此 naïve 梯度反向传播需要存储所有迭代的中间状态，从而导致极高的内存消耗。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这篇论文中，研究者提出的 AdjointDPM 首先通过求解相应的概率流 ODE 从扩散模型中生成新样本。然后，通过求解另一个增强的 ODE，使用邻接灵敏度方法反向传播模型参数（包括调节信号、网络权重和初始噪声）损失的梯度。为了减少前向生成和梯度反向传播过程中的数值误差，研究者使用指数积分进一步将概率流 ODE 和增强型 ODE 重新参数化为简单的非刚性 ODE。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">研究者指出，AdjointDPM 在三个任务中极具价值：将视觉效果转换为识别文本嵌入、针对特定类型的风格化对扩散概率模型进行微调，以及优化初始噪声以生成用于安全审计的对抗样本，以减少优化工作中的成本。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">对于视觉类的感知任务，采用文本到图像的扩散模型作为特征提取器的方法也受到越来越多的关注。在这一方向上，字节跳动的研究者在论文中提出了一种简单而有效的方案。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426722" data-ratio="0.25277777777777777" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5gucl3XWVs3zIPcFf14Wkx9rgdtOJFcfNMwk0NyFyLCSAbnb5UfsUsw/640?wx_fmt=png&amp;from=appmsg"></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题；Harnessing Diffusion Models for Visual Perception with Meta Prompts</span></p></li><li style="color: rgb(123, 12, 0);"><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/2312.14733.pdf</span></p></li></ul><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">这篇论文的核心创新是在预训练的扩散模型中引入可学习的嵌入（元提示）以提取感知特征，不依赖额外的多模态模型来生成图像标题，也不使用数据集中的类别标签。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">元提示有两方面的作用：首先，作为 T2I 模型中文本嵌入的直接替代物，它可以在特征提取过程中激活与任务相关的特征；其次，它将用于重新排列提取的特征，以确保模型专注于与手头任务最相关的特征。此外，研究者还设计了一种循环细化训练策略，充分利用扩散模型的特性，从而获得更强的视觉特征。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">「中文版 Sora」诞生之前</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;text-align: center;"><strong><span style="font-size: 16px;">还有多远的路要走？</span></strong></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">在这几篇新论文中，我们已经了解到字节跳动这样的国内科技公司，在视频生成技术上的一系列积极的探索。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">但是与 Sora 相比，无论是字节跳动，还是 AI 视频生成领域的一众明星公司，都存在肉眼可见的差距。Sora 的优势建立在对 Scaling Law 的信仰和突破性的技术创新上：通过 patchs 统一视频数据，依托 Diffusion Transformer 等技术架构和 DALL・E 3 的语义理解能力，真正做到了「遥遥领先」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">从 2022 年文生图的大爆发，到 2024 年 Sora 的横空出世，人工智能领域的技术迭代速度，已经超过了大家的想象。2024 年，相信这一领域还会出现更多的「爆款」。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">字节显然也在加紧投入技术研发。近期，谷歌 VideoPoet 项目负责人蒋路，开源多模态大模型 LLaVA 团队成员之一、前微软研究院首席研究员 Chunyuan Li 均被曝出已加入字节跳动智能创作团队。该团队还在大力招聘，官网上已放出多个大模型算法相关岗位。</span></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><br></p><p style="margin-top: 0px;margin-bottom: 0px;line-height: 1.75em;"><span style="font-size: 15px;">不仅仅是字节，BAT 等老牌巨头也放出众多令人瞩目的视频生成研究成果，一众大模型创业公司更是极具冲劲。文生视频技术又将出现哪些新的突破？我们拭目以待。</span></p><p><br></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;caret-color: rgb(34, 34, 34);text-size-adjust: inherit;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[前端不存在了？盲测64%的人更喜欢GPT-4V的设计，杨笛一等团队新作]]></title>
        <id>2650910318_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910318&amp;idx=1&amp;sn=79915af15817ce7b9c5ccbf45d6e662f&amp;chksm=84e46a10b393e3060f6ef83e9dab5fdcc9c1750115c4f79929af8a14275c35f43944bbdd7081#rd"/>
        <updated>2024-03-11T04:10:56.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：Panda</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="14" data-source-title=""><div class="js_blockquote_digest"><p>前端工程师是不是开始慌了？</p></div></blockquote><p><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">3 月 9 日央视的一档节目上，百度创始人、董事长兼 CEO 李彦宏指出，以后不会存在「程序员」这种职业了，因为只要会说话，人人都会具备程序员的能力。「未来的编程语言只会剩下两种，一种叫做英文，一种叫做中文。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426623" data-ratio="0.562962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5l0KeweYovnibpvA2k0NwknH9vQfLt9xhPmWChCicU6aFqiarjZQQdwqBA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">自大模型技术突破以来，越来越多的行业拥有了自动化的趋势，这其中进度最快的领域似乎是软件开发本身。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">根据你的自然语言指令，ChatGPT 这样的工具可以和你边聊边生成代码，结果逐渐靠谱且速度很快。在最近多模态技术进步以后，甚至截个图让 AI 自行领会意图也能生成你想要的设计：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426624" data-ratio="0.6972222222222222" data-s="300,640" data-type="gif" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW92vANWeGB3nATtuVoxNgG5fKM8WnLFbWAFBtzkvpBH0Rn4ptLl1R8xI7MD9UjibnhlZcSKlcIIKTA/640?wx_fmt=gif&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这种方法是装装样子还是来真的？AI 距离「替代程序员」还有多远？有研究告诉我们：已经很可怕了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;"><span style="display: none;line-height: 0px;">‍</span>我们离自动化前端工程还有多远？</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">将视觉设计实现成执行功能的代码是一项颇具挑战性的任务，因为这需要理解视觉元素和它们的布局，然后将它们翻译成结构化的代码。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这个过程需要复杂的技能，也因此让很多普通人无法构建自己的网络应用，即便他们已经有了非常具体的构建或设计思路。不仅如此，由于这个过程需要不同领域的专业知识，因此往往需要具备不同技能的人互相合作，这就会让整个网页构建过程更加复杂，甚至可能导致目标设计与实际实现之间出现偏差。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">如果能基于视觉设计有效地自动生成功能性代码，那么势必有望实现前端网页应用开发的大众化，也就是让非专家人士也能轻松快捷地构建应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近些年，基于自然语言的代码生成领域发展迅速，但少有人研究基于用户界面（UI）设计来自动生成代码实现，原因包括用户界面存在多样化的视觉和文本信号、结果代码的搜索空间巨大等。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最近，多模态 LLM 进入了新的发展时代，大规模预训练模型可以针对多种基于视觉的任务通过处理视觉和文本输入来生成文本输出，其中代表性的模型包括 Flamingo、GPT-4V 和 Gemini。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这样的进展为上述任务带来了全新的解决方案范式：取一张用户网站设计的截图并将其提供给系统，就能得到完整的代码实现，然后这些代码又可以被渲染成用户想要的网页。整个过程是完全端到端式的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">近日，斯坦福大学、佐治亚理工学院等机构的一个联合团队评估了当前的多模态模型在这一任务上的表现。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426625" data-ratio="0.44814814814814813" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5GCKeI78rQWZ4PrFxWfNMehP6aicTKEG8zy2q0lcPZKBHUPxQyP9L6ibw/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：Design2Code: How Far Are We From Automating Front-End Engineering?</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.03163.pdf</span></p></li><li style="color: rgb(123, 12, 0);"><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目主页：https://salt-nlp.github.io/Design2Code/</span><span style="font-size: 15px;"></span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们将这个任务称为 Design2Code。通过一系列的基准评测，我们可以从这些结果中了解自动化前端工程已经发展到哪一步了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了实现系统化和严格的基准评测，该团队为 Design2Code 任务构建了首个真实世界基准。表 1 给出了一些示例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426626" data-ratio="0.7787037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5FRUOWyrwI02A21t4o6QjKo8TSjklNnTNMtIEKMnWFSwO1I3w9MCSsg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了最好地反映真实用例，他们使用了真实世界的网页，而非用生成方法得到合成网页。他们收集了 C4 验证集中的网页，并对所有样本进行了仔细的人工调整，最终得到了 484 个高质量、高难度和多样化的网页。它们可代表不同复杂度的多种真实世界用例。他们执行了定性和定量分析，证明这个基准数据集覆盖了广泛的 HTML 标签用法、领域和复杂度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此外，为了促进高效的评估和模型开发，该团队还为这个任务开发了一些评估指标 —— 可自动比较生成网页的截图与给定的截图输入。这些新指标考虑的维度很全面，包括边界框匹配、文本内容、位置和所有已匹配视觉元素的颜色。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">然后，该团队调查了 GPT-4V 和 Gemini 等当前的多模态 LLM 在这一任务上的表现。为了让这些模型能展现出自己的最优能力，该团队使用了一些不同的 prompt 设计方案，包括文本增强式 prompt 设计和自我修正式 prompt 设计。其中文本增强式 prompt 设计是为视觉输入提供文本元素作为补充，从而可以降低光学字符识别（OCR）的任务负载；自我修正式 prompt 设计则是让模型比较之前的生成结果与输入的网页截图，让其自我改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者发现，在 GPT-4V 和 Gemini Pro 上，相比于使用直接 prompt 设计法，文本增强式 prompt 设计都能带来提升，但自我修正式方法只能为 GPT-4V 带来积极影响。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">尽管这些商用模型的表现是当前最佳的，但它们都是缺乏透明度的黑箱。因此，该团队还为这一任务贡献了一个开源的 18B 参数的已微调模型：Design2Code-18B。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">具体来说，该模型基于当前最佳的开源模型 CogAgent 构建，并使用合成的 Design2Code 数据进行了微调。令人惊讶的是，在新提出的基准上，尽管合成的训练数据与真实的测试数据之间存在差异，但这个「小型」开源模型的表现依然颇具竞争力 —— 足以媲美 Gemini Pro Vision。这说明专用型的「小型」开放模型是有发展潜力的，并且模型也可以从合成数据中学习获取技能。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">Design2Code</span></strong><strong><span style="font-size: 16px;"> 基准</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到基准数据，该团队首先收集了 C4 验证集中的所有网站链接。然后他们将所有 CSS 代码嵌入到了 HTML 文件中，从而让每个网页都只有一个代码实现文件。这样得到了共计 12.79 万个网页。然后他们又执行了进一步的过滤和处理，包括自动调整和人工调节。最终他们得到了包含 484 个测试样本的基准。下表 1 比较了新提出的 Design2Code 与 Huggingface 的 WebSight 数据集。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426627" data-ratio="0.37777777777777777" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5kBvb3KhwVFAxTS1OWk1qyQGNqtg9GBxpfHv9Yoe0lGicboTyU1S77kw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">图 2 总结了 Design2Code 的主要主题。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426628" data-ratio="1.0699815837937385" data-type="png" data-w="543" style="width: 308px;height: 330px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG57Xdjz1S1pPSakictPMPBH2ohf9m6SaqXqSeY5bKSGqnrfOf6D18sMEA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">至于评估指标，该团队提出了一种高层级的视觉相似度指标，即比较参考网页和生成网页的相似度。另外他们还使用了一组低层级的元素匹配指标，包括块元素、位置、文本和颜色等的匹配程度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">结果自动评估和人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">表 2 和图 3 给出了自动评估的结果。请注意，这里的比较并不是公平的，因为不同模型有不同的模型大小和训练数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426633" data-ratio="0.5768518518518518" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5CW5Hbsj3w9gnL7GzM32pXxCWrFocYb3GMwkENctibxPGPm7Hk4tic9Pw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503426634" data-ratio="0.9929577464788732" data-type="png" data-w="568" style="width: 278px;height: 276px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5N8x1qQ9HEdiaEylxrp8ialFw9WVK9KO7BDuOyUAYDMEUN59QS4IibLMJQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">可以观察到：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 在颜色之外的所有维度上都表现最好，而在颜色维度上领先的是 WebSight VLM-8B。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于 GPT-4V 和 Gemini Pro Vision，文本增强式 prompt 设计均可以成功提升块元素匹配分数和文本相似度分数，这说明提供提取出的文本元素是有用的。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对 GPT-4V 而言，自我修正式 prompt 设计可以为块元素匹配和位置相似度带来少量提升，但对 Gemini Pro Vision 来说却并无提升。可能的原因是：在没有外部反馈的前提下，LLM 执行内部自我校正的能力有限。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">通过比较 Design2Code-18B 和基础版本的 CogAgent-18B，可以看出微调能为所有维度带来显著提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">相比于 WebSight VLM-8B，该团队微调得到的 Design2Code-18B 在块元素匹配和文本相似度指标上表现更好，但在位置相似度和颜色相似度指标上表现更差。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队表示，前两个观察可以归因于更强更大的基础模型，而后两个则可归功于更大量的微调数据。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">人类评估</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也进行了人类评估。下面是主要的评估协议和结果。每一个问题都由 5 位人类标注者给出评估意见，最终结果遵从多数意见。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">成对模型比较：也就是让标注者给一对生成的网页排名（一个来自基线方法，另一个来自受测方法），以决定哪一个与参考网页更相似。这里的基线是对 Gemini Pro Vision 采用直接 prompt 设计，收集的数据是其它七种方法与这种基线方法的胜 / 平 / 负的比例。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426635" data-ratio="0.587037037037037" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG55FQge4TFcozW0iac3wbRvoMuG7DuLRwcNnSwFRtDR9lKdibC2ibXcUyOA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果见图 4，可以看出：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPT-4V 显著优于其它基线，而且文本增强式 prompt 设计和自我修正式 prompt 设计能在直接 prompt 设计的基础上进一步提升。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文本增强式 prompt 设计可以少量提升 Gemini，但进一步增加自我修正方法却没有帮助。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WebSight VLM-8B 优于 Gemini 直接 prompt 设计方法（54% 的胜率和 35% 的败率），这说明在大量数据上进行微调可以在特定领域比肩商用模型。</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">新模型 Design2Code-18B 的表现与 Gemini Pro Vision 直接 prompt 设计方法相当（38% 的胜率和 37% 的败率）。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">直接评估：尽管有这些比较，但读者可能还是会问：「我们离自动化前端工程还有多远？」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">为了得到一个更直观的答案，该团队进一步让人类标注者比较了参考网页与最佳的 AI 生成网页（使用了 GPT-4V 自我修正式 prompt 设计）。他们从两个方面进行了直接评估：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1.AI 生成的网页能否替代原始网页？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">人类标注者认为：AI 生成的网页中，49% 可与参考网页互换。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2. 参考网页和 AI 生成的网页哪个更好？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">结果有点出人意料：在 64% 的案例中，人类标注者更偏爱 GPT-4V 生成的网页，也就是说他们认为 AI 生成的网页比原始参考图像的设计更好！</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><strong><span style="font-size: 15px;">自动评估 vs 人类评估</span></strong><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该团队也研究了自动指标与人类配对偏好之间的相关性。结果发现，人类通常更关注高层级的视觉效果和布局，而不是细节内容，这说明人类的思考方式是自上而下的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过，针对论文给出的结果，有人提出了不同意见，认为前端的工作流程远比表面看上去复杂，因此真正实现「自动化前端工程」还需要一段时间。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426636" data-ratio="0.6222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5IicWnRN4I4GopXnYKapwLPzKiaZpZpzZibLiaa67a7WXQ7Cf503MuaTJzg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426637" data-ratio="0.25462962962962965" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW92vANWeGB3nATtuVoxNgG5ibSYgvvIrlFsOUdBhwGIticKYia4DrUviaOQIbyIKQgQPG7gzxXvtzpH0w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于这个问题，你怎么看？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426638" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;color: rgba(0, 0, 0, 0.9);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>关注大模型的</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于DiT，支持4K图像生成，华为诺亚0.6B文生图模型PixArt-Σ来了]]></title>
        <id>2650910168_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910168&amp;idx=1&amp;sn=28f0f0028fab54f86aaf9f0fcf897b4e&amp;chksm=84e469a6b393e0b0c2848c926abaf61a938e02275aff189145a170625ddb14580c0df12639ff#rd"/>
        <updated>2024-03-10T04:25:41.000Z</updated>
        <summary type="html"><![CDATA[<p style="text-align: justify;line-height: 1.75em;"><br></p><div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="20" data-source-title=""><div class="js_blockquote_digest"><div><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">这个模型和 Sora 一样采用了 DiT 框架。</span></p></div></div></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">众所周知，开发顶级的文生图（T2I）模型需要大量资源，因此资源有限的个人研究者基本都不可能承担得起，这也成为了 AIGC（人工智能内容生成）社区创新的一大阻碍。同时随着时间的推移，AIGC 社区又能获得持续更新的、更高质量的数据集和更先进的算法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">于是关键的问题来了：我们能以怎样的方式将这些新元素高效地整合进现有模型，依托有限的资源让模型变得更强大？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了探索这个问题，华为诺亚方舟实验室等研究机构的一个研究团队提出一种新的训练方法：由弱到强式训练（weak-to-strong training）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426500" data-ratio="0.2962962962962963" data-s="300,640" data-type="jpeg" data-w="972" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMBfyOSF62KuGyoS0peibtUgro2SrevibcUia01tgtxkvMZuYiahSIE04f5g/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文标题：PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/pdf/2403.04692.pdf</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">项目页面：https://pixart-alpha.github.io/PixArt-sigma-project/</span><span style="font-size: 15px;color: rgb(123, 12, 0);"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">他们的研究基于他们去年十月提出的一种高效的文生图训练方法 PixArt-α，参阅机器之心报道《<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650893566&amp;idx=5&amp;sn=410500c06a19922d2e473e937826647e&amp;chksm=84e4a880b3932196af9191f8fcc9de7204213c8983ee90df627d3fe2fa19b978e6018cfd472f&amp;scene=21#wechat_redirect" textvalue="超低训练成本文生图模型 PixArt 来了，效果媲美 MJ，只需 SD 10% 训练时间" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">超低训练成本文生图模型 PixArt 来了，效果媲美 MJ，只需 SD 10% 训练时间</a>》。PixArt-α 是 DiT（扩散 Transformer）框架的一种早期尝试。而现在，随着 Sora 登上热搜以及 Stable Diffusion 层出不穷的应用，DiT 架构的有效性得到了研究社区越来越多工作的验证，例如 PixArt, Dit-3D, GenTron 等「1」。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队使用 PixArt-α 的预训练基础模型，通过整合高级元素以促进其持续提升，最终得到了一个更加强大的模型 PixArt-Σ。图 1 展示了一些生成结果示例。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426501" data-ratio="1.4834054834054835" data-s="300,640" data-type="jpeg" data-w="693" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMVbHsNQpEiaVJWUbmiaicvhZhgrUDiceGVjr11Pft15HNsC8ibYO8kVtWTBw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: center;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">PixArt-Σ 如何炼成？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">具体来说，为了实现由弱到强式训练，造出 PixArt-Σ，该团队采用了以下改进措施。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">更高质量的训练数据</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队收集了一个高质量数据集 Internal-Σ，其主要关注两个方面：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(1) 高质量图像：该数据集包含 3300 万张来自互联网的高分辨率图像，全都超过 1K 分辨率，包括 230 万张分辨率大约为 4K 的图像。这些图像的主要特点是美观度高并且涵盖广泛的艺术风格。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(2) 密集且准确的描述：为了给上述图像提供更精准和详细的描述，该团队将 PixArt-α 中使用的 LLaVA 替换成了一种更强大的图像描述器 Share-Captioner。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">不仅如此，为了提升模型对齐文本概念和视觉概念的能力，该团队将文本编码器（即 Flan-T5）的 token 长度扩展到了大约 300 词。他们观察到，这些改进可以有效消除模型产生幻觉的倾向，实现更高质量的文本 - 图像对齐。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">下表 1 展示了不同数据集的统计数据。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426502" data-ratio="0.36830601092896176" data-s="300,640" data-type="jpeg" data-w="915" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMiceqCreO9JMjaT91umTpzkfBakV7C8osicibgmYJeRgkL4ibb2u1H0X8Cg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">高效的 token 压缩</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了增强 PixArt-α，该团队将其生成分辨率从 1K 提升到了 4K。为了生成超高分辨率（如 2K/4K）的图像，token 数量会大幅增长，这就会导致计算需求大幅增长。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了解决这一难题，他们引入了一种专门针对 DiT 框架调整过的自注意力模块，其中使用了键和值 token 压缩。具体来说，他们使用了步长为 2 的分组卷积来执行键和值的局部聚合，如下图 7 所示。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426503" data-ratio="0.7302325581395349" data-s="300,640" data-type="jpeg" data-w="645" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMnibNWvAp0rcFK5ibdIRwe67PJd85rf6yPadQ2iag1R1iaib0wfbmUxmibmfA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，该团队还采用了一种专门设计的权重初始化方案，可在不使用 KV（键 - 值）压缩的前提下从预训练模型实现平滑适应。这一设计可有效将高分辨率图像生成的训练和推理时间降低大约 34%。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">由弱到强式训练策略</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队提出了多种微调技术，可快速高效地将弱模型调整为强模型。其中包括：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(1) 替换使用了一种更强大的变分自动编码器（VAE）：将 PixArt-α 的 VAE 替换成了 SDXL 的 VAE。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(2) 从低分辨率到高分辨率扩展，这个过程为了应对性能下降的问题，他们使用了位置嵌入（PE）插值方法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">(3) 从不使用 KV 压缩的模型演进为使用 KV 压缩的模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">实验结果验证了由弱到强式训练方法的可行性和有效性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通过上述改进，PixArt-Σ 能以尽可能低的训练成本和尽可能少的模型参数生成高质量的 4K 分辨率图像。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">具体来说，通过从一个已经预训练的模型开始微调，该团队仅额外使用 PixArt-α 所需的 9% 的 GPU 时间，就得到了能生成 1K 高分辨率图像的模型。如此表现非常出色，因为其中还替换使用了新的训练数据和更强大的 VAE。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，PixArt-Σ 的参数量也只有 0.6B，相较之下，SDXL 和 SD Cascade 的参数量分别为 2.6B 和 5.1B。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">PixArt-Σ 生成的图像的美观程度足以比肩当前最顶级的文生图产品，比如 DALL・E 3 和 MJV6。此外，PixArt-Σ 还展现出了与文本 prompt 细粒度对齐的卓越能力。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br>图 2 展示了一张 PixArt-Σ 生成 4K 高分辨率图像的结果，可以看到生成结果很好地遵从了复杂且信息密集的文本指令。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426504" data-ratio="0.7122302158273381" data-s="300,640" data-type="jpeg" data-w="973" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMbjOEna6BQGKRLM0FPF8rXwOnia8xjBpaqwVLXR7rtmia5ziaAWOYaxiauQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">实验</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">实现细节</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">训练细节：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">对于执行条件特征提取的文本编码器，该团队按照 Imagen 和 PixArt-α 的做法使用了 T5 的编码器（即 Flan-T5-XXL）。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">基础扩散模型就是 PixArt-α。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">不同于大多数研究提取固定的 77 个文本 token 的做法，这里将文本 token 的长度从 PixArt-α 的 120 提升到了 300，因为 Internal-Σ 中整理的描述信息更加密集，可以提供高细粒度的细节。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">另外 VAE 使用了来自 SDXL 的已预训练的冻结版 VAE。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">其它实现细节与 PixArt-α 一样。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">模型是基于 PixArt-α 的 256px 预训练检查点开始微调的，并使用了位置嵌入插值技术。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">最终的模型（包括 1K 分辨率）是在 32 块 V100 GPU 上训练的。他们还额外使用了 16 块 A100 GPU 来训练 2K 和 4K 图像生成模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">评估指标：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">为了更好地展示美观度和语义能力，该团队收集了 3 万对高质量文本 - 图像，以对最强大的文生图模型进行基准评估。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">这里主要是通过人类和 AI 偏好来评估 PixArt-Σ，因为 FID 指标可能无法适当地反映生成质量。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">性能比较</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">图像质量评估：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">该团队定性地比较了 PixArt-Σ 与闭源文生图（T2I）产品和开源模型的生成质量。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">如图 3 所示，相比于开源模型 SDXL 和该团队之前的 PixArt-α，PixArt-Σ 生成的人像的真实感更高，并且也有更好的语义分析能力。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">与 SDXL 相比，PixArt-Σ 能更好地遵从用户指令。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426505" data-ratio="1.2919605077574048" data-s="300,640" data-type="jpeg" data-w="709" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHM18VG6IxibczibaeiaibnjJlY0MOL1JiaIYongZ4MicECYzCLoKMBOC2IDTIw/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">PixArt-Σ 不仅优于开源模</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">型，而且与当前的闭源产品相比也颇具竞争力，如图 4 所示。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426507" data-ratio="1.1418539325842696" data-s="300,640" data-type="jpeg" data-w="712" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHMjsHcBhiayT2QPcC6TO17mXGNRVk6zWyM9VrgzBFjuXLibZXY6P2tUXDA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">生成高分辨率图像：</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">新方法可以直接生成 4K 分辨率的图像，而无需任何后处理。</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">此外，PixArt-Σ 也能准确遵从用户提供的复杂和详细的长文本。</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;">因此，用户无需费心去设计 prompt 也能得到让人满意的结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">人类 / AI（GPT-4V）偏好研究：</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">该团队也研究了人类和 AI 对生成结果的偏好。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">他们收集了 6 个开源模型的生成结果，包括 PixArt-α、PixArt-Σ、SD1.5、Stable Turbo、Stable XL、Stable Cascade 和 Playground-V2.0。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">他们开发了一个网站，可通过展现 prompt 和对应的图像来收集人类偏好反馈。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">人类评估者可根据生成质量以及与 prompt 的匹配程度来给图像排名。结果见图 9 的蓝色条形图。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">可以看出人类评估者对 PixArt-Σ 的喜爱胜过其它 6 个生成器。相比于之前的文生图扩散模型，如 SDXL（2.6B 参数）和 SD Cascade（5.1B 参数），PixArt-Σ 能以少得多的参数（0.6B）生成质量更高且更符合用户 prompt 的图像。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426508" data-ratio="0.8250904704463209" data-s="300,640" data-type="jpeg" data-w="829" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9NdLREzHXJhic8sa9osVSHM2FpZeuIwXeSE5ZqVz670NoiaU4GY4pI2clqg5lgcS8eD6DlcWTcSTtA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">此外，该团队还使用了先进的多模态模型 GPT-4 Vision 来执行 AI 偏好研究。他们的做法是给 GPT-4 Vision 提供两张图像，让它基于图像质量和图像 - 文本对齐程度进行投票。结果见图 9 中的橙色和绿色条形图，可以看到情况与人类评估基本一致。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该团队也进行了消融研究来验证各种改进措施的有效性。更多详情，请访问原论文。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 15px;">参考文章：1.<span style="color: rgb(136, 136, 136);font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">https://www.shoufachen.com/Awesome-Diffusion-Transformers/</span></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><br></span></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426509" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[奥特曼重返OpenAI董事会：看完3万份文件，调查组认定了]]></title>
        <id>2650910146_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910146&amp;idx=1&amp;sn=21ded1f8772cef5bb40780e9bdf2bfba&amp;chksm=84e469bcb393e0aa49d0160edff7024d3ef051db981656f441435614ac1c8d438ecb5e89d180#rd"/>
        <updated>2024-03-09T03:23:37.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：小舟、泽南</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="9" data-source-title=""><div class="js_blockquote_digest"><p>过山车一样的剧情。</p></div></blockquote><p><span style="font-size: 15px;letter-spacing: 0.034em;">特别独立调查委员会发现，在去年 OpenAI 管理层动荡时，首席执行官萨姆・奥特曼（Sam Altman）的行为「不构成强制解雇」，现在他重新加入董事会了。</span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">历时超过 110 天，OpenAI 的宫斗剧现在迎来了盖棺定论的时刻。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426472" data-ratio="0.9782383419689119" data-s="300,640" data-type="jpeg" data-w="965" style="width: 558px;height: 546px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a381Rb4rWia6o9equk3QnAPgaattdFFfnDeyMLtiauC5p1qaIHE8fmXtVg/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">没有通用人工智能危机，也和神秘的技术突破 Q* 无关，国际律师事务所 WilmerHale 在大量调查之后认定，这次动荡的原因在于董事会成员之间关系破裂。现在，奥特曼和 Greg Brockman 是「OpenAI 的正确领导者」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426471" data-ratio="0.575" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3opPpTWnHhSfLxHcew5v1rNntqVyqo7U2ejKZABY8YvZkibicic00oUnIA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在发布消息与记者通话时，奥特曼坐在 Greg Brockman 旁边，显得很高兴。不过在采访中他也被问及 OpenAI 联合创始人兼首席科学家 Ilya Sutskever 现在的情况，后者被认为在失败的政变中发挥了关键作用 —— 先是支持开除奥特曼，但当大多数 OpenAI 员工威胁如果奥特曼不回来就辞职时，他改变了立场。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在去年的宫斗发生后，Sutskever 便不再频繁向外界发声，这引发了人们对他未来参与公司事务的质疑。奥特曼在电话会议上表示：「没什么可宣布的，但 Ilya 太棒了…… 我希望我们在余下的职业生涯中共同努力。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">奥特曼表示，最近那些「让我们互相对抗的困难」没有破坏团队，他「很高兴整件事都结束了」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">曾经三天三换 CEO，现在<span style="font-size: 15px;letter-spacing: 0.51px;text-wrap: wrap;">「走在正确道路上」</span>，看到 OpenAI 的管理回到正轨，人们纷纷表示欢迎：已经迫不及待想用上 GPT-5 了。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426473" data-ratio="0.7948717948717948" data-s="300,640" data-type="jpeg" data-w="975" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3HVNgNzkI1aXoiaoRD6bH4rGy258YoSzyuZxJMqsNdRicmbjKII5MJkibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">不过在极客和创业者扎堆的 hackernews 上也有人表示，这虽说不是坏事，但也很难评，他们就是宫斗赢了：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426474" data-ratio="0.38055555555555554" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3Okz3glP08JpQGTgfdZJSCpZ6P69SbktOziceEUXCuM52RkKMB1Q27wA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">的确，在奥特曼回归后，之前罢免他的董事已被纷纷解雇。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 在今天早晨发表了一份官方声明，简单介绍了其邀请的独立调查机构得出的结论。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426475" data-ratio="0.26851851851851855" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3oZcA9mrtIXXqqYibetfIatc13ibMGlkYFWTkI6YgR59VtUkNoMfCq3RA/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，除奥特曼之外，OpenAI 其他新一届董事会成员包括：Sue Desmond-Hellmann、Nicole Seligman 和 Fidji Simo。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">他们将与 Quora 首席执行官 Adam D'Angelo、美国前财政部长拉里・萨默斯（Larry Summers）以及 Salesforce 前联合首席执行官、董事长布雷特・泰勒（Bret Taylor）共同监督公司事务。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 董事会特别委员会今天宣布完成由美国著名律师事务所 WilmerHale 执行的审查，该审查对 OpenAI 前任董事会成员、OpenAI 高管、前任董事会顾问以及其他相关证人进行了数十次询问；审查了 30000 多份文件，并评估了各种企业行为。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">根据 WilmerHale 制定的记录并根据特别委员会的建议，董事会对山姆·奥特曼和 Greg Brockman 持续领导 OpenAI 表示充分信任。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 董事会主席 Bret Taylor 表示：「我们一致认为 Sam 和 Greg 是 OpenAI 的合适领导者。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">审查的结果意味着：奥特曼作为首席执行官将重新加入 OpenAI 董事会。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">同时，OpenAI 董事会宣布了以下几点新举措：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">采用一套新的公司治理准则；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">优化 OpenAI 的利益冲突政策；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">创建举报热线，作为所有 OpenAI 员工和承包商的匿名举报资源；</span></p></li><li><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;">创建额外的董事会委员会，包括专注于 OpenAI 核心使命的实现和推进的「使命与战略委员会」。</span></p></li></ul><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">扩大后的董事会将优先考虑优化治理程序的关键工作，以最好地实现 OpenAI 的使命。Bret Taylor 补充道：「我们认识到我们在管理变革技术以造福全球方面所发挥的重要作用。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会认可 WilmerHale 在进行此次广泛审查中所做的重要工作，并感谢 OpenAI 现任和前任董事会成员、顾问和员工的合作。OpenAI 董事会特别委员会发布了审查结果摘要。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>WilmerHale 的审查和调查结果摘要</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2023 年 12 月 8 日，特别委员会聘请 WilmerHale 对 2023 年 11 月 17 日山姆·奥特曼和 Greg Brockman 被解除 OpenAI 董事会职务以及奥特曼终止首席执行官职务的事件进行审查。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会向 WilmerHale 提供了进行全面审查所需的资源和权力。许多 OpenAI 员工以及现任和前任董事会成员都配合了审查过程。WilmerHale 多次向特别委员会通报审查进展和结论。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 评估了引起前一届董事会注意的管理和治理问题，以及 WilmerHale 在审查过程中发现的其他问题。WilmerHale 发现前任董事会与<span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">山姆·奥特曼</span>之间的信任破裂，导致了 11 月 17 日的事件。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 审查了上一届董事会于去年 11 月 17 日发布的公开内容，并得出结论认为，该声明准确地叙述了前任董事会的决定和理由。WilmerHale 发现，前任董事会当时认为其行动将缓解内部管理挑战，但没有预料到其行动会破坏公司的稳定。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">WilmerHale 还发现，先前董事会的决定并非出于对产品安全或保障、开发进度、OpenAI 的财务状况，或其向投资者、客户或业务合作伙伴的声明的担忧。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">相反，这是前任董事会与奥特曼之间关系破裂和失去信任的结果。WilmerHale 发现，前任董事会在较短的时间内执行了其决定，没有提前通知主要利益相关者，也没有进行全面调查，也没有让奥特曼有机会解决前任董事会的担忧。WilmerHale 发现，前任董事会在其广泛的自由裁量权范围内终止了奥特曼的职务，但也发现他的行为「不构成强制解雇」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在审查 WilmerHale 的调查结果后，特别委员会向全体董事会建议批准 11 月 21 日重新聘用奥特曼和 Brockman 的决定。了解审查结果后，特别委员会对奥特曼和 Brockman 对 OpenAI 的持续领导表示充分信任。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">特别委员会很高兴结束此次审查，并期待继续开展 OpenAI 的重要工作。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>新任命的董事</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 本次任命的另外三位新董事，或许向人们指明了这家 AI 领域最热门公司未来的走向。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426476" data-ratio="0.6666666666666666" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3397Whmibjibqjthp6ib8wFdSudH3TkfRoOVqVGKd4VEDc7PialahUYPBfw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">苏德斯・蒙德 - 赫尔曼博士（Sue Desmond-Hellmann）是比尔和梅琳达・盖茨基金会的前首席执行官，目前也在辉瑞董事会以及总统科学技术顾问委员会任职，她是美国肿瘤学和生物技术领域的著名学者。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426478" data-ratio="1" data-s="300,640" data-type="png" data-w="1080" style="width: 437px;height: 437px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a36Bl7iahIAkoMEqy9s4ukw52TcVx9rwLkOSM0hyXVNNkZNQhCZURibESw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">妮可・塞利格曼（Nicole Seligman 是一位律师，她因在伊朗反对派听证会上代表 Oliver North 中校，以及在弹劾案中代表比尔・克林顿总统而受到美国全国的关注。她是索尼前执行副总裁和全球总法律顾问、索尼娱乐总裁，她在派拉蒙全球、Meira GTx 和 Intuitive Machines, Inc. 董事会任职。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426479" data-ratio="0.6666666666666666" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8MrN6Gctz0omxdNt7Fd2a3rhlAlia7P1VqVjuia34ITe8GA01fqH8tRmqBm08jCX5iabM6MbxYiaOvMA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">菲吉・西莫（Fidji Simo）的职业经历则一直在科技领域的范畴之内。她曾在 eBay 工作，担任过 Facebook App 副总裁兼负责人、Instacart 的首席执行官和主席，目前她也在 Shopify 董事会上。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">参考内容：</span><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://www.theverge.com/2024/3/8/24094885/openai-sam-altman-investigation-board-results</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://www.theinformation.com/articles/sam-altman-to-return-to-openai-board-of-directors</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">https://news.ycombinator.com/item?id=39647105</span></em></span><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426496" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span><span style="color: rgb(136, 136, 136);font-size: 12px;font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;"></span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[清华姚班本科生连发两作，十年来最大改进：矩阵乘法接近理论最优]]></title>
        <id>2650910100_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650910100&amp;idx=1&amp;sn=f6fde6912a71233d5608b1af31f01bfb&amp;chksm=84e469eab393e0fc0a1f68ab7263928b49e32254b65e7fcec9d139784a3792a22b1b203bb175#rd"/>
        <updated>2024-03-08T04:32:19.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">选自QuantaMagazine</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编译</strong></span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">作者：Steve Nadis</strong></span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：杜伟、大盘鸡</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="41" data-source-title="" style="letter-spacing: 0.578px;text-wrap: wrap;"><div class="js_blockquote_digest"><p>通过消除「隐藏的低效」问题，计算机科学家提出了一种比以往更快的大型矩阵相乘新方法。</p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">矩阵乘法作为众多 GPU 算子的基础操作，是高性能计算的重要问题之一，也是 AI 等应用的基石。它的算法机制本身相当简单，但为了达到更快的速度，人们多年来不懈努力，优化程度却一直有限。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">今日，在《量子杂志》的一篇报道中，我们看到了推动矩阵乘法速度进一步提升的两篇论文，其中清华姚班一位大四本科生全程参与了两篇论文的撰写，为该领域的算法改进带来了全新的希望。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="503426404" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8n9DkCzAP5E6kMK4OLlKSbejibZQ8KViay4iakjwYpWeibC5rOqmvQnW39w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><strong><span style="font-size: 16px;">矩阵乘法改进出现新「奇点」</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">计算机科学家是一群要求很高的人。对于他们来说，仅仅获得问题的正确答案是不够的，往往还要尽可能高效地获得答案。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们以矩阵或数字数组相乘为例，1812 年，法国数学家 Jacques Philippe Marie Binet 提出了一套人们至今仍在教授学生的基本规则。这套规则运行得很好，但已经有数学家找到了简化和加速该过程的方法。&nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426407" data-ratio="1.0685096153846154" data-s="300,640" data-type="png" data-w="832" style="width: 321px;height: 343px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8t7CqObCDVQ1IzLSTjuwmx1KbrhuysDhYZjOo547vNJRxhEZ6iblsIMQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="letter-spacing: 0.034em;color: rgb(136, 136, 136);">法国数学家 Jacques Philippe Marie Binet。</span></em></span><span style="font-size: 15px;letter-spacing: 0.034em;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，加速矩阵乘法过程的任务成为数学和计算机科学的交叉点。研究人员至今仍在继续改进该过程，尽管近几十年来进展相当有限。名古屋大学计算机科学家 François Le Gall 表示，自 1987 年以来，矩阵乘法的数值改进「一直很小，而且极其难以实现」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最近，来自清华大学的段然（Ran Duan）、周任飞（Renfei Zhou）和加州大学伯克利分校的 Hongxun Wu 在解决这个长期存在的问题上迈出了重要一步，撰写的论文足足有 87 页。对于三位研究者的成果， Le Gall 表示尽管改进本身相对较小，但是「从概念上讲比以往的改进都大。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">该论文被计算机科学领域的顶会 FOCS 2023 接收。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426408" data-ratio="0.28888888888888886" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8XiaozGfYwZAhiaCEn6eMbXn4mn7qOEFH31y3Xcd7qzoR6aguiadn1IFkw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文 v1 发布在 2022 年 10 月，v5 在 2023 年 11 月。论文地址：https://arxiv.org/abs/2210.10173</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">其中，段然为清华大学交叉信息研究院副教授，主要研究方向为图论算法、数据结构、计算理论。Hongxun Wu 为加州大学伯克利分校二年级博士生，也是清华姚班出身。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">周任飞为清华姚班 2020 级的大四本科生，主修理论计算机科学（TCS）。他主要研究（简洁）数据结构和快速矩阵乘法，并对 TCS 的其他领域具有广泛兴趣，比如流算法、博弈论和在线算法等。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此前，周任飞曾在理论计算机科学顶级会议 FOCS/SODA 上发表多篇论文。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426409" data-ratio="0.412962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8IVLS5ptsuRF9tRe7q7Kx4oicgP9VHjeEXziax3PNAWEdEqgUOs059z6w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">三位研究者的论文揭示了以前未知且未开发的潜在改进来源，并且已经取得了成果。2024 年 1 月发表的第二篇论文（周任飞同样参与撰写）以此为基础，展示了如何进一步增强矩阵乘法。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426410" data-ratio="0.3101851851851852" data-s="300,640" data-type="jpeg" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8VT6scRkZMpvjWic4dVwR4RD1ibSg79PsjCX2Br32BQlt5Iy4OY4Utj3A/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://epubs.siam.org/doi/10.1137/1.9781611977912.134</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">哈佛大学理论计算机科学家 William Kuszmaul 对此表示，这是一项重大的技术突破，是十多年来我们所看到的矩阵乘法的最大改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>矩阵乘法要改进什么问题</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">矩阵乘法可能看起来是一个晦涩的问题，但它是一种基本的计算操作。它被融入了人们每天使用的大部分算法中，用于各种任务，从显示更清晰的计算机图形到解决网络理论中的物流问题。就像在计算的其他领域一样，速度至关重要。即使是微小的改进最终也可能大大减少所需要的时间、计算能力和金钱。但目前，理论家主要感兴趣的是弄清这个过程到底能够有多快。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">传统的两个 n×n 矩阵相乘的方法 —— 即将第一个矩阵中每一行的数字与第二个矩阵中每一列的数字相乘 —— 需要进行 n³ 次独立的乘法操作。对于 2 乘 2 的矩阵而言，这意味着需要进行 2³，也就是 8 次乘法操作。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426411" data-ratio="1.9321428571428572" data-s="300,640" data-type="png" data-w="560" style="width: 377px;height: 728px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8OYBh3BvicwAF0oIDZ5j9VflLiciblDl0ktQKltiaekibiaZKxF55Abdf07pg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1969 年，数学家 Volker Strassen 发现了一种更精巧的方法，只需 7 个乘法步骤和 18 个加法步骤，就能完成 2×2 矩阵的乘法运算。两年后，计算机科学家 Shmuel Winograd 证明，对于 2×2 矩阵来说，7 步乘法确实是绝对最小值。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426412" data-ratio="2.032142857142857" data-s="300,640" data-type="png" data-w="560" style="width: 442px;height: 898px;" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8RqNqoVFKorIjH2iaVNSCS9ImFMRiaKnMa9c34mgMqZXVSLhM6plr74FA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Strassen 利用同样的想法证明，所有较大的 n×n 矩阵也可以用少于&nbsp; n3 步的方法进行乘法运算。这一策略中的一个关键因素涉及一个称为分解的程序：将一个大矩阵分解成一个个更小的子矩阵，这些子矩阵最终可能小到 2×2 甚至 1×1（只是单个数字）。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于将巨型数组分解成小块的理由相当简单，麻省理工学院的计算机科学家 Virginia Vassilevska Williams 说：「对于一个大矩阵（比如 100×100 的矩阵），人类很难想到最佳的算法。」即使是 3 乘 3 的矩阵也还没有完全解决。「然而，人们可以使用已经为小矩阵开发的快速算法来获得更大矩阵的快速算法。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究人员确定，速度的关键在于减少乘法步骤的数量，尽可能将指数从 n3（传统方法）降低。可能的最低值 n² 基本上就是写出答案所需的时间。计算机科学家把这个指数称为 Ω，即 ω。nω 是当 n 越来越大时，成功将两个 n×n 矩阵相乘所需的最少步骤。同为 2024 年 1 月论文合著者的周任飞说：「这项工作的重点，是看你能接近 2 多少，并且是否可以在理论上实现。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>激光法</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1986 年，Strassen 取得了另一项重大突破，他推出了矩阵乘法的激光法。Strassen 用它确定了 ω 的上限值为 2.48。虽然该方法只是大型矩阵乘法的一个步骤，但却是最重要的步骤之一，因为研究人员一直在不断改进它。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一年后，Winograd 和 Don Coppersmith 推出了一种新算法，对激光法进行了完美的补充。这套工具的组合在后来几乎所有加速矩阵乘法的研究中都得到了应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">下面是一个简化的方法，让我们来看看这些不同的元素是如何结合在一起的。让我们从两个大型矩阵 A 和 B 开始，将它们相乘。首先，你要把它们分解成许多较小的子矩阵，有时也叫块。接下来，你就可以使用 Coppersmith 和 Winograd 的算法，将其作为处理并最终组装这些块的指导手册。Vassilevska Williams 说：「它告诉我在乘积矩阵 C 中要乘什么、加什么，以及哪些元素在哪里。」「它只是一个从 A 和 B 建立 C 的『配方』」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">然而，这里有一个问题：有时你会得到具有共同元素的块。保留这些共同元素会相当于将这些元素计算两次，因此在某个时候，需要消除这些重叠部分。研究人员通过「消灭」它们所在的块来解决这个问题 —— 将它们的分量设置为零以将它们从计算中移除。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426413" data-ratio="1.1398148148148148" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8T86ozib0E5RGBo1VJV3Viaxq8gJI8ZfYptHZJF4bgCd7mZBwYzUTtO0w/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">Virginia Vassilevska Williams 是改进矩阵乘法新方法的团队成员之一，她提出了目前最快的方法。</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这就是 Strassen 的激光法最终发挥作用的地方。Le Gall 说，「激光法通常非常有效，并且通常能找到消除重叠的子块的好方法」。在激光消除了所有重叠之后，你就可以构建最终的乘积矩阵 C。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">将这些各种技术结合起来，就得到了一种用尽量少的乘法总数来乘两个矩阵的算法，至少在理论上是这样。激光法并不是为了实际应用；它只是一种思考矩阵相乘的理想方式。周任飞表示，「我们从未在计算机上运行这种方法，我们进行对它的分析。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">正是这种分析促成了 ω 十多年来的最大改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>被发现的「隐藏损失」</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在段然、周任飞和 Hongxun Wu 的第一篇论文《Faster Matrix Multiplication via Asymmetric Hashing》中，他们表明，施特拉森算法的进程可以大大加快。这一切要得益于他们称之为「隐藏损失」（hidden loss）的概念。周任飞表示，该概念深深地隐藏在以前的分析中，是无意中消除了太多块的结果。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">激光法的工作原理是将重叠的块标记为垃圾，并安排处理，而其他块被认为有价值并将被保存。不过，选择过程有些随机。事实上，被标记为垃圾的块可能最终还是有用的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这并不完全令人惊讶，但通过检查许多随机选择，段然团队确定激光法系统性地低估了块的价值，因此应该保存更多的块，减少扔掉的块。而且，正如通常的情况一样，更少的浪费可以转化为更高的效率。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">对于段然团队的做法，Le Gall 认为，「能够保留更多块而不重叠，这种做法实现了更快的矩阵乘法算法。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在证明了这种损失的存在后，段然团队修改了激光法标记块的方式，从而大大减少了浪费。他们将 ω 的新上限设定在了 2.371866 左右，这要比 Josh Alman 和 Vassilevska Williams 在 2020 年设定的上限 2.3728596 有所改进。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这看起来是一个不大的变化，<strong><span style="color: rgb(61, 170, 214);">将上限降低了大约 0.001，但这是自 2010 年以来科学家们看到的最大进步</span></strong>。相比之下，Vassilevska Williams 和 Alman 2020 年的结果只比之前的结果提高了 0.00001。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426414" data-ratio="0.26296296296296295" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8LszHqFERdHOZ29vo0C3fM5mEWa8BqSKc7yViafg3XGgibaMnVcLeLJVQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当然，对研究人员来说，最令人兴奋的不仅仅是新纪录本身，该记录并没有持续多久。事实上，这篇论文揭示了一种新的改进途径，而在此之前，这种途径完全没有被注意到。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Le Gall 称，近四十年来，每个人都依赖相同的激光法。随着段然等三位研究者的论文出现，我们可以做得更好。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">因此，周任飞参与撰写的 2024 年 1 月的论文改善了这种新方法，进一步减少了隐藏损失。他们又<strong><span style="color: rgb(61, 170, 214);">进一步提高了 ω 的上限，使它降低到了 2.371552</span></strong>。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="503426415" data-ratio="0.4462962962962963" data-s="300,640" data-type="png" data-w="1080" style="" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicntZlVR7FRzRy9zc9yDdic8HC3t3XBFAiar2U8PibuAY2KIC7E3SIoQc4byzIJyO5WiahAlTWvBqWOnQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">研究者还使用同样的方法来改进矩形（n×m）矩阵的乘法过程，该乘法过程在图论、机器学习和其他领域均有广泛应用。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">沿着这些方向取得一些进一步的进展几乎是肯定的，但这是有限度的。2015 年，Le Gall 和两位合作者证明，目前的方法，也就是激光法，再加上 Coppersmith 和 Winograd 的方法，无法得到低于 2.3078 的 ω。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">Le Gall 说：「要想进一步改进，就必须在 Coppersmith and Winograd 的原始方法基础上加以改进，而这种方法自 1987 年以来就没有真正改变过。」但到目前为止，还没有人提出更好的方法。也许根本就没有。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">周任飞说：「改进 ω 实际上是理解这个问题的一部分。如果我们能很好地理解这个问题，就能设计出更好的算法。不过，人们对这个古老问题的理解还处于非常初级的阶段。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">原文链接：</span><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">https://www.quantamagazine.org/new-breakthrough-brings-matrix-multiplication-closer-to-ideal-20240307/</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426416" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[「还是谷歌好」，离职创业一年，我才发现训练大模型有这么多坑]]></title>
        <id>2650909932_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909932&amp;idx=1&amp;sn=8b248b29157a4120671baa55b8a491e2&amp;chksm=84e46892b393e184560cc8c2cc6e28dcba0978db3b041aff00a495a0a98047572dbe85bca490#rd"/>
        <updated>2024-03-07T04:27:45.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">编辑：蛋酱、小舟</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="19" data-source-title="" style="outline: 0px;color: var(--weui-FG-1);letter-spacing: 0.544px;text-wrap: wrap;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p style="outline: 0px;visibility: visible;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">Karpathy：</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中肯的，一针见血的。</span></p></div></blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;">如何在不到一年的时间里创办一家公司、筹集资金、购买芯片，并搭建出追赶 Gemini pro/GPT 3.5 的 LLM？</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">很多人都对构建基础架构和训练大语言模型和多模态模型感到好奇，但真正走完「从零开始」这一流程的人很少。我们普遍认为，储备技术人才是前提，掌握核心算法是关键，但实际上，工程实践中冒出来的挑战，也实在令人头疼。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一年前，乘着大模型的热潮，Yi Tay 离开了工作 3 年多的谷歌，参与创办了一家名为 Reka 的公司并担任首席科学家，主攻大型语言模型。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在谷歌时，Yi Tay 参与过许多知名的大型语言模型和多模态模型工作，包括 PaLM、UL2、Flan-U-PaLM、LaMDA/Bard、ViT-22B、PaLI、MUM 等。即使经验如此深厚，他还是遇到了以往无法想象的困难。为了帮助更多创业者避雷，Yi Tay 在一篇博客中分享了自己踩过的那些「坑」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">「计算稀缺和不可靠的计算提供商使事情比预期困难得多，但我们凭借强大的技术实力渡过了难关。终于，我写了这篇博文，揭示了其中的一些挑战和经验教训。我希望这篇文章对很多人来说都是有趣或有教育意义的。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章发出后，得到了众多技术创业者的议论和转发。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426233" data-ratio="0.16666666666666666" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teK958G7b0HTMMnERjN4aCibN3EAX2dVPSeiaQptv7klfDaoqmglVf3g8MA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">连 Andrej Karpathy 也深有同感：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426234" data-ratio="0.21851851851851853" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teKhe3ibaaFajxpicic3iaiccw2j7CRPVL0bx2Bs1ENKQGX6OfZNFFfnibgj28w/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span></p><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="64" data-source-title=""><div class="js_blockquote_digest"><p>成熟的公司有专门的团队维护集群。随着规模的扩大，集群已经脱离了工程学的范畴，变得更加生物化，因此需要专门负责「硬件健康」的团队。</p></div></blockquote><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="189" data-source-title=""><div class="js_blockquote_digest"><p>「照看」训练运行是一项令人沮丧的训练大型模型日常生活体验。你需要仔细监控运行的生命体征：损失峰值、数值问题、吞吐量、梯度规范、策略熵等。每当运行性能下降或趋于平稳时（可能经常发生），你都要快速查找堆栈跟踪，看看发生了什么。你必须快速完成这项工作，否则可能会有 10000 个 GPU 闲置。通常，这是你从未见过的新的、奇特的、可怕的错误，所以你需要寻求帮助，看看是否有人能发现问题所在。</p></div></blockquote><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="198" data-source-title=""><div class="js_blockquote_digest"><div><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最严重的错误发生在凌晨 4 点。通常没人能看到，所以你只能禁止一些看起来有点可疑的节点，并尝试重新启动运行。有时，运行失败只是因为你当天没有得到神的眷顾，所以你在启动命令中加入了 while True: 循环。潜在的问题可能多种多样，从某些 GPU 发热过高、偶尔突然做错乘法运算到某些路由器宕机导致网络文件系统 I/O 减少，再到数据中心的某个人在未沟通的维护过程中物理断开电线连接。有的问题你甚至永远不会知道。</span></p></div></div></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span><span style="font-size: 15px;letter-spacing: 0.034em;">也有人发现了亮点：</span><span style="font-size: 15px;letter-spacing: 0.034em;">Yi Tay 所说的「荒野」（Wild）意思是「谷歌之外的公司」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;letter-spacing: 0.034em;"><img class="rich_pages wxw-img" data-imgfileid="503426235" data-ratio="0.2222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8meCIrSukoNOkLMDRP5teKIsyuYdo7icNcB6Qic2gxkOh7L7YYm7e7H2BPa3ic9ya2Ikia4icJNvLf6TA/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">要是从基础设施和硬件的角度来说，能媲美谷歌的团队还真是不多。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，让我们一起看看博客内容：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>LLM 时代的硬件彩票</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">训练模型的首要条件是获得计算能力。这看似简单易行，然而，最大的惊喜却是<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>计算提供商的不稳定性，以及集群、加速器及其连接质量因来源不同而存在的巨大差异</strong></span>。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">人们总以为这只是一个加速器选择的问题 / 争论（TPU 与 GPU 等），所有 GPU 集群都是一样的。我们的体验是，这很快就被证明是错误的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们对不同的服务提供商进行了抽样调查，发现即使是相同的硬件，即 GPU（H100），硬件质量的差异也非常大。请注意，这里的硬件指的是集群的整体质量，而不一定是芯片或加速器本身。整体感觉就像购买彩票一样。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">更具体地说，我们从几家计算提供商那里租用了几个集群，每个集群都有数百到数千个芯片。我们所见过的集群有的还过得去（只存在一些小问题，但只需花几个小时的时间就能解决），有的则完全无法使用，每隔几个小时就会因各种原因出现故障。具体来说，有些集群的节点每隔 N 个小时就会出现故障，问题包括布线问题（N 小得不合理）、GPU 硬件错误等。更令人惊讶的是，同一家提供商的每个集群在鲁棒性方面也可能存在巨大差异。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">同时，即使其他一些集群的节点明显更稳定，它们也可能存在 I/O 和文件系统不佳的问题，甚至连保存检查点都可能导致超时，或耗费大量时间来降低集群利用率。其他一些计算资源甚至需要完全不同的软件层才能运行，而且对自带代码库的团队不友好 — 运行实验或大型工作需要额外的迁移成本。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">凡事都不会尽善尽美，但可以确定的是，提供商的服务质量是参差不齐的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最令人沮丧的是什么？几乎不可能真正提前知道，尤其是在万事俱备的情况下，会得到什么样的硬件，以及体验会有多么强大 / 容错性如何。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">此外，如果供应商不能按时交货，将装备时间推迟几个月，导致用户在数周或数月内无法从其他来源采购，你更无从得知。有些供应商还会不小心删除你的检查点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我有没有说过，不同的集群会有不同的模型翻转利用率（MFU）？如果你不幸找到了一个节点布线不良或存在其他问题的提供商，那么浪费的计算量是不可忽视的。如果系统的文件系统非常不理想，那么当团队成员开始跨集群传输大量数据时，训练运行的 MFU 就会下降。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">每个服务提供商的售后水平也各不相同。从礼貌客气到不冷不热，从「对话式」的预制回复到将所有问题都归咎于用户，不一而足。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">总之，我们尝试过的每一个集群都有自己的风格、斗争和失败模式。而且，几乎每个集群都需要自己的热修复程序来解决一系列问题。尽管如此，我们还是认识到故障安全是非常重要的，为任何集群找到快速的热修复方案都是关键所在。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在过去的几个月里，我们构建了许多工具，以确保一切都可用，例如，围绕监控、高效检查点和其他各种优化的工具，甚至安装了我们的自定义文件系统，以实现可扩展的数据存储，而这只是实际需求的冰山一角。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">这些工具的组合为 MFU 带来了非同小可的改进，同时也最大限度地减少了在硬件条件恶劣的情况下的停机时间。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>GPU vs TPU</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">就我自己的公司来说，大部分时间都在使用 GPU 训练模型。不过在加入 Reka 之前，我在谷歌的大型语言模型训练中一直使用 TPU。CUDA 和 nccl 对我来说是最陌生的东西 (我是从一位曾在 Nvidia 工作的同事那里才知道它的发音是 Nickel 的）。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">与我在谷歌使用 TPU 的经历相比，GPU 的故障率让我完全大吃一惊。事实上，我并不记得 TPU 发生过很多故障，即使是在大型运行中也是如此，不过我不确定，自己是否只是因为拥有出色的基础架构和专门的硬件团队才不知道这一点。事实上，谷歌的 UL2 20B 模型是通过意外运行一个月来训练的。如果是在 GPU 领域，它肯定会在最初几天内就失败。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">话虽如此，<span style="font-size: 15px;color: rgb(61, 170, 214);"><strong>我认为这可能更多与管理加速器的硬件团队的能力有关</strong></span>，而不是底层芯片。拥有良好的硬件支持（来自计算提供商）非常重要。而这在很大程度上取决于他们是否真的有能力，于是，又印证了「硬件彩票」的概念。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">GPU 领域给人的感觉很奇怪。与分布式训练在 TPU pods 上的一等公民地位相比，多节点训练更像是一种事后考虑。在 GPU 领域，感觉就像不同的提供商以不同的方式将它们连接起来，以实现多节点训练，这导致不同地方的做法差异很大。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">虽然我不是硬件专家，但这就是我的真实印象。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>多集群设置的痛苦</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我职业生涯的大部分时间都是在谷歌基础架构上度过的，这些基础架构主要运行在 Borg、Xmanager 和 Colossus 上。因此，必须在不同的集群中建立新环境的概念对我来说非常陌生。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在当今时代，拥有多个加速器池集群似乎是不可避免的，除非专门在一个地点建立大量的加速器池。更具体地说，GPU 的供应（或供应不足）自然而然地造成了这种集群采购模式，在这种模式下，事物的性质是支离破碎的。训练大型模型还需要大量的 TB 级数据，即使只是移动数据也会带来诸多不便。同时，复制数据通常也不是一件简单的事情，而且在超大规模的情况下，复制数据的成本也很高。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">显然，最理想的情况是建立某种编排层，专门将作业发送到不同的服务器。我相信，许多注重人工智能的大公司一般都有某种基础设施，以提高研究人员的生活质量。但是，对于一家初创公司来说，在开始阶段建立这种复杂而花哨的 ML 训练基础设施其实是不可能的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">目前，我们公司开发了许多内部工作流程来缓解这些问题，并继续朝着世界级实验基础设施的黄金标准迈进。(有人告诉我，对于非顶级 / 大型公司来说，这种简陋的设置或多或少是一种常态）。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>野生代码</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">众所周知，一直以来我最喜欢的代码库是 T5X 和 Mesh Tensorflow，但它们有一些缺点：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">1）它们在 Google 之外没有得到那么多的支持；</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2）它们有点被弃用了；</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">3）它们对我们团队中的非 xoogler 不友好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们最终选择了一些普通的、看似稳定且更流行的东西，即 pytorch。pytorch 对团队中的大多数人（除了我）来说更容易使用。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">在最初的几个月里，我对 pip、git、docker 和所有「野生(wild)」的东西感到困惑。话又说回来，我不能 100% 确定在外部使用 google 代码库有多稳定或多用户友好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">坦率地说，外部代码库的质量明显落后于我在谷歌习惯使用的代码库。主要是因为谷歌内部的代码库往往是由 ML 大牛自己编写的（例如 Noam Shazeer、Barret Zoph、Adam Roberts、Hyung Won Chung 等人），并且与我在外部尝试过的代码库相比感觉更好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">另外，我从来不知道更改模型并行性的能力不是自动（免费）的，直到某些代码库要求我编写一个转换器来更改模型的并行性。对我来说，这绝对是一个「WTF 时刻」。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">令人惊讶的是，这些代码库对大规模编码器 - 解码器训练甚至 prefixLM 训练的支持非常少。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 16px;"><strong>少一点原则，多一点 Yolo</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">系统地扩展模型通常需要有原则地从小到大，即分多个阶段（1B→8B→64B→300B 等）进行实验，然后选出获胜者并不断扩展它们。在初创公司中，我们执行大规模扫描来检查超参数的计算量要少得多。最后，我们不得不多次运行 Yolo，幸运的是结果很好。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最终，我们只需要极少数的较小规模和较短的烧蚀运行即可获得强大的 21B Reka Flash 和 7B 边缘模型，以及我们即将推出的最大核心模型。在运行次数非常有限的情况下找到可靠的方案具有挑战性，并且考虑到搜索空间极其巨大，需要立即更改许多变量。为了做到这一点，人们必须放弃大型科技公司的系统性，而在很大程度上依赖「Yolo」、直觉和本能。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">值得庆幸的是，我以及我们团队中的许多人在我们的 ML 职业生涯中已经积累了相当多的「直觉」，可以在很短的尝试时间内得到正确的结果。虽然我们在之前的工作中训练过非常好的模型，但训练基础设施、数据、新想法的融合和其他环境问题的差异仍然可能导致结果的巨大差异。也就是说，强大的先验有助于显著减少搜索空间，这可能就是我们能够通过如此少的试验、资源和实验训练出真正强大的模型的原因之一。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">&nbsp;</span></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 0px;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">原文链接：https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426277" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name/>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[刚刚，OpenAI官方发文驳斥马斯克，自曝8年间邮件往来截图]]></title>
        <id>2650909778_1</id>
        <link href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909778&amp;idx=1&amp;sn=ff24affa123b432ed266603c244a7a3b&amp;chksm=84e4682cb393e13a97da7431683dcf99962616cd52f354a9e02ac6e451555bca2fa2d1bb0031#rd"/>
        <updated>2024-03-06T03:54:18.000Z</updated>
        <summary type="html"><![CDATA[<div data-mpa-powered-by="yiban.io" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-bottom: 0px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;text-size-adjust: inherit;caret-color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="outline: 0px;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><div data-darkmode-bgcolor-16095509242984="rgb(25, 25, 25)" data-darkmode-original-bgcolor-16095509242984="rgb(255, 255, 255)" data-style="margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;" mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;line-height: 27.2px;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><p style="margin-top: -1.2em;margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;color: rgb(163, 163, 163) !important;text-shadow: transparent 0px 0px 0px, rgba(0, 0, 0, 0.4) 0px 0px 0px !important;"><span mp-original-font-size="15" mp-original-line-height="29.75" style="outline: 0px;color: rgb(255, 255, 255);font-family: inherit;font-size: 15px;letter-spacing: 0.544px;background-color: rgb(117, 117, 118);text-decoration: inherit;visibility: visible;line-height: 29.75px;">机器之心报道</span></p><p style="margin-right: 8px;margin-left: 8px;outline: 0px;text-align: center;visibility: visible;line-height: 1.75em;"><span mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;visibility: visible;line-height: 29.75px;"><strong mp-original-font-size="12" mp-original-line-height="29.75" style="outline: 0px;visibility: visible;line-height: 29.75px;">机器之心编辑部</strong></span></p></div></div></div></div></div><blockquote class="js_blockquote_wrap" data-type="2" data-url="" data-author-name="" data-content-utf8-length="22" data-source-title="" style="outline: 0px;color: var(--weui-FG-1);font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;"><div class="js_blockquote_digest" style="outline: 0px;visibility: visible;"><p>「不幸的是，人类的未来掌握在■■■的手上。」</p></div></blockquote><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">最热科技公司 OpenAI 对全球首富马斯克，这场史诗大战进入了新的高度。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">刚刚，OpenAI 用一篇长文《OpenAI and Elon Musk》，正式驳斥了马斯克的所有指控。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426087" data-ratio="0.6203703703703703" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FsuShXVOic78NABmr5qibna5YKAQQJTggwkezXsTAUJIEPdMOXkNHddibA/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">标题简洁，但内容却相当吸引眼球。OpenAI 直接晒出了八年来各位创始团队成员与马斯克的往来邮件截图，并反复重申 OpenAI 对成立使命的不懈追求。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章开篇表示：「OpenAI 的使命是确保 AGI 惠及全人类，这意味着既要构建安全、有益的 AGI，又要帮助创造广泛的利益。我们正在分享我们在实现使命方面所学到的知识，以及有关我们与马斯克关系的一些事实。我们打算驳回马斯克的所有主张。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">与此同时，曝光邮件内容中的一些「神秘信息」，再次加重了人们的好奇：马斯克所提到的人类未来，到底取决于什么？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426088" data-ratio="0.5435185185185185" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FcSt80n5bJ79X3dSuFs2Vt7diaVy9315MFSJns8OFoJCRYA8BBolAEPg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">文章由 Greg Brockman、Ilya Sutskever、John Schulman、Sam Altman、Wojciech Zaremba 共同署名。当科技圈都在猜测 Ilya 是否已经因为与 Sam Altman 的矛盾离开 OpenAI 的时候，署名算是一种变相回应：至少此时，Ilya 仍作为核心成员全力抵御 OpenAI 正在面对的风暴。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426089" data-ratio="0.992" data-type="png" data-w="1000" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FrP2O6BHsqJNsTemUsiaIG4iaRUJo7IqIpK4lCjlALo4P83lr6l1UOJ5g/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">几天前，马斯克这位前 OpenAI 联合创始人在一份长达 46 页、总字数超过 1.4 万字的诉讼文件中，指控 OpenAI 不计后果地开发人类级别的人工智能，并将其移交给微软。马斯克的诉讼要求法院强制 OpenAI 回归开源，并阻止公司及其创始人以及微软等背后支持者从中获利。（参见：<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650909259&amp;idx=1&amp;sn=bfda3ce888d99208aa7cc6aa33844889&amp;chksm=84e46e35b393e723e624968bfc2e1e728ce738c5429e0b24cd6aa580b9fc45d5ad0379763b3b&amp;scene=21#wechat_redirect" textvalue="《马斯克起诉 OpenAI：他们做出了 AGI 还授权给微软，这是对创始协议赤裸裸的背叛》" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">《马斯克起诉 OpenAI：他们做出了 AGI 还授权给微软，这是对创始协议赤裸裸的背叛》</a>）</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI CEO 对此的态度显得意味深长，让事情变得越发神秘：</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426091" data-ratio="0.2101851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8Fv9rO03ViawJZVf09EbeuQPfkk6xAicletVBFoR6y2nOIicrq7JOmeB8Ng/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426092" data-ratio="0.22037037037037038" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FyzO56n83ZpCL3tOQZqTYZcIibMEt4TcgkaqrZWCDQGZSZ1kPbT4xhww/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克与 OpenAI 之间是理念不合，还是利益之争，局外人很难揣测。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">一切没说破的话，可能都在这封信里。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>OpenAI 的反击</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 在最新发布的长信中写道：&nbsp; &nbsp;</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们意识到构建 AGI 需要的资源比我们最初想象的要多得多</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">（在 2015 年的一封邮件中）马斯克称，我们应该对外宣布一份 10 亿美元的启动资金。但总的来说，作为非营利组织的 OpenAI 总共从马斯克那里筹集了不到 4500 万美元，从其他捐助者那里筹集了超过 9000 万美元。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">2015 年底创办 OpenAI 时，Greg Brockman 和 Sam Altman 最初计划筹集 1 亿美元。马斯克在一封电子邮件中表示：「我们需要一个比 1 亿美元大得多的数字，以避免听起来目标无望…… 我认为我们应该从 10 亿美元的资金承诺开始…… 对于其他人未提供的部分，我将负责补足。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426093" data-ratio="0.9648148148148148" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FfK1hfzIA5JqrqN780GeKW84QN3ZUrj2LNtVQBML3yt2PhEldiavckCQ/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们花了很多时间试图设想一条可行的通用人工智能道路。2017 年年初，我们意识到构建 AGI 将需要大量算力，并且开始计算 AGI 可能需要多少计算量。我们都知道，我们需要更多的资金才能成功完成我们的使命 —— 每年大概数十亿美元，这远远超过我们任何人，尤其是马斯克认为我们作为非营利组织能够筹集到的资金。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们和马斯克认识到：需要一个营利性实体来获取这些资源</span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当我们讨论以营利为目的的结构以进一步实现我们的使命时，马斯克希望我们与特斯拉合并，或者由他来完全控制 OpenAI。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">后来，马斯克离开了 OpenAI，表示需要有一个与谷歌 / DeepMind 竞争的对手，而他将自己做这件事。他说他会支持我们找到自己的道路。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">事情是这样的，2017 年年底，我们和马斯克决定下一步的任务是创建一个营利性实体。马斯克想要获得多数股权、初始董事会控制权并担任首席执行官，并且在讨论期间，他扣留了资金。Reid Hoffman 弥补了工资和运营方面的缺口。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们无法与马斯克就营利性条款达成一致，因为我们认为任何个人对 OpenAI 拥有绝对控制权都是违背使命的。然后马斯克建议将 OpenAI 并入特斯拉。2018 年 2 月上旬，马斯克向我们转发了一封电子邮件，建议 OpenAI 应该「将特斯拉作为其摇钱树」，并评论说：「这是完全正确的……特斯拉是唯一有希望与谷歌相媲美的选择。即便如此，制衡谷歌的可能性也很小。它只是不为零。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426094" data-ratio="0.8314814814814815" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8F8iabWV9gg8tNrPrMBmUFOksXMpad75vW2iaKl03oCEtlrsuwHwQvdFzg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426095" data-ratio="0.8851851851851852" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FcuBnCWM4WhZF3CYdF7kXdamd52msxibb5ibAgAqhV7D1ym4pmvtKsDOg/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426098" data-ratio="0.9342592592592592" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8F08LrzvicDqRddxexAMKaicOVcdDY2ico4PsJwY0v02ZW8h6eNlK8zZrYQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">很快，马斯克就选择离开 OpenAI，他说我们成功的可能性为零，他计划在特斯拉内部建立一个 AGI 竞争对手。2018 年 2 月底离开时，他告诉我们团队，他支持我们自己寻找融资数十亿美元的道路。2018 年 12 月，马斯克给我们发邮件说：「即使筹集到几亿美元也不够。做这件事需要每年迅速筹集数十亿美元，否则就算了。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426099" data-ratio="1.1222222222222222" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FoyIvf0R1Pib7UR4zdiawTgvuALH2SsaqvrrCwOFvDurAl31IFLsmHotw/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="line-height: 1.75em;margin-bottom: 0px;"><strong><span style="font-size: 15px;">我们通过构建可广泛使用的有益工具来推进我们的使命</span></strong><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们通过开源贡献等方式，使我们的技术能够广泛使用，从而增强人们的能力，改善他们的日常生活。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们广泛提供当今最强大的 AI，包括每天有数亿人使用的免费版本。例如，阿尔巴尼亚正在使用 OpenAI 的工具，将其加入欧盟的时间加快了 5.5 年；Digital Green 正在帮助肯尼亚和印度提高农民收入，以 OpenAI 的技术为基础将农业推广服务的成本降低至原来的 1/100；罗得岛州最大的医疗保健提供商 Lifespan 使用 GPT-4 将其手术同意书从大学阅读水平简化为六年级阅读水平；冰岛正在使用 GPT-4 保护冰岛语。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">马斯克明白，我们的使命并不意味着开源 AGI。Ilya 曾告诉马斯克「随着我们越来越接近构建人工智能，开始降低开放程度是有意义的。openAI 中的『开放』是指人工智能建成后，每个人都应从人工智能的成果中获益，但不分享科学成果也完全没问题......」。当时，马斯克回答道：「是的（Yup）」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426100" data-ratio="1.4444444444444444" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FxYiaVvRrvTIYVI1SRKO3WhibVd9IBdhyezfxy4vIS8HtuNX0Wyib5qm6Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-imgfileid="503426101" data-ratio="1.1583333333333334" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FibkF2VFibd4E1do2iazhjFtIcuiaWM0WmY7aMQMPx6qygVXmEU2MNiaOz0Q/640?wx_fmt=png&amp;from=appmsg"></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">我们很伤心，因为我们深深敬佩的一个人 —— 他激励我们向更高的目标迈进 —— 他说我们会失败，还成立了一个竞争对手公司。</span><span style="font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.034em;">当我们在没有他的情况下开始朝着 OpenAI 的使命取得有意义的进展时，他又起诉了我们。</span><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">我们专注于推进我们的使命，任重而道远。随着我们不断改进我们的工具，我们很高兴能部署这些系统，让每个人都能使用它们。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: center;"><span style="font-size: 16px;"><strong>吃瓜的网友</strong></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">OpenAI 的长信回应一时之间引爆了社交圈，吃瓜网友纷纷搬起了小板凳，围观这场「闹剧」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">活跃软件开发者、Deep trading 创始人 Yam Peleg 表示，「赶紧来吃新瓜啊」。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426102" data-ratio="0.475" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FkV6Qrlz1ap9N7PJ8ibBFAnmSsmoWKlqQhVPyxsZqEUFSRfWK0Z0uic4Q/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">著名学者、纽约大学教授 Gary Marcus 对 OpenAI 的声明嗤之以鼻，直言 OpenAI 所说的「我们持之以恒追求自己的使命，比如不受产生利益回报的限制、不为私人利益掣肘、在适用的情况下寻求技术开源」是胡说八道。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">现在，微软拥有 OpenAI 的独家所有权，49％的利润都归微软所有。OpenAI 不是在帮助全人类，而是从艺术家、作家那里窃取来增加自己的利润。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">更重要的是，OpenAI 几乎完全放弃了开源，并且成为最不开放的 AI 公司之一。无论与马斯克之间的恩怨如何，OpenAI 声称保持使命不变都是不诚实的。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">总之，OpenAI违背了 2015 年成立时的宣言。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426103" data-ratio="1.6861111111111111" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FulcuaYI9ibD2Fhsv9g7HR4Vz8uZ6VbeiaEZPsQE0dKclvyPOdFhNQa6g/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">另外，大家比较好奇的一点是双方邮件中的一句话，「不幸的是，人类的未来掌握在■■■的手上。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426104" data-ratio="0.4166666666666667" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FibXibOmCZPyTVkjnu5YcBUfZ5XaAvzkibgiaf8pQy8gAvLic0nDr5KUm2kQ/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">众多网友纷纷猜测■■■中是谁？谷歌吗？</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">还有人发现了以前从未听过的有趣观点，「OpenAI 的开放（open）只意味着每个人都可以从 AI 构建的成果中获益，而不向科学界分享是完全可以的。」</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426105" data-ratio="1.150925925925926" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FYxRRWvzOkeGqiaU6oib3DgDPic4UVmCG0icw6kbgg3CHjghGehjKx8Phyg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;">当然，更多人迫切地想要看到马斯克会如何回应。</span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503426106" data-ratio="0.2212962962962963" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibuCNMlxk5NtLy2NCiafDW8FpvQ8z9qibib35Z8Kib5PGB2lvH8aoIdRszrmyOlVLhp0COdrhbBAmMoIg/640?wx_fmt=png&amp;from=appmsg"></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="line-height: 1.75em;margin-bottom: 0px;text-align: left;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">参考链接：https://openai.com/blog/openai-elon-musk</span></em></span></p><p style="line-height: 1.75em;margin-bottom: 0px;"><br></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;margin-bottom: 0px;"><img class="rich_pages wxw-img" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="645" data-galleryid="" data-imgfileid="503426110" data-ratio="0.8305555555555556" data-s="300,640" data-type="png" data-w="1080" style="height: 480px;width: 578px;" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9BcwDFz901ZX0iaIEk07W0Q6SZr9pu0tfoIelKhoSSymoOjB3PbKtjqiaiarOibX8GTCnRWwfO2ehVibQ/640?wx_fmt=jpeg&amp;from=appmsg"></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br></p><p class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzkwODI4NjY2Mw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/bTicW6huibprLhW8WYM5M1CENXBd7Xn3bIjaTia6BX9cRhZ4ic6YHCDrv4ec9ao4tbtd6YEy7E9zCiascIEZzsEWibAA/0?wx_fmt=png" data-nickname="机器之心PRO会员" data-alias="almosthuman2014pro" data-signature="一份让您从此不再担心因业务繁忙而错失AI &amp; Robotics 赛道良机的业内通讯" data-from="0" data-is_biz_ban="0"></mp-common-profile></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;"><br></span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;line-height: 19.2px;">©&nbsp;THE END&nbsp;</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">转载请联系本公众号获得授权</span></p><p mp-original-font-size="17" mp-original-line-height="27.200000762939453" style="margin-top: 5px;margin-bottom: 0px;text-wrap: wrap;outline: 0px;color: rgb(34, 34, 34);font-family: -apple-system, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;caret-color: rgb(34, 34, 34);text-size-adjust: auto;background-color: rgb(255, 255, 255);text-align: center;line-height: 27.2px;"><span mp-original-font-size="12" mp-original-line-height="19.200000762939453" style="outline: 0px;font-size: 12px;color: rgb(136, 136, 136);line-height: 19.2px;">投稿或寻求报道：content@jiqizhixin.com</span></p><p style="display: none;"><mp-style-type data-value="10000"></mp-style-type></p>]]></summary>
        <author>
            <name>机器之心</name>
        </author>
    </entry>
</feed>